{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../auto_LiRPA/\")\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "sys.path.append(\"../complete_verifier/\")\n",
    "from arguments import ConfigHandler\n",
    "from abcrown import ABCROWN\n",
    "\n",
    "\n",
    "import onnx\n",
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/convert/layer.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/convert/model.py:151: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"./models/net6x50_best.onnx\")\n",
    "model = ConvertModel(onnx_model, experimental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvertModel(\n",
       "  (Flatten_/0/Flatten_output_0): Flatten()\n",
       "  (Gemm_/1/Gemm_output_0): Linear(in_features=12, out_features=50, bias=True)\n",
       "  (Relu_/2/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_/3/Gemm_output_0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (Relu_/4/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_/5/Gemm_output_0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (Relu_/6/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_/7/Gemm_output_0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (Relu_/8/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_/9/Gemm_output_0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (Relu_/10/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_/11/Gemm_output_0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (Relu_/12/Relu_output_0): ReLU(inplace=True)\n",
       "  (Gemm_Y): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Format\n",
    "\n",
    "The dataset consists of $5000$ samples of $(l, u, \\alpha)$ for $2\\times 2$ maxpool units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  torch.Size([5000, 3, 2, 2])\n",
      "y.shape =  torch.Size([5000, 1])\n"
     ]
    }
   ],
   "source": [
    "ds_val = torch.load(\"./datasets/maxpool2x2_val_100k_sorted.pth\")\n",
    "X, y = ds_val.tensors\n",
    "\n",
    "print(\"X.shape = \", X.shape)\n",
    "print(\"y.shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows, which entries of the model's input represent which quantities (the batch dimension doesn't have to be $5$, but can be an arbitrary value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lua = X[:5,0:3,:,:]\n",
    "l = X[:5,0:1,:,:]\n",
    "u = X[:5,1:2,:,:]\n",
    "alpha = X[:5,2:3,:,:]\n",
    "\n",
    "x_in = torch.ones_like(l)\n",
    "\n",
    "b = model(lua)\n",
    "x_hat = l + (u - l)*x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformulate Specification\n",
    "\n",
    "In the specification, we ultimately want\n",
    "$$\n",
    "a^T \\hat{x} + b \\geq \\hat{x}_i ~~ \\forall i = 1,\\dots,n\n",
    "$$\n",
    "We reorder the dimension to efficiently represent the dot products as a single matrix multiplication.\n",
    "\n",
    "Denote by \n",
    "$$\n",
    "r_{nc} = \\sum_{h,w} a_{nchw} \\hat{x}_{nchw}\n",
    "$$\n",
    "the dot product for each of the input neurons.\n",
    "\n",
    "Since we sum over all values of $h, w$ anyways, we can also flatten the tensors beforehand to get the equivalent result\n",
    "$$\n",
    "r_{nc} = \\sum_{z} a_{ncz} \\hat{x}_{ncz}\n",
    "$$\n",
    "where $z \\leq h \\cdot w$.\n",
    "\n",
    "If we reorder the dimensions of $\\hat{x}$ to $\\hat{x}^T_{nzc}$, then we have the formula for batched matrix multiplication\n",
    "$$\n",
    "r_{nc} = \\sum_{z} a_{ncz} \\hat{x}^T_{nzc}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_flat = alpha.flatten(-2)\n",
    "x_hat_flat = x_hat.flatten(-2)\n",
    "x_hat_T = x_hat_flat.transpose(-1, -2)\n",
    "\n",
    "a_T_x = torch.matmul(alpha_flat, x_hat_T).squeeze(-1)\n",
    "y = a_T_x + b\n",
    "\n",
    "violation = x_hat.flatten(-3) - y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model for Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$-$\\beta$-CROWN's parsing of ONNX files and PyTorch models seems to be dependent on the names of the parameters in the layers.\n",
    "For that reason, we can't use submodules or `moduleList`.\n",
    "\n",
    "Instead, we use a regular python list to store all layers of the `bias_model`.\n",
    "\n",
    "For tracing, we need to ensure that the parameters of these layers, however, don't require a gradient.\n",
    "So we have to set `requires_grad=False` for all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach doesn't seem to work due to \n",
    "- `onnx2pytorch` being unable to handle linear layers with no parameters (since they don't require a gradient) and\n",
    "- $\\alpha$-$\\beta$-CROWN has some problems with the `BoundSlice` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolVerification_6Layers(nn.Module):\n",
    "\n",
    "    def __init__(self, bias_model_layers):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = bias_model_layers[1]\n",
    "        self.fc2 = bias_model_layers[3]\n",
    "        self.fc3 = bias_model_layers[5]\n",
    "        self.fc4 = bias_model_layers[7]\n",
    "        self.fc5 = bias_model_layers[9]\n",
    "        self.fc6 = bias_model_layers[11]\n",
    "        self.fc7 = bias_model_layers[13]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # l, u, alpha for all batches and for all of the w x h inputs\n",
    "        lua = x[:,0:3,:,:]\n",
    "        l = x[:,0:1,:,:]\n",
    "        u = x[:,1:2,:,:]\n",
    "        alpha = x[:,2:3,:,:]\n",
    "        # normalized inputs to [0, 1] for all batches ans all w x h inputs\n",
    "        x_in = x[:,3:4,:,:]\n",
    "\n",
    "        # get bias prediction from model\n",
    "        #b = self.bias_model(lua)\n",
    "        b = b = self.flatten(lua)\n",
    "        b = self.fc1(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc2(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc3(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc4(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc5(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc6(b)\n",
    "        b = nn.functional.relu(b)\n",
    "        b = self.fc7(b)\n",
    "        \n",
    "        # transfrom input from [0, 1] to [l, u]\n",
    "        x_hat = l + (u - l)*x_in \n",
    "\n",
    "        # we need alpha^T x_hat for all the batches \n",
    "        # so we manipulate the shapes to express it as matmul\n",
    "        alpha_flat = alpha.flatten(-2)\n",
    "        x_hat_flat = x_hat.flatten(-2)\n",
    "        x_hat_T = x_hat_flat.transpose(-1, -2)\n",
    "        a_T_x = torch.matmul(alpha_flat, x_hat_T)[:,:,0]\n",
    "        y = a_T_x + b\n",
    "\n",
    "        # we want alpha^T x_hat + b >= x_hat_i for all i\n",
    "        # <==> 0 >= x_hat_i - (alpha^T x_hat  + b)\n",
    "        # so we want the RHS to be smaller equal 0, if it is \n",
    "        # larger than 0, we have a violation!!!\n",
    "        violation = x_hat.flatten(-3) - y \n",
    "\n",
    "        return violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"./models/net6x50_best.onnx\")\n",
    "model = ConvertModel(onnx_model, experimental=True)\n",
    "\n",
    "mpv6 = MaxPoolVerification_6Layers(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2598, -0.2598, -0.2598, -0.2598]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1,4,2,2)\n",
    "mpv6(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(mpv6, x, './models/mpv6.onnx', export_params=True, do_constant_folding=True, input_names=['X'], output_names=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_abcrown = BoundedModule(mpv6, x)\n",
    "\n",
    "data_min = torch.zeros_like(x)\n",
    "data_max = torch.ones_like(x)\n",
    "center = 0.5 * (data_min + data_max)\n",
    "\n",
    "ptb = PerturbationLpNorm(x_L=data_min, x_U=data_max)\n",
    "bound_x = BoundedTensor(center, ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lb =  tensor([[-278.2891, -278.2891, -278.2891, -278.2891]], grad_fn=<ViewBackward0>)\n",
      "ub =  tensor([[244.6781, 244.6781, 244.6781, 244.6781]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lb, ub = model_abcrown.compute_bounds(x=(bound_x,), method='crown')\n",
    "print(\"lb = \", lb)\n",
    "print(\"ub = \", ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lb =  tensor([[-91.7884, -92.4998, -91.8478, -92.2516]])\n",
      "ub =  tensor([[88.5101, 88.5159, 88.4735, 88.4429]])\n"
     ]
    }
   ],
   "source": [
    "lb, ub = model_abcrown.compute_bounds(x=(bound_x,), method='alpha-crown')\n",
    "print(\"lb = \", lb)\n",
    "print(\"ub = \", ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification with $\\alpha$-$\\beta$-CROWN\n",
    "\n",
    "Use https://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers/blob/main/neural_lyapunov_training/levelset.py as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vnnlib(filename, lbs, ubs, n_out):\n",
    "    n_in = lbs.shape[0]\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"; Input variables l, u, alpha, x\\n\")\n",
    "        f.write(f\"; l:     {0} -- {int(n_in / 4)}\\n\")\n",
    "        f.write(f\"; u:     {0} -- {int(n_in / 4)}\\n\")\n",
    "        f.write(f\"; alpha: {0} -- {int(n_in / 4)}\\n\")\n",
    "        f.write(f\"; x:     {0} -- {int(n_in / 4)}\\n\")\n",
    "        for i, (lb, ub) in enumerate(zip(lbs, ubs)):\n",
    "            f.write(f\"(declare-const X_{i} Real)\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"; Output variables (violations)\\n\")\n",
    "        for i in range(n_out):\n",
    "            f.write(f\"(declare-const Y_{i} Real)\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"; Input constraints\\n\")\n",
    "        for i, (lb, ub) in enumerate(zip(lbs, ubs)):\n",
    "            f.write(f\"(assert (>= X_{i} {lb}))\\n\")\n",
    "            f.write(f\"(assert (<= X_{i} {ub}))\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"; Counterexample found, if one of the violations is > 0\\n\")\n",
    "        f.write(\"(assert (or\\n\")\n",
    "        for i in range(n_out):\n",
    "            f.write(f\"\\t(and (>= Y_{i} 0))\\n\")\n",
    "\n",
    "        f.write(\"))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(csv_path, onnx_path, vnnlib_path, timeout=100):\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(f\"{onnx_path},{vnnlib_path},{timeout}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vnnlib(\"./verification/specs/mpv2x2.vnnlib\", torch.zeros(4*4), torch.ones(4*4), 4)\n",
    "generate_csv(\"./verification/specs/mpv2x2.csv\", \"./models/mpv6.onnx\", \"./specs/mpv2x2.vnnlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "\n",
      "general:\n",
      "  device: cpu\n",
      "  seed: 100\n",
      "  conv_mode: patches\n",
      "  deterministic: false\n",
      "  double_fp: true\n",
      "  loss_reduction_func: sum\n",
      "  sparse_alpha: true\n",
      "  sparse_interm: true\n",
      "  save_adv_example: false\n",
      "  eval_adv_example: false\n",
      "  show_adv_example: false\n",
      "  precompile_jit: false\n",
      "  complete_verifier: bab\n",
      "  enable_incomplete_verification: true\n",
      "  csv_name: specs/mpv2x2.csv\n",
      "  results_file: mpv2x2_6x50_results.txt\n",
      "  root_path: ..\n",
      "  deterministic_opt: false\n",
      "  graph_optimizer: 'Customized(\"custom_graph_optimizer\", \"default_optimizer\")'\n",
      "  buffer_has_batchdim: false\n",
      "  save_output: true\n",
      "  output_file: mpv2x2_6x50_out.pkl\n",
      "  return_optimized_model: false\n",
      "model:\n",
      "  name: null\n",
      "  path: null\n",
      "  onnx_path: null\n",
      "  onnx_path_prefix: ''\n",
      "  cache_onnx_conversion: false\n",
      "  debug_onnx: false\n",
      "  onnx_quirks: null\n",
      "  input_shape: [-1, 4, 2, 2]\n",
      "  onnx_loader: default_onnx_and_vnnlib_loader\n",
      "  onnx_optimization_flags: none\n",
      "  onnx_vnnlib_joint_optimization_flags: none\n",
      "  check_optmized: false\n",
      "  flatten_final_output: false\n",
      "  optimize_graph: null\n",
      "  with_jacobian: false\n",
      "data:\n",
      "  start: 0\n",
      "  end: 10000\n",
      "  select_instance: null\n",
      "  num_outputs: 10\n",
      "  mean: 0.0\n",
      "  std: 1.0\n",
      "  pkl_path: null\n",
      "  dataset: null\n",
      "  data_filter_path: null\n",
      "  data_idx_file: null\n",
      "specification:\n",
      "  type: lp\n",
      "  robustness_type: verified-acc\n",
      "  norm: .inf\n",
      "  epsilon: null\n",
      "  epsilon_min: 0.0\n",
      "  vnnlib_path: null\n",
      "  vnnlib_path_prefix: ''\n",
      "  rhs_offset: null\n",
      "solver:\n",
      "  batch_size: 64\n",
      "  auto_enlarge_batch_size: false\n",
      "  min_batch_size_ratio: 0.1\n",
      "  use_float64_in_last_iteration: false\n",
      "  early_stop_patience: 10\n",
      "  start_save_best: 0.5\n",
      "  bound_prop_method: alpha-crown\n",
      "  init_bound_prop_method: same\n",
      "  prune_after_crown: false\n",
      "  optimize_disjuncts_separately: false\n",
      "  crown:\n",
      "    batch_size: 1000000000\n",
      "    max_crown_size: 1000000000\n",
      "    relu_option: adaptive\n",
      "  alpha-crown:\n",
      "    alpha: true\n",
      "    lr_alpha: 0.1\n",
      "    iteration: 100\n",
      "    share_alphas: true\n",
      "    lr_decay: 0.98\n",
      "    full_conv_alpha: true\n",
      "    max_coeff_mul: .inf\n",
      "    matmul_share_alphas: false\n",
      "    disable_optimization: []\n",
      "  invprop:\n",
      "    apply_output_constraints_to: []\n",
      "    tighten_input_bounds: false\n",
      "    best_of_oc_and_no_oc: false\n",
      "    directly_optimize: []\n",
      "    oc_lr: 0.1\n",
      "    share_gammas: false\n",
      "  beta-crown:\n",
      "    lr_alpha: 0.01\n",
      "    lr_beta: 0.05\n",
      "    lr_decay: 0.98\n",
      "    optimizer: adam\n",
      "    iteration: 50\n",
      "    beta: true\n",
      "    beta_warmup: true\n",
      "    enable_opt_interm_bounds: false\n",
      "    all_node_split_LP: false\n",
      "  forward:\n",
      "    refine: false\n",
      "    dynamic: false\n",
      "    max_dim: 10000\n",
      "    reset_threshold: 1.0\n",
      "  multi_class:\n",
      "    label_batch_size: 32\n",
      "    skip_with_refined_bound: true\n",
      "  mip:\n",
      "    parallel_solvers: null\n",
      "    solver_threads: 1\n",
      "    refine_neuron_timeout: 15\n",
      "    refine_neuron_time_percentage: 0.8\n",
      "    early_stop: true\n",
      "    adv_warmup: true\n",
      "    mip_solver: gurobi\n",
      "    skip_unsafe: false\n",
      "bab:\n",
      "  initial_max_domains: 1\n",
      "  max_domains: .inf\n",
      "  decision_thresh: 0\n",
      "  timeout: 360\n",
      "  timeout_scale: 1\n",
      "  max_iterations: -1\n",
      "  override_timeout: null\n",
      "  get_upper_bound: false\n",
      "  pruning_in_iteration: true\n",
      "  pruning_in_iteration_ratio: 0.2\n",
      "  sort_targets: false\n",
      "  batched_domain_list: true\n",
      "  optimized_interm: ''\n",
      "  interm_transfer: true\n",
      "  recompute_interm: false\n",
      "  sort_domain_interval: -1\n",
      "  vanilla_crown: false\n",
      "  cut:\n",
      "    enabled: false\n",
      "    implication: false\n",
      "    bab_cut: false\n",
      "    lp_cut: false\n",
      "    method: null\n",
      "    lr: 0.01\n",
      "    lr_decay: 1.0\n",
      "    iteration: 100\n",
      "    bab_iteration: -1\n",
      "    early_stop_patience: -1\n",
      "    lr_beta: 0.02\n",
      "    number_cuts: 50\n",
      "    topk_cuts_in_filter: 1000\n",
      "    batch_size_primal: 100\n",
      "    max_num: 1000000000\n",
      "    patches_cut: false\n",
      "    cplex_cuts: false\n",
      "    cplex_cuts_wait: 0\n",
      "    cplex_cuts_revpickup: true\n",
      "    cut_reference_bounds: true\n",
      "    fix_intermediate_bounds: false\n",
      "  branching:\n",
      "    method: kfsb\n",
      "    candidates: 3\n",
      "    reduceop: min\n",
      "    enable_intermediate_bound_opt: false\n",
      "    branching_input_and_activation: false\n",
      "    branching_input_and_activation_order: [input, relu]\n",
      "    branching_input_iterations: 30\n",
      "    branching_relu_iterations: 50\n",
      "    nonlinear_split:\n",
      "      method: shortcut\n",
      "      branching_point_method: uniform\n",
      "      num_branches: 2\n",
      "      filter: false\n",
      "      filter_beta: false\n",
      "      filter_batch_size: 10000\n",
      "      filter_iterations: 25\n",
      "      use_min: false\n",
      "      loose_tanh_threshold: null\n",
      "      dynamic_bbps: false\n",
      "      dynamic_options: [uniform, three_left, three_right]\n",
      "    input_split:\n",
      "      enable: false\n",
      "      enhanced_bound_prop_method: alpha-crown\n",
      "      enhanced_branching_method: naive\n",
      "      enhanced_bound_patience: 100000000.0\n",
      "      attack_patience: 100000000.0\n",
      "      adv_check: 0\n",
      "      split_partitions: 2\n",
      "      sb_margin_weight: 1.0\n",
      "      sb_sum: false\n",
      "      bf_backup_thresh: -1\n",
      "      bf_rhs_offset: 0\n",
      "      bf_iters: 1000000000.0\n",
      "      bf_batch_size: 100000\n",
      "      bf_zero_crossing_score: false\n",
      "      touch_zero_score: 0\n",
      "      ibp_enhancement: false\n",
      "      catch_assertion: false\n",
      "      compare_with_old_bounds: false\n",
      "      update_rhs_with_attack: false\n",
      "      sb_coeff_thresh: 0.001\n",
      "      sort_index: null\n",
      "      sort_descending: true\n",
      "      show_progress: false\n",
      "  attack:\n",
      "    enabled: false\n",
      "    beam_candidates: 8\n",
      "    beam_depth: 7\n",
      "    max_dive_fix_ratio: 0.8\n",
      "    min_local_free_ratio: 0.2\n",
      "    mip_start_iteration: 5\n",
      "    mip_timeout: 30.0\n",
      "    adv_pool_threshold: null\n",
      "    refined_mip_attacker: false\n",
      "    refined_batch_size: null\n",
      "attack:\n",
      "  pgd_order: skip\n",
      "  pgd_steps: 100\n",
      "  pgd_restarts: 30\n",
      "  pgd_batch_size: 100000000\n",
      "  pgd_early_stop: true\n",
      "  pgd_lr_decay: 0.99\n",
      "  pgd_alpha: auto\n",
      "  pgd_alpha_scale: false\n",
      "  pgd_loss_mode: null\n",
      "  enable_mip_attack: false\n",
      "  adv_saver: default_adv_saver\n",
      "  early_stop_condition: default_early_stop_condition\n",
      "  adv_example_finalizer: default_adv_example_finalizer\n",
      "  pgd_loss: default_pgd_loss\n",
      "  cex_path: ./test_cex.txt\n",
      "  attack_mode: PGD\n",
      "  attack_tolerance: 0.0\n",
      "  attack_func: attack_with_general_specs\n",
      "  gama_lambda: 10.0\n",
      "  gama_decay: 0.9\n",
      "  check_clean: false\n",
      "  input_split:\n",
      "    pgd_steps: 100\n",
      "    pgd_restarts: 30\n",
      "    pgd_alpha: auto\n",
      "  input_split_enhanced:\n",
      "    pgd_steps: 200\n",
      "    pgd_restarts: 500000\n",
      "    pgd_alpha: auto\n",
      "  input_split_check_adv:\n",
      "    pgd_steps: 5\n",
      "    pgd_restarts: 5\n",
      "    pgd_alpha: auto\n",
      "    max_num_domains: 10\n",
      "debug:\n",
      "  view_model: false\n",
      "  lp_test: null\n",
      "  rescale_vnnlib_ptb: null\n",
      "  test_optimized_bounds: false\n",
      "  test_optimized_bounds_after_n_iterations: 0\n",
      "  print_verbose_decisions: false\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(config='./configs/mpv.yaml', device='cuda', seed=100, conv_mode='patches', deterministic=False, double_fp=False, loss_reduction_func='sum', no_sparse_alpha=True, no_sparse_interm=True, save_adv_example=False, eval_adv_example=False, show_adv_example=False, precompile_jit=False, complete_verifier='bab', incomplete=True, csv_name=None, results_file='out.txt', root_path='', deterministic_opt=False, graph_optimizer='Customized(\"custom_graph_optimizer\", \"default_optimizer\")', buffer_has_batchdim=False, save_output=False, output_file='out.pkl', return_optimized_model=False, model=None, load_model=None, onnx_path=None, onnx_path_prefix='', cache_onnx_conversion=False, debug_onnx=False, onnx_quirks=None, input_shape=None, onnx_loader='default_onnx_and_vnnlib_loader', onnx_optimization_flags='none', onnx_vnnlib_joint_optimization_flags='none', check_optmized=False, flatten_final_output=False, optimize_graph=None, model_with_jacobian=False, start=0, end=10000, select_instance=None, num_outputs=10, mean=0.0, std=1.0, pkl_path=None, dataset=None, filter_path=None, data_idx_file=None, spec_type='lp', robustness_type='verified-acc', norm=inf, epsilon=None, epsilon_min=0.0, vnnlib_path=None, vnnlib_path_prefix='', rhs_offset=None, batch_size=64, auto_enlarge_batch_size=False, min_batch_size_ratio=0.1, use_float64_in_last_iteration=False, early_stop_patience=10, start_save_best=0.5, bound_prop_method='alpha-crown', init_bound_prop_method='same', prune_after_crown=False, optimize_disjuncts_separately=False, crown_batch_size=1000000000, max_crown_size=1000000000, relu_option='adaptive', alpha=True, lr_init_alpha=0.1, init_iteration=100, share_alphas=False, alpha_lr_decay=0.98, full_conv_alpha=True, max_coeff_mul=inf, matmul_share_alphas=False, disable_optimization=[], apply_output_constraints_to=[], tighten_input_bounds=False, best_of_oc_and_no_oc=False, directly_optimize=[], oc_lr=0.1, share_gammas=False, lr_alpha=0.01, lr_beta=0.05, lr_decay=0.98, optimizer='adam', iteration=50, beta=True, beta_warmup=True, enable_opt_interm_bounds=False, all_node_split_LP=False, forward_refine=False, dynamic_forward=False, forward_max_dim=10000, reset_start_layer_threshold=1.0, label_batch_size=32, skip_with_refined_bound=True, mip_multi_proc=None, mip_threads=1, mip_perneuron_refine_timeout=15, mip_refine_timeout=0.8, mip_early_stop=True, adv_warmup=True, mip_solver='gurobi', unsafe=False, initial_max_domains=1, max_domains=inf, decision_thresh=0, timeout=360, timeout_scale=1, max_iterations=-1, override_timeout=None, get_upper_bound=False, pruning_in_iteration=True, pruning_in_iteration_ratio=0.2, sort_targets=False, batched_domain_list=True, optimized_interm='', interm_transfer=True, recompute_interm=False, sort_domain_interval=-1, vanilla_crown_bab=False, enable_cut=False, enable_implication=False, enable_bab_cut=False, enable_lp_cut=False, cut_method=None, lr_cuts=0.01, cut_lr_decay=1.0, cut_iteration=100, cut_bab_iteration=-1, cut_early_stop_patience=-1, cut_lr_beta=0.02, number_cuts=50, topk_implication=1000, batch_size_primal=100, cut_max_num=1000000000, enable_patches_cut=False, cplex_cuts=False, cplex_cuts_wait=0, no_cplex_cuts_revpickup=True, no_cut_reference_bounds=True, fix_cut_intermediate_bounds=False, branching_method='kfsb', branching_candidates=3, branching_reduceop='min', enable_intermediate_bound_opt=False, branching_input_and_activation=False, branching_input_and_activation_order=['input', 'relu'], branching_input_iterations=30, branching_relu_iterations=50, nonlinear_split_method='shortcut', branching_point_method='uniform', nonlinear_branches=2, nonlinear_split_filtering=False, nonlinear_split_filtering_beta=False, filter_batch_size=10000, filter_iterations=25, nonlinear_split_use_min=False, loose_tanh_threshold=None, dynamic_nonlinear_split_bbps=False, dynamic_nonlinear_split_options=['uniform', 'three_left', 'three_right'], enable_input_split=False, enhanced_bound_prop_method='alpha-crown', enhanced_branching_method='naive', input_split_enhanced_bound_patience=100000000.0, input_split_attack_patience=100000000.0, input_split_adv_check=0, input_split_partitions=2, sb_margin_weight=1.0, sb_sum=False, bf_backup_thresh=-1, bf_rhs_offset=0, bf_iters=1000000000.0, bf_batch_size=100000, bf_zero_crossing_score=False, touch_zero_score=0, ibp_enhancement=False, catch_assertion=False, compare_input_split_with_old_bounds=False, input_split_update_rhs_with_attack=False, sb_coeff_thresh=0.001, input_split_sort_index=None, input_split_sort_descending=True, input_split_show_progress=False, enable_bab_attack=False, beam_candidates_number=8, beam_split_depth=7, max_dive_fix_ratio=0.8, min_local_free_ratio=0.2, submip_start_iteration=5, submip_timeout=30.0, adv_pool_threshold=None, refined_mip_attacker=False, refined_batch_size=None, pgd_order='before', pgd_steps=100, pgd_restarts=30, pgd_batch_size=100000000, pgd_early_stop=True, pgd_lr_decay=0.99, pgd_alpha='auto', pgd_alpha_scale=False, pgd_loss_mode=None, mip_attack=False, adv_saver='default_adv_saver', early_stop_condition='default_early_stop_condition', adv_example_finalizer='default_adv_example_finalizer', pgd_loss='default_pgd_loss', cex_path='./test_cex.txt', attack_mode='PGD', attack_tolerance=0.0, attack_func='attack_with_general_specs', attack_gama_lambda=10.0, attack_gama_decay=0.9, check_clean=False, input_split_pgd_steps=100, input_split_pgd_restarts=30, input_split_pgd_alpha='auto', input_split_enhanced_pgd_steps=200, input_split_enhanced_pgd_restarts=500000, input_split_enhanced_pgd_alpha='auto', input_split_check_adv_pgd_steps=5, input_split_check_adv_pgd_restarts=5, input_split_check_adv_pgd_alpha='auto', input_split_check_adv_max_num_domains=10, view_model=False, lp_test=None, rescale_vnnlib_ptb=None, test_optimized_bounds=False, test_optimized_bounds_after_n_iterations=0, print_verbose_decisions=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigHandler()\n",
    "config.parse_config([\"--config=./configs/mpv.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "\n",
      "general:\n",
      "  device: cpu\n",
      "  seed: 100\n",
      "  conv_mode: patches\n",
      "  deterministic: false\n",
      "  double_fp: true\n",
      "  loss_reduction_func: sum\n",
      "  sparse_alpha: true\n",
      "  sparse_interm: true\n",
      "  save_adv_example: false\n",
      "  eval_adv_example: false\n",
      "  show_adv_example: false\n",
      "  precompile_jit: false\n",
      "  complete_verifier: bab\n",
      "  enable_incomplete_verification: true\n",
      "  csv_name: specs/mpv2x2.csv\n",
      "  results_file: mpv2x2_6x50_results.txt\n",
      "  root_path: ../jupyter/verification\n",
      "  deterministic_opt: false\n",
      "  graph_optimizer: 'Customized(\"custom_graph_optimizer\", \"default_optimizer\")'\n",
      "  buffer_has_batchdim: false\n",
      "  save_output: true\n",
      "  output_file: mpv2x2_6x50_out.pkl\n",
      "  return_optimized_model: false\n",
      "model:\n",
      "  name: null\n",
      "  path: null\n",
      "  onnx_path: null\n",
      "  onnx_path_prefix: ''\n",
      "  cache_onnx_conversion: false\n",
      "  debug_onnx: false\n",
      "  onnx_quirks: null\n",
      "  input_shape: [-1, 4, 2, 2]\n",
      "  onnx_loader: default_onnx_and_vnnlib_loader\n",
      "  onnx_optimization_flags: none\n",
      "  onnx_vnnlib_joint_optimization_flags: none\n",
      "  check_optmized: false\n",
      "  flatten_final_output: false\n",
      "  optimize_graph: null\n",
      "  with_jacobian: false\n",
      "data:\n",
      "  start: 0\n",
      "  end: 10000\n",
      "  select_instance: null\n",
      "  num_outputs: 10\n",
      "  mean: 0.0\n",
      "  std: 1.0\n",
      "  pkl_path: null\n",
      "  dataset: null\n",
      "  data_filter_path: null\n",
      "  data_idx_file: null\n",
      "specification:\n",
      "  type: lp\n",
      "  robustness_type: verified-acc\n",
      "  norm: .inf\n",
      "  epsilon: null\n",
      "  epsilon_min: 0.0\n",
      "  vnnlib_path: null\n",
      "  vnnlib_path_prefix: ''\n",
      "  rhs_offset: null\n",
      "solver:\n",
      "  batch_size: 64\n",
      "  auto_enlarge_batch_size: false\n",
      "  min_batch_size_ratio: 0.1\n",
      "  use_float64_in_last_iteration: false\n",
      "  early_stop_patience: 10\n",
      "  start_save_best: 0.5\n",
      "  bound_prop_method: alpha-crown\n",
      "  init_bound_prop_method: same\n",
      "  prune_after_crown: false\n",
      "  optimize_disjuncts_separately: false\n",
      "  crown:\n",
      "    batch_size: 1000000000\n",
      "    max_crown_size: 1000000000\n",
      "    relu_option: adaptive\n",
      "  alpha-crown:\n",
      "    alpha: true\n",
      "    lr_alpha: 0.1\n",
      "    iteration: 100\n",
      "    share_alphas: true\n",
      "    lr_decay: 0.98\n",
      "    full_conv_alpha: true\n",
      "    max_coeff_mul: .inf\n",
      "    matmul_share_alphas: false\n",
      "    disable_optimization: []\n",
      "  invprop:\n",
      "    apply_output_constraints_to: []\n",
      "    tighten_input_bounds: false\n",
      "    best_of_oc_and_no_oc: false\n",
      "    directly_optimize: []\n",
      "    oc_lr: 0.1\n",
      "    share_gammas: false\n",
      "  beta-crown:\n",
      "    lr_alpha: 0.01\n",
      "    lr_beta: 0.05\n",
      "    lr_decay: 0.98\n",
      "    optimizer: adam\n",
      "    iteration: 50\n",
      "    beta: true\n",
      "    beta_warmup: true\n",
      "    enable_opt_interm_bounds: false\n",
      "    all_node_split_LP: false\n",
      "  forward:\n",
      "    refine: false\n",
      "    dynamic: false\n",
      "    max_dim: 10000\n",
      "    reset_threshold: 1.0\n",
      "  multi_class:\n",
      "    label_batch_size: 32\n",
      "    skip_with_refined_bound: true\n",
      "  mip:\n",
      "    parallel_solvers: null\n",
      "    solver_threads: 1\n",
      "    refine_neuron_timeout: 15\n",
      "    refine_neuron_time_percentage: 0.8\n",
      "    early_stop: true\n",
      "    adv_warmup: true\n",
      "    mip_solver: gurobi\n",
      "    skip_unsafe: false\n",
      "bab:\n",
      "  initial_max_domains: 1\n",
      "  max_domains: .inf\n",
      "  decision_thresh: 0\n",
      "  timeout: 360\n",
      "  timeout_scale: 1\n",
      "  max_iterations: -1\n",
      "  override_timeout: null\n",
      "  get_upper_bound: false\n",
      "  pruning_in_iteration: true\n",
      "  pruning_in_iteration_ratio: 0.2\n",
      "  sort_targets: false\n",
      "  batched_domain_list: true\n",
      "  optimized_interm: ''\n",
      "  interm_transfer: true\n",
      "  recompute_interm: false\n",
      "  sort_domain_interval: -1\n",
      "  vanilla_crown: false\n",
      "  cut:\n",
      "    enabled: false\n",
      "    implication: false\n",
      "    bab_cut: false\n",
      "    lp_cut: false\n",
      "    method: null\n",
      "    lr: 0.01\n",
      "    lr_decay: 1.0\n",
      "    iteration: 100\n",
      "    bab_iteration: -1\n",
      "    early_stop_patience: -1\n",
      "    lr_beta: 0.02\n",
      "    number_cuts: 50\n",
      "    topk_cuts_in_filter: 1000\n",
      "    batch_size_primal: 100\n",
      "    max_num: 1000000000\n",
      "    patches_cut: false\n",
      "    cplex_cuts: false\n",
      "    cplex_cuts_wait: 0\n",
      "    cplex_cuts_revpickup: true\n",
      "    cut_reference_bounds: true\n",
      "    fix_intermediate_bounds: false\n",
      "  branching:\n",
      "    method: kfsb\n",
      "    candidates: 3\n",
      "    reduceop: min\n",
      "    enable_intermediate_bound_opt: false\n",
      "    branching_input_and_activation: false\n",
      "    branching_input_and_activation_order: [input, relu]\n",
      "    branching_input_iterations: 30\n",
      "    branching_relu_iterations: 50\n",
      "    nonlinear_split:\n",
      "      method: shortcut\n",
      "      branching_point_method: uniform\n",
      "      num_branches: 2\n",
      "      filter: false\n",
      "      filter_beta: false\n",
      "      filter_batch_size: 10000\n",
      "      filter_iterations: 25\n",
      "      use_min: false\n",
      "      loose_tanh_threshold: null\n",
      "      dynamic_bbps: false\n",
      "      dynamic_options: [uniform, three_left, three_right]\n",
      "    input_split:\n",
      "      enable: false\n",
      "      enhanced_bound_prop_method: alpha-crown\n",
      "      enhanced_branching_method: naive\n",
      "      enhanced_bound_patience: 100000000.0\n",
      "      attack_patience: 100000000.0\n",
      "      adv_check: 0\n",
      "      split_partitions: 2\n",
      "      sb_margin_weight: 1.0\n",
      "      sb_sum: false\n",
      "      bf_backup_thresh: -1\n",
      "      bf_rhs_offset: 0\n",
      "      bf_iters: 1000000000.0\n",
      "      bf_batch_size: 100000\n",
      "      bf_zero_crossing_score: false\n",
      "      touch_zero_score: 0\n",
      "      ibp_enhancement: false\n",
      "      catch_assertion: false\n",
      "      compare_with_old_bounds: false\n",
      "      update_rhs_with_attack: false\n",
      "      sb_coeff_thresh: 0.001\n",
      "      sort_index: null\n",
      "      sort_descending: true\n",
      "      show_progress: false\n",
      "  attack:\n",
      "    enabled: false\n",
      "    beam_candidates: 8\n",
      "    beam_depth: 7\n",
      "    max_dive_fix_ratio: 0.8\n",
      "    min_local_free_ratio: 0.2\n",
      "    mip_start_iteration: 5\n",
      "    mip_timeout: 30.0\n",
      "    adv_pool_threshold: null\n",
      "    refined_mip_attacker: false\n",
      "    refined_batch_size: null\n",
      "attack:\n",
      "  pgd_order: skip\n",
      "  pgd_steps: 100\n",
      "  pgd_restarts: 30\n",
      "  pgd_batch_size: 100000000\n",
      "  pgd_early_stop: true\n",
      "  pgd_lr_decay: 0.99\n",
      "  pgd_alpha: auto\n",
      "  pgd_alpha_scale: false\n",
      "  pgd_loss_mode: null\n",
      "  enable_mip_attack: false\n",
      "  adv_saver: default_adv_saver\n",
      "  early_stop_condition: default_early_stop_condition\n",
      "  adv_example_finalizer: default_adv_example_finalizer\n",
      "  pgd_loss: default_pgd_loss\n",
      "  cex_path: ./test_cex.txt\n",
      "  attack_mode: PGD\n",
      "  attack_tolerance: 0.0\n",
      "  attack_func: attack_with_general_specs\n",
      "  gama_lambda: 10.0\n",
      "  gama_decay: 0.9\n",
      "  check_clean: false\n",
      "  input_split:\n",
      "    pgd_steps: 100\n",
      "    pgd_restarts: 30\n",
      "    pgd_alpha: auto\n",
      "  input_split_enhanced:\n",
      "    pgd_steps: 200\n",
      "    pgd_restarts: 500000\n",
      "    pgd_alpha: auto\n",
      "  input_split_check_adv:\n",
      "    pgd_steps: 5\n",
      "    pgd_restarts: 5\n",
      "    pgd_alpha: auto\n",
      "    max_num_domains: 10\n",
      "debug:\n",
      "  view_model: false\n",
      "  lp_test: null\n",
      "  rescale_vnnlib_ptb: null\n",
      "  test_optimized_bounds: false\n",
      "  test_optimized_bounds_after_n_iterations: 0\n",
      "  print_verbose_decisions: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verifier = ABCROWN([\"--config=./verification/mpv.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments at Mon Feb 10 16:34:48 2025 on polyphem\n",
      "no customized start/end sample, testing all samples in specs/mpv2x2.csv\n",
      "Internal results will be saved to mpv2x2_6x50_results.txt.\n",
      "\n",
      " %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Using onnx ./models/mpv6.onnx\n",
      "Using vnnlib ./specs/mpv2x2.vnnlib\n",
      "16 inputs and 4 outputs in vnnlib\n",
      "Loading onnx ../jupyter/verification/./models/mpv6.onnx wih quirks {}\n",
      "\n",
      "*************Error traceback*************\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/load_model.py\", line 191, in load_model_onnx\n",
      "    output_onnx = inference_onnx(path, x.numpy())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/load_model.py\", line 114, in inference_onnx\n",
      "    res = sess.run(None, {sess.get_inputs()[0].name: input})[0]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 217, in run\n",
      "    return self._sess.run(output_names, input_feed, run_options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/load_model.py\", line 194, in load_model_onnx\n",
      "    output_onnx = inference_onnx(path, x.numpy().squeeze(0))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/load_model.py\", line 114, in inference_onnx\n",
      "    res = sess.run(None, {sess.get_inputs()[0].name: input})[0]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 217, in run\n",
      "    return self._sess.run(output_names, input_feed, run_options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))\n",
      "\n",
      "*****************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/convert/model.py:151: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.\n",
      "  warnings.warn(\n",
      "/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/load_model.py:200: UserWarning: Not able to check model's conversion correctness\n",
      "  warnings.warn('Not able to check model\\'s conversion correctness')\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/slice.py:73: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (steps == 1 or steps == -1) and axes == int(axes) and start == int(start) and end == int(end)\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/slice.py:73: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (steps == 1 or steps == -1) and axes == int(axes) and start == int(start) and end == int(end)\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return value.ndim == 0 or value.shape == torch.Size([1])\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif all(x == 1 for x in input[0].shape):\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/shape.py:7: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.tensor(input.shape, device=input.device)\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/reshape.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if shape[0] == 1 and len(shape) in [2, 3, 4, 5] and self.quirks.get(\"fix_batch_size\") is True:\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/reshape.py:36: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if shape[0] == 1 and len(shape) in [2, 3, 4, 5] and self.quirks.get(\"fix_batch_size\") is True:\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/reshape.py:54: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  if (torch.prod(torch.tensor(input.shape)) != torch.prod(shape) and len(input.size()) == len(shape) + 1\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/reshape.py:58: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  shape = [x if x != 0 else input.size(i) for i, x in enumerate(shape)]\n",
      "/home/philipp/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/onnx2pytorch/operations/gather.py:14: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if indices.numel() == 1 and indices == -1:\n",
      "/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../auto_LiRPA/auto_LiRPA/operators/leaf.py:192: UserWarning: The \"has_batchdim\" option for BoundBuffers is deprecated. It may be removed from the next release.\n",
      "  warnings.warn('The \"has_batchdim\" option for BoundBuffers is deprecated.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BoundedModule(\n",
      "  (/0): BoundInput(name=/0, inputs=[], perturbed=True)\n",
      "  (/15): BoundBuffers(name=/15, inputs=[], perturbed=False)\n",
      "  (/36): BoundParams(name=/36, inputs=[], perturbed=False)\n",
      "  (/37): BoundParams(name=/37, inputs=[], perturbed=False)\n",
      "  (/38): BoundParams(name=/38, inputs=[], perturbed=False)\n",
      "  (/39): BoundParams(name=/39, inputs=[], perturbed=False)\n",
      "  (/40): BoundParams(name=/40, inputs=[], perturbed=False)\n",
      "  (/41): BoundParams(name=/41, inputs=[], perturbed=False)\n",
      "  (/42): BoundParams(name=/42, inputs=[], perturbed=False)\n",
      "  (/43): BoundParams(name=/43, inputs=[], perturbed=False)\n",
      "  (/44): BoundParams(name=/44, inputs=[], perturbed=False)\n",
      "  (/45): BoundParams(name=/45, inputs=[], perturbed=False)\n",
      "  (/46): BoundParams(name=/46, inputs=[], perturbed=False)\n",
      "  (/47): BoundParams(name=/47, inputs=[], perturbed=False)\n",
      "  (/48): BoundParams(name=/48, inputs=[], perturbed=False)\n",
      "  (/49): BoundParams(name=/49, inputs=[], perturbed=False)\n",
      "  (/53): BoundBuffers(name=/53, inputs=[], perturbed=False)\n",
      "  (/57): BoundBuffers(name=/57, inputs=[], perturbed=False)\n",
      "  (/61): BoundBuffers(name=/61, inputs=[], perturbed=False)\n",
      "  (/62): BoundConstant(name=/62, value=1)\n",
      "  (/63): BoundConstant(name=/63, value=0)\n",
      "  (/64): BoundConstant(name=/64, value=3)\n",
      "  (/65): BoundConstant(name=/65, value=0)\n",
      "  (/66): BoundConstant(name=/66, value=3)\n",
      "  (/67): BoundAdd(name=/67, inputs=[/65, /66], perturbed=False)\n",
      "  (/68): BoundConstant(name=/68, value=tensor([0]))\n",
      "  (/69): BoundUnsqueeze(name=/69, inputs=[/62, /68], perturbed=False)\n",
      "  (/70): BoundConstant(name=/70, value=tensor([0]))\n",
      "  (/71): BoundUnsqueeze(name=/71, inputs=[/63, /70], perturbed=False)\n",
      "  (/72): BoundConstant(name=/72, value=tensor([0]))\n",
      "  (/73): BoundUnsqueeze(name=/73, inputs=[/67, /72], perturbed=False)\n",
      "  (/74): BoundSlice(name=/74, inputs=[/0, /71, /73, /69], perturbed=True)\n",
      "  (/75): BoundConstant(name=/75, value=0)\n",
      "  (/76): BoundConstant(name=/76, value=1)\n",
      "  (/77): BoundAdd(name=/77, inputs=[/75, /76], perturbed=False)\n",
      "  (/78): BoundConstant(name=/78, value=tensor([0]))\n",
      "  (/79): BoundUnsqueeze(name=/79, inputs=[/62, /78], perturbed=False)\n",
      "  (/80): BoundConstant(name=/80, value=tensor([0]))\n",
      "  (/81): BoundUnsqueeze(name=/81, inputs=[/63, /80], perturbed=False)\n",
      "  (/82): BoundConstant(name=/82, value=tensor([0]))\n",
      "  (/83): BoundUnsqueeze(name=/83, inputs=[/77, /82], perturbed=False)\n",
      "  (/84): BoundSlice(name=/84, inputs=[/0, /81, /83, /79], perturbed=True)\n",
      "  (/85): BoundConstant(name=/85, value=1)\n",
      "  (/86): BoundAdd(name=/86, inputs=[/85, /85], perturbed=False)\n",
      "  (/87): BoundConstant(name=/87, value=tensor([0]))\n",
      "  (/88): BoundUnsqueeze(name=/88, inputs=[/62, /87], perturbed=False)\n",
      "  (/89): BoundConstant(name=/89, value=tensor([0]))\n",
      "  (/90): BoundUnsqueeze(name=/90, inputs=[/62, /89], perturbed=False)\n",
      "  (/91): BoundConstant(name=/91, value=tensor([0]))\n",
      "  (/92): BoundUnsqueeze(name=/92, inputs=[/86, /91], perturbed=False)\n",
      "  (/93): BoundSlice(name=/93, inputs=[/0, /90, /92, /88], perturbed=True)\n",
      "  (/94): BoundConstant(name=/94, value=2)\n",
      "  (/95): BoundConstant(name=/95, value=2)\n",
      "  (/96): BoundConstant(name=/96, value=1)\n",
      "  (/97): BoundAdd(name=/97, inputs=[/95, /96], perturbed=False)\n",
      "  (/98): BoundConstant(name=/98, value=tensor([0]))\n",
      "  (/99): BoundUnsqueeze(name=/99, inputs=[/62, /98], perturbed=False)\n",
      "  (/100): BoundConstant(name=/100, value=tensor([0]))\n",
      "  (/101): BoundUnsqueeze(name=/101, inputs=[/94, /100], perturbed=False)\n",
      "  (/102): BoundConstant(name=/102, value=tensor([0]))\n",
      "  (/103): BoundUnsqueeze(name=/103, inputs=[/97, /102], perturbed=False)\n",
      "  (/104): BoundSlice(name=/104, inputs=[/0, /101, /103, /99], perturbed=True)\n",
      "  (/105): BoundConstant(name=/105, value=3)\n",
      "  (/106): BoundConstant(name=/106, value=1)\n",
      "  (/107): BoundAdd(name=/107, inputs=[/105, /106], perturbed=False)\n",
      "  (/108): BoundConstant(name=/108, value=tensor([0]))\n",
      "  (/109): BoundUnsqueeze(name=/109, inputs=[/62, /108], perturbed=False)\n",
      "  (/110): BoundConstant(name=/110, value=tensor([0]))\n",
      "  (/111): BoundUnsqueeze(name=/111, inputs=[/64, /110], perturbed=False)\n",
      "  (/112): BoundConstant(name=/112, value=tensor([0]))\n",
      "  (/113): BoundUnsqueeze(name=/113, inputs=[/107, /112], perturbed=False)\n",
      "  (/114): BoundSlice(name=/114, inputs=[/0, /111, /113, /109], perturbed=True)\n",
      "  (/115): BoundFlatten(name=/115, inputs=[/74], perturbed=True)\n",
      "  (/input): BoundLinear(name=/input, inputs=[/115, /36, /37], perturbed=True)\n",
      "  (/117): BoundRelu(name=/117, inputs=[/input], perturbed=True)\n",
      "  (/input.3): BoundLinear(name=/input.3, inputs=[/117, /38, /39], perturbed=True)\n",
      "  (/119): BoundRelu(name=/119, inputs=[/input.3], perturbed=True)\n",
      "  (/input.7): BoundLinear(name=/input.7, inputs=[/119, /40, /41], perturbed=True)\n",
      "  (/121): BoundRelu(name=/121, inputs=[/input.7], perturbed=True)\n",
      "  (/input.11): BoundLinear(name=/input.11, inputs=[/121, /42, /43], perturbed=True)\n",
      "  (/123): BoundRelu(name=/123, inputs=[/input.11], perturbed=True)\n",
      "  (/input.15): BoundLinear(name=/input.15, inputs=[/123, /44, /45], perturbed=True)\n",
      "  (/125): BoundRelu(name=/125, inputs=[/input.15], perturbed=True)\n",
      "  (/input.19): BoundLinear(name=/input.19, inputs=[/125, /46, /47], perturbed=True)\n",
      "  (/127): BoundRelu(name=/127, inputs=[/input.19], perturbed=True)\n",
      "  (/128): BoundLinear(name=/128, inputs=[/127, /48, /49], perturbed=True)\n",
      "  (/129): BoundSub(name=/129, inputs=[/93, /84], perturbed=True)\n",
      "  (/130): BoundMul(name=/130, inputs=[/129, /114], perturbed=True)\n",
      "  (/131): BoundAdd(name=/131, inputs=[/84, /130], perturbed=True)\n",
      "  (/132): BoundConstant(name=/132, inputs=[], perturbed=False)\n",
      "  (/shape): BoundConcat(name=/shape, inputs=[/132, /53], perturbed=False)\n",
      "  (/134): BoundConstant(name=/134, inputs=[], perturbed=False)\n",
      "  (/135): BoundSplit(name=/135, inputs=[/shape, /134], perturbed=False)\n",
      "  (/136): BoundSplit(name=/136, inputs=[/shape, /134], perturbed=False)\n",
      "  (/137): BoundSplit(name=/137, inputs=[/shape, /134], perturbed=False)\n",
      "  (/138): BoundConstant(name=/138, value=tensor([0]))\n",
      "  (/139): BoundSqueeze(name=/139, inputs=[/135, /138], perturbed=False)\n",
      "  (/140): BoundConstant(name=/140, value=tensor([0]))\n",
      "  (/141): BoundSqueeze(name=/141, inputs=[/136, /140], perturbed=False)\n",
      "  (/142): BoundConstant(name=/142, value=tensor([0]))\n",
      "  (/143): BoundSqueeze(name=/143, inputs=[/137, /142], perturbed=False)\n",
      "  (/144): BoundConstant(name=/144, value=tensor([0]))\n",
      "  (/145): BoundUnsqueeze(name=/145, inputs=[/139, /144], perturbed=False)\n",
      "  (/146): BoundConstant(name=/146, value=tensor([0]))\n",
      "  (/147): BoundUnsqueeze(name=/147, inputs=[/141, /146], perturbed=False)\n",
      "  (/148): BoundConstant(name=/148, value=tensor([0]))\n",
      "  (/149): BoundUnsqueeze(name=/149, inputs=[/143, /148], perturbed=False)\n",
      "  (/150): BoundConcat(name=/150, inputs=[/145, /147, /149], perturbed=False)\n",
      "  (/151): BoundReshape(name=/151, inputs=[/104, /150], perturbed=True)\n",
      "  (/152): BoundConstant(name=/152, inputs=[], perturbed=False)\n",
      "  (/shape.3): BoundConcat(name=/shape.3, inputs=[/152, /57], perturbed=False)\n",
      "  (/154): BoundConstant(name=/154, inputs=[], perturbed=False)\n",
      "  (/155): BoundSplit(name=/155, inputs=[/shape.3, /154], perturbed=False)\n",
      "  (/156): BoundSplit(name=/156, inputs=[/shape.3, /154], perturbed=False)\n",
      "  (/157): BoundSplit(name=/157, inputs=[/shape.3, /154], perturbed=False)\n",
      "  (/158): BoundConstant(name=/158, value=tensor([0]))\n",
      "  (/159): BoundSqueeze(name=/159, inputs=[/155, /158], perturbed=False)\n",
      "  (/160): BoundConstant(name=/160, value=tensor([0]))\n",
      "  (/161): BoundSqueeze(name=/161, inputs=[/156, /160], perturbed=False)\n",
      "  (/162): BoundConstant(name=/162, value=tensor([0]))\n",
      "  (/163): BoundSqueeze(name=/163, inputs=[/157, /162], perturbed=False)\n",
      "  (/164): BoundConstant(name=/164, value=tensor([0]))\n",
      "  (/165): BoundUnsqueeze(name=/165, inputs=[/159, /164], perturbed=False)\n",
      "  (/166): BoundConstant(name=/166, value=tensor([0]))\n",
      "  (/167): BoundUnsqueeze(name=/167, inputs=[/161, /166], perturbed=False)\n",
      "  (/168): BoundConstant(name=/168, value=tensor([0]))\n",
      "  (/169): BoundUnsqueeze(name=/169, inputs=[/163, /168], perturbed=False)\n",
      "  (/170): BoundConcat(name=/170, inputs=[/165, /167, /169], perturbed=False)\n",
      "  (/171): BoundReshape(name=/171, inputs=[/131, /170], perturbed=True)\n",
      "  (/172): BoundTranspose(name=/172, inputs=[/171], perturbed=True)\n",
      "  (/173): BoundMatMul(name=/173, inputs=[/151, /172], perturbed=True)\n",
      "  (/174): BoundCast(name=/174, inputs=[/15], perturbed=False)\n",
      "  (/175): BoundConstant(name=/175, value=tensor([1]))\n",
      "  (/176): BoundReshape(name=/176, inputs=[/174, /175], perturbed=False)\n",
      "  (/177): BoundGather(name=/177, inputs=[/173, /176], perturbed=True)\n",
      "  (/178): BoundConstant(name=/178, value=tensor([2]))\n",
      "  (/179): BoundSqueeze(name=/179, inputs=[/177, /178], perturbed=True)\n",
      "  (/180): BoundAdd(name=/180, inputs=[/179, /128], perturbed=True)\n",
      "  (/181): BoundConstant(name=/181, value=tensor([1]))\n",
      "  (/shape.7): BoundConcat(name=/shape.7, inputs=[/181, /61], perturbed=False)\n",
      "  (/183): BoundConstant(name=/183, inputs=[], perturbed=False)\n",
      "  (/184): BoundSplit(name=/184, inputs=[/shape.7, /183], perturbed=False)\n",
      "  (/185): BoundSplit(name=/185, inputs=[/shape.7, /183], perturbed=False)\n",
      "  (/186): BoundConstant(name=/186, value=tensor([0]))\n",
      "  (/187): BoundSqueeze(name=/187, inputs=[/184, /186], perturbed=False)\n",
      "  (/188): BoundConstant(name=/188, value=tensor([0]))\n",
      "  (/189): BoundSqueeze(name=/189, inputs=[/185, /188], perturbed=False)\n",
      "  (/190): BoundConstant(name=/190, value=tensor([0]))\n",
      "  (/191): BoundUnsqueeze(name=/191, inputs=[/187, /190], perturbed=False)\n",
      "  (/192): BoundConstant(name=/192, value=tensor([0]))\n",
      "  (/193): BoundUnsqueeze(name=/193, inputs=[/189, /192], perturbed=False)\n",
      "  (/194): BoundConcat(name=/194, inputs=[/191, /193], perturbed=False)\n",
      "  (/195): BoundReshape(name=/195, inputs=[/131, /194], perturbed=True)\n",
      "  (/196): BoundSub(name=/196, inputs=[/195, /180], perturbed=True)\n",
      ")\n",
      "Original output: tensor([[-0.02807432, -0.02807432, -0.02807432, -0.02807432]])\n",
      "Split layers:\n",
      "  BoundLinear(name=/input, inputs=[/115, /36, /37], perturbed=True): [(BoundRelu(name=/117, inputs=[/input], perturbed=True), 0)]\n",
      "  BoundLinear(name=/input.3, inputs=[/117, /38, /39], perturbed=True): [(BoundRelu(name=/119, inputs=[/input.3], perturbed=True), 0)]\n",
      "  BoundLinear(name=/input.7, inputs=[/119, /40, /41], perturbed=True): [(BoundRelu(name=/121, inputs=[/input.7], perturbed=True), 0)]\n",
      "  BoundLinear(name=/input.11, inputs=[/121, /42, /43], perturbed=True): [(BoundRelu(name=/123, inputs=[/input.11], perturbed=True), 0)]\n",
      "  BoundLinear(name=/input.15, inputs=[/123, /44, /45], perturbed=True): [(BoundRelu(name=/125, inputs=[/input.15], perturbed=True), 0)]\n",
      "  BoundLinear(name=/input.19, inputs=[/125, /46, /47], perturbed=True): [(BoundRelu(name=/127, inputs=[/input.19], perturbed=True), 0)]\n",
      "  BoundSub(name=/129, inputs=[/93, /84], perturbed=True): [(BoundMul(name=/130, inputs=[/129, /114], perturbed=True), 0)]\n",
      "  BoundSlice(name=/114, inputs=[/0, /111, /113, /109], perturbed=True): [(BoundMul(name=/130, inputs=[/129, /114], perturbed=True), 1)]\n",
      "  BoundReshape(name=/151, inputs=[/104, /150], perturbed=True): [(BoundMatMul(name=/173, inputs=[/151, /172], perturbed=True), 0)]\n",
      "  BoundTranspose(name=/172, inputs=[/171], perturbed=True): [(BoundMatMul(name=/173, inputs=[/151, /172], perturbed=True), 1)]\n",
      "Nonlinear functions:\n",
      "   BoundRelu(name=/117, inputs=[/input], perturbed=True)\n",
      "   BoundRelu(name=/119, inputs=[/input.3], perturbed=True)\n",
      "   BoundRelu(name=/121, inputs=[/input.7], perturbed=True)\n",
      "   BoundRelu(name=/123, inputs=[/input.11], perturbed=True)\n",
      "   BoundRelu(name=/125, inputs=[/input.15], perturbed=True)\n",
      "   BoundRelu(name=/127, inputs=[/input.19], perturbed=True)\n",
      "   BoundMul(name=/130, inputs=[/129, /114], perturbed=True)\n",
      "   BoundMatMul(name=/173, inputs=[/151, /172], perturbed=True)\n",
      "layer /117 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /117 start_node /input.3 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /117 start_node /input.7 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /117 start_node /input.11 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /117 start_node /input.15 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /117 start_node /input.19 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /117 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "layer /119 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /119 start_node /input.7 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /119 start_node /input.11 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /119 start_node /input.15 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /119 start_node /input.19 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /119 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "layer /121 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /121 start_node /input.11 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /121 start_node /input.15 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /121 start_node /input.19 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /121 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "layer /123 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /123 start_node /input.15 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /123 start_node /input.19 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /123 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "layer /125 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /125 start_node /input.19 using full alpha [2, 1, 1, 50] with unstable size None total_size 1 output_shape 1\n",
      "layer /125 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "layer /127 using full alpha with shape torch.Size([50]); unstable size 50; total size 50 ([1, 50])\n",
      "layer /127 start_node /196 using full alpha [2, 4, 1, 50] with unstable size None total_size 4 output_shape 4\n",
      "Optimizable variables initialized.\n",
      "initial CROWN bounds: tensor([[-244.67813853, -244.67813853, -244.67813853, -244.67813853]]) None\n",
      "best_l after optimization: -454.9230923349227\n",
      "alpha/beta optimization time: 7.583201885223389\n",
      "initial alpha-crown bounds: tensor([[-113.73390617, -113.72952422, -113.72786674, -113.73179521]])\n",
      "Worst class: (+ rhs) -113.73390617049394\n",
      "Total VNNLIB file length: 4, max property batch size: 1, total number of batches: 4\n",
      "lA shape: [torch.Size([4, 1, 50]), torch.Size([4, 1, 50]), torch.Size([4, 1, 50]), torch.Size([4, 1, 50]), torch.Size([4, 1, 50]), torch.Size([4, 1, 50]), torch.Size([4, 1, 1, 2, 2]), torch.Size([4, 1, 1, 1])]\n",
      "\n",
      "Properties batch 0, size 1\n",
      "Remaining timeout: 91.9812023639679\n",
      "##### Instance 0 first 10 spec matrices: \n",
      "tensor([[[-1.,  0.,  0.,  0.]]])\n",
      "thresholds: tensor([-0.]) ######\n",
      "Remaining spec index tensor([0]) with bounds tensor([[-113.73390617]]) need to verify.\n",
      "Model prediction is: tensor([-0.02807432, -0.02807432, -0.02807432, -0.02807432])\n",
      "build_with_refined_bounds batch [1/1]\n",
      "setting alpha for layer /117 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /119 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /121 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /123 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /125 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /127 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /130 start_node /196 with alignment adjustment\n",
      "setting alpha for layer /173 start_node /196 with alignment adjustment\n",
      "all alpha initialized\n",
      "directly get lb and ub from refined bounds\n",
      "c shape: torch.Size([1, 1, 4])\n",
      "lA shapes: [torch.Size([1, 1, 50]), torch.Size([1, 1, 50]), torch.Size([1, 1, 50]), torch.Size([1, 1, 50]), torch.Size([1, 1, 50]), torch.Size([1, 1, 50]), torch.Size([1, 1, 1, 2, 2]), torch.Size([1, 1, 1, 1])]\n",
      "(alpha-)CROWN with fixed intermediate bounds: tensor([[-113.73390617]]) tensor([[inf]])\n",
      "Intermediate layers: /input,/input.3,/input.7,/input.11,/input.15,/input.19,/129,/114,/151,/172,/196\n",
      "Keeping alphas for these layers: ['/196']\n",
      "Keeping alphas for these layers: ['/196']\n",
      "Node /117 input 0: size torch.Size([50]) unstable 50\n",
      "Node /119 input 0: size torch.Size([50]) unstable 50\n",
      "Node /121 input 0: size torch.Size([50]) unstable 50\n",
      "Node /123 input 0: size torch.Size([50]) unstable 50\n",
      "Node /125 input 0: size torch.Size([50]) unstable 50\n",
      "Node /127 input 0: size torch.Size([50]) unstable 50\n",
      "Node /130 input 0: size torch.Size([1, 2, 2]) unstable 4\n",
      "Node /130 input 1: size torch.Size([1, 2, 2]) unstable 4\n",
      "Node /173 input 0: size torch.Size([1, 4]) unstable 4\n",
      "Node /173 input 1: size torch.Size([4, 1]) unstable 4\n",
      "-----------------\n",
      "# of unstable neurons: 316\n",
      "-----------------\n",
      "\n",
      "BaB round 1\n",
      "batch: 1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m verifier\u001b[38;5;241m.\u001b[39mmain()\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/abcrown.py:684\u001b[0m, in \u001b[0;36mABCROWN.main\u001b[0;34m(self, interm_bounds)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m verified_success\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m arguments\u001b[38;5;241m.\u001b[39mConfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete_verifier\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m verified_status \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown-mip\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    683\u001b[0m     batched_vnnlib \u001b[38;5;241m=\u001b[39m batch_vnnlib(vnnlib)  \u001b[38;5;66;03m# [x, [(c, rhs, y, pidx)]] in batch-wise\u001b[39;00m\n\u001b[0;32m--> 684\u001b[0m     verified_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomplete_verifier(\n\u001b[1;32m    685\u001b[0m         model_ori, model_incomplete, vnnlib, batched_vnnlib, vnnlib_shape,\n\u001b[1;32m    686\u001b[0m         new_idx, bab_ret\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mbab_ret, cplex_processes\u001b[38;5;241m=\u001b[39mcplex_processes,\n\u001b[1;32m    687\u001b[0m         timeout_threshold\u001b[38;5;241m=\u001b[39mtimeout_threshold \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mstart_time),\n\u001b[1;32m    688\u001b[0m         attack_images\u001b[38;5;241m=\u001b[39mall_adv_candidates,\n\u001b[1;32m    689\u001b[0m         attack_margins\u001b[38;5;241m=\u001b[39mattack_margins, results\u001b[38;5;241m=\u001b[39mret)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (bab_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcut\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m bab_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcut\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcplex_cuts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m model_incomplete \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    693\u001b[0m     terminate_mip_processes(mip_building_proc, cplex_processes)\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/abcrown.py:463\u001b[0m, in \u001b[0;36mABCROWN.complete_verifier\u001b[0;34m(self, model_ori, model_incomplete, vnnlib, batched_vnnlib, vnnlib_shape, index, timeout_threshold, bab_ret, cplex_processes, attack_images, attack_margins, results)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRemaining spec index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_failure_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbounds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrlb[init_failure_idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m need to verify.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    457\u001b[0m         (reference_alphas, lA_trim, x, data_min, data_max,\n\u001b[1;32m    458\u001b[0m         trimmed_lower_bounds, trimmed_upper_bounds, c) \u001b[38;5;241m=\u001b[39m prune_by_idx(\n\u001b[1;32m    459\u001b[0m             reference_alphas, init_verified_cond, final_name, lA_trim, x,\n\u001b[1;32m    460\u001b[0m             data_min, data_max, lA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    461\u001b[0m             trimmed_lower_bounds, trimmed_upper_bounds, c)\n\u001b[0;32m--> 463\u001b[0m     l, nodes, ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbab(\n\u001b[1;32m    464\u001b[0m         data\u001b[38;5;241m=\u001b[39mx, targets\u001b[38;5;241m=\u001b[39minit_failure_idx, time_stamp\u001b[38;5;241m=\u001b[39mtime_stamp,\n\u001b[1;32m    465\u001b[0m         data_ub\u001b[38;5;241m=\u001b[39mdata_max, data_lb\u001b[38;5;241m=\u001b[39mdata_min, data_dict\u001b[38;5;241m=\u001b[39mdata_dict,\n\u001b[1;32m    466\u001b[0m         lower_bounds\u001b[38;5;241m=\u001b[39mtrimmed_lower_bounds, upper_bounds\u001b[38;5;241m=\u001b[39mtrimmed_upper_bounds,\n\u001b[1;32m    467\u001b[0m         c\u001b[38;5;241m=\u001b[39mc, reference_alphas\u001b[38;5;241m=\u001b[39mreference_alphas, cplex_processes\u001b[38;5;241m=\u001b[39mcplex_processes,\n\u001b[1;32m    468\u001b[0m         activation_opt_params\u001b[38;5;241m=\u001b[39mresults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_opt_params\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    469\u001b[0m         refined_betas\u001b[38;5;241m=\u001b[39mresults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefined_betas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), rhs\u001b[38;5;241m=\u001b[39mrhs[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    470\u001b[0m         reference_lA\u001b[38;5;241m=\u001b[39mlA_trim, attack_images\u001b[38;5;241m=\u001b[39mthis_spec_attack_images,\n\u001b[1;32m    471\u001b[0m         model_incomplete\u001b[38;5;241m=\u001b[39mmodel_incomplete, timeout\u001b[38;5;241m=\u001b[39mtimeout, vnnlib\u001b[38;5;241m=\u001b[39mvnnlib,\n\u001b[1;32m    472\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_ori)\n\u001b[1;32m    473\u001b[0m     bab_ret\u001b[38;5;241m.\u001b[39mappend([index, \u001b[38;5;28mfloat\u001b[39m(l), nodes,\n\u001b[1;32m    474\u001b[0m                     time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time_bab,\n\u001b[1;32m    475\u001b[0m                     init_failure_idx\u001b[38;5;241m.\u001b[39mtolist()])\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/abcrown.py:294\u001b[0m, in \u001b[0;36mABCROWN.bab\u001b[0;34m(self, data_lb, data_ub, c, rhs, data, targets, vnnlib, timeout, time_stamp, data_dict, lower_bounds, upper_bounds, reference_alphas, attack_images, cplex_processes, activation_opt_params, reference_lA, model_incomplete, refined_betas, create_model, model, return_domains, max_iterations)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_domains, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_domains is only for input split for now\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 294\u001b[0m     result \u001b[38;5;241m=\u001b[39m general_bab(\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain, x,\n\u001b[1;32m    296\u001b[0m         refined_lower_bounds\u001b[38;5;241m=\u001b[39mlower_bounds, refined_upper_bounds\u001b[38;5;241m=\u001b[39mupper_bounds,\n\u001b[1;32m    297\u001b[0m         activation_opt_params\u001b[38;5;241m=\u001b[39mactivation_opt_params, reference_lA\u001b[38;5;241m=\u001b[39mreference_lA,\n\u001b[1;32m    298\u001b[0m         reference_alphas\u001b[38;5;241m=\u001b[39mreference_alphas, attack_images\u001b[38;5;241m=\u001b[39mattack_images,\n\u001b[1;32m    299\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout, max_iterations\u001b[38;5;241m=\u001b[39mmax_iterations,\n\u001b[1;32m    300\u001b[0m         refined_betas\u001b[38;5;241m=\u001b[39mrefined_betas, rhs\u001b[38;5;241m=\u001b[39mrhs,\n\u001b[1;32m    301\u001b[0m         model_incomplete\u001b[38;5;241m=\u001b[39mmodel_incomplete, time_stamp\u001b[38;5;241m=\u001b[39mtime_stamp)\n\u001b[1;32m    303\u001b[0m min_lb \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_lb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/bab.py:308\u001b[0m, in \u001b[0;36mgeneral_bab\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bab_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcut\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m bab_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcut\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcplex_cuts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    307\u001b[0m         fetch_cut_from_cplex(net)\n\u001b[0;32m--> 308\u001b[0m     global_lb \u001b[38;5;241m=\u001b[39m act_split_round(\n\u001b[1;32m    309\u001b[0m         domains, net, batch, iter_idx\u001b[38;5;241m=\u001b[39mtotal_round,\n\u001b[1;32m    310\u001b[0m         impl_params\u001b[38;5;241m=\u001b[39mimpl_params, stats\u001b[38;5;241m=\u001b[39mstats,\n\u001b[1;32m    311\u001b[0m         branching_heuristic\u001b[38;5;241m=\u001b[39mbranching_heuristic)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(global_lb, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    314\u001b[0m     global_lb \u001b[38;5;241m=\u001b[39m global_lb\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/bab.py:164\u001b[0m, in \u001b[0;36mact_split_round\u001b[0;34m(domains, net, batch, iter_idx, stats, impl_params, branching_heuristic)\u001b[0m\n\u001b[1;32m    161\u001b[0m     cplex_update_general_beta(net, d)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     split_domain(net, domains, d, batch, impl_params\u001b[38;5;241m=\u001b[39mimpl_params,\n\u001b[1;32m    165\u001b[0m                  stats\u001b[38;5;241m=\u001b[39mstats, fix_interm_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m recompute_interm,\n\u001b[1;32m    166\u001b[0m                  branching_heuristic\u001b[38;5;241m=\u001b[39mbranching_heuristic, iter_idx\u001b[38;5;241m=\u001b[39miter_idx)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of domains:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(domains))\n\u001b[1;32m    168\u001b[0m     stats\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mprint()\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/bab.py:65\u001b[0m, in \u001b[0;36msplit_domain\u001b[0;34m(net, domains, d, batch, impl_params, stats, set_init_alpha, fix_interm_bounds, branching_heuristic, iter_idx)\u001b[0m\n\u001b[1;32m     62\u001b[0m split_depth \u001b[38;5;241m=\u001b[39m get_split_depth(batch, min_batch_size)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Increase the maximum number of candidates for fsb and kfsb if there are more splits needed.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m branching_decision, branching_points, split_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 65\u001b[0m     branching_heuristic\u001b[38;5;241m.\u001b[39mget_branching_decisions(\n\u001b[1;32m     66\u001b[0m         d, split_depth, method\u001b[38;5;241m=\u001b[39mbranch_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m         branching_candidates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(branch_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m'\u001b[39m], split_depth),\n\u001b[1;32m     68\u001b[0m         branching_reduceop\u001b[38;5;241m=\u001b[39mbranch_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduceop\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     69\u001b[0m         iter_idx\u001b[38;5;241m=\u001b[39miter_idx, num_all_domains\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(domains)))\n\u001b[1;32m     70\u001b[0m print_average_branching_neurons(\n\u001b[1;32m     71\u001b[0m     branching_decision, stats\u001b[38;5;241m.\u001b[39mimplied_cuts, impl_params\u001b[38;5;241m=\u001b[39mimpl_params)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(branching_decision) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()))):\n",
      "File \u001b[0;32m~/anaconda3/envs/abcrown-fork/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/heuristics/kfsb.py:64\u001b[0m, in \u001b[0;36mKfsbBranching.get_branching_decisions\u001b[0;34m(self, domains, split_depth, branching_candidates, branching_reduceop, use_beta, keep_all_decision, prioritize_alphas, method, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbabsr_score_intercept_only(lower_bounds, upper_bounds, lAs, batch)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkfsb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     score, intercept_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbabsr_score(\n\u001b[1;32m     65\u001b[0m         lower_bounds, upper_bounds, lAs, mask, reduce_op,\n\u001b[1;32m     66\u001b[0m         number_bounds, prioritize_alphas)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported branching method \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for relu splits.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/heuristics/babsr.py:121\u001b[0m, in \u001b[0;36mBabsrBranching.babsr_score\u001b[0;34m(self, lower_bounds, upper_bounds, lAs, mask, reduce_op, number_bounds, prioritize_alphas)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# intercept_tb is a list of intercept scores, each with a array of (batch, neuron).\u001b[39;00m\n\u001b[1;32m    119\u001b[0m intercept_tb\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, adjusted_intercept_candidate)\n\u001b[0;32m--> 121\u001b[0m b_temp \u001b[38;5;241m=\u001b[39m get_preact_params(layer)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# In some cases, bias=0, we can't treat it like tensors\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b_temp, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/VerifyNN/alpha-beta-Crown-fork/jupyter/../complete_verifier/heuristics/utils.py:30\u001b[0m, in \u001b[0;36mget_preact_params\u001b[0;34m(act, zero_default)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_preact_params\u001b[39m(act, zero_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Legacy code for getting bias when there is a single input\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(act\u001b[38;5;241m.\u001b[39minputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_babsr_biases(act, zero_default)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verifier.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcrown-fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
