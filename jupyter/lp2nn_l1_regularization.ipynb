{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/VerifyNN/alpha-beta-Crown-fork/jupyter/../auto_LiRPA/auto_LiRPA/operators/gurobi_maxpool_lp.py:186: DeprecationWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import schedulefree\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../auto_LiRPA/\")\n",
    "import auto_LiRPA\n",
    "from auto_LiRPA.operators.gurobi_maxpool_lp import compute_maxpool_bias\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bounds(l, u):\n",
    "    \"\"\"\n",
    "    Takes bounds tensors and normalizes them to [0, 1], s.t. smallest lower bound is mapped to 0\n",
    "    and largest upper bound is mapped to 1\n",
    "\n",
    "    args:\n",
    "        l (batch x channels x w x h) - concrete lower bounds\n",
    "        u (batch x channels x w x h) - concrete upper bounds\n",
    "\n",
    "    returns:\n",
    "        l_norm (batch x channels x w x h) - normalized concrete lower bounds\n",
    "        u_norm (batch x channels x w x h) - normalized concrete upper bounds\n",
    "    \"\"\"\n",
    "    lmin = l.flatten(-2).min(dim=-1)[0]\n",
    "    umax = u.flatten(-2).max(dim=-1)[0]\n",
    "    lmin = lmin.unsqueeze(1)\n",
    "    umax = umax.unsqueeze(1)\n",
    "\n",
    "    l_norm = (l.flatten(-2) - lmin) / (umax - lmin)\n",
    "    u_norm = (u.flatten(-2) - lmin) / (umax - lmin)\n",
    "    l_norm = l_norm.view(l.shape)\n",
    "    u_norm = u_norm.view(u.shape)\n",
    "\n",
    "    return l_norm, u_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_lower_bound(X):\n",
    "    \"\"\"\n",
    "    Sorts tensor of shape (n_neurons, 3, w, h) by concrete lower bounds (the first channel dim).\n",
    "    \"\"\"\n",
    "    _, ind_tensor = X.flatten(-2)[:,0].sort(dim=-1)\n",
    "    ind_tensor = ind_tensor.unsqueeze(1).expand(-1, X.size(1), -1)\n",
    "\n",
    "    return torch.gather(X.flatten(-2), dim=2, index=ind_tensor).view(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_neurons, h, w):\n",
    "    # since the normalized version suffices, just stick to that\n",
    "    x1 = torch.rand(n_neurons, 1, h, w)\n",
    "    x2 = torch.rand(n_neurons, 1, h, w)\n",
    "\n",
    "    l = torch.where(x1 <= x2, x1, x2)\n",
    "    u = torch.where(x1  > x2, x1, x2)\n",
    "    l, u = normalize_bounds(l, u)\n",
    "\n",
    "    alpha = torch.rand(n_neurons, 1, h, w)\n",
    "\n",
    "    biases = compute_maxpool_bias(l, u, alpha)\n",
    "\n",
    "    return l, u, alpha, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has shape `(n_neurons, 3, h, w)` and\n",
    "- `X[:,0,:,:]` represents the lower bounds\n",
    "- `X[:,1,:,:]` represents the upper bounds\n",
    "- `X[:,2,:,:]` represents the slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(n_neurons_train, n_neurons_val, h, w, sort_by_lb=True):\n",
    "    l, u, alpha, bias = create_dataset(n_neurons_train, h, w)\n",
    "    X = torch.cat((l, u, alpha), dim=1)\n",
    "\n",
    "    if sort_by_lb:\n",
    "        X = sort_by_lower_bound(X)\n",
    "\n",
    "    dataset_train = TensorDataset(X, bias)\n",
    "\n",
    "    l, u, alpha, bias = create_dataset(n_neurons_val, h, w)\n",
    "    X = torch.cat((l, u, alpha), dim=1)\n",
    "\n",
    "    if sort_by_lb:\n",
    "        X = sort_by_lower_bound(X)\n",
    "        \n",
    "    dataset_val = TensorDataset(X, bias)\n",
    "\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small dataset to test training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 115.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 127.90it/s]\n"
     ]
    }
   ],
   "source": [
    "CREATE_DATASET = True\n",
    "\n",
    "if CREATE_DATASET:\n",
    "    ds_train, ds_val = create_tensor_dataset(100, 10, 2, 2)\n",
    "    torch.save(ds_train, './datasets/maxpool2x2_train_100.pth')\n",
    "    torch.save(ds_val, './datasets/maxpool2x2_val_100.pth')\n",
    "else:\n",
    "    ds_train = torch.load('./datasets/maxpool2x2_train_100.pth')\n",
    "    ds_val   = torch.load('./datasets/maxpool2x2_val_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(ds_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(net, train_dataloader, val_dataloader, patience=10, num_epochs=100, timeout=60, lossfun='mse', opt='adam', l1_weight=0):\n",
    "    if lossfun == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif lossfun == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    else:\n",
    "        raise ValueError('Unknown loss function!')\n",
    "    \n",
    "    if opt == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "    elif opt == 'schedulefree':\n",
    "        optimizer = schedulefree.AdamWScheduleFree(net.parameters(), lr=0.0025)\n",
    "    else:\n",
    "        raise ValueError('Uknown optimizer!')\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    train_maes = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    val_maxs = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_cnt = 0\n",
    "    t_start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        t_cur = time.time()\n",
    "        if t_cur - t_start > timeout:\n",
    "            print(f\"Timeout reached ({t_cur - t_start} sec)\")\n",
    "            break \n",
    "        \n",
    "        net.train()\n",
    "\n",
    "        if opt == 'schedulefree':\n",
    "            optimizer.train()\n",
    "\n",
    "        train_loss = 0.\n",
    "        train_mae = 0.\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            y_hat = net(batch_X)\n",
    "            loss = criterion(y_hat, batch_y)\n",
    "\n",
    "            l1_loss = 0\n",
    "            for param in net.parameters():\n",
    "                l1_loss += param.abs().sum()\n",
    "\n",
    "            loss += l1_weight * l1_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_mae += torch.abs(y_hat - batch_y).mean().item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_mae /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_maes.append(train_mae)\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        if opt == 'schedulefree':\n",
    "            optimizer.eval()\n",
    "            \n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "        val_max = torch.tensor(0)\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_dataloader:\n",
    "                y_hat = net(batch_X)\n",
    "                loss = criterion(y_hat, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += torch.abs(y_hat - batch_y).mean().item()\n",
    "                val_max = torch.maximum(val_max, torch.max(torch.abs(y_hat - batch_y)))\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_mae /= len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_maes.append(val_mae)\n",
    "        val_maxs.append(val_max.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, train_mae: {train_mae:.4f}, val_mae: {val_mae:.4f}, val_max: {val_max.item():.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_cnt = 0\n",
    "            best_net_state = net.state_dict()\n",
    "        else:\n",
    "            early_stopping_cnt += 1\n",
    "            if early_stopping_cnt >= patience:\n",
    "                print(f\"Stopping early (patience of {patience} reached)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    return train_losses, val_losses, train_maes, val_maes, val_maxs, best_net_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - train_loss: 97.5540, val_loss: 0.1034, train_mae: 0.3429, val_mae: 0.2727\n",
      "Epoch [2/1000] - train_loss: 92.3242, val_loss: 0.1031, train_mae: 0.3380, val_mae: 0.2723\n",
      "Epoch [3/1000] - train_loss: 87.2752, val_loss: 0.1027, train_mae: 0.3464, val_mae: 0.2717\n",
      "Epoch [4/1000] - train_loss: 82.3781, val_loss: 0.1023, train_mae: 0.3199, val_mae: 0.2712\n",
      "Epoch [5/1000] - train_loss: 77.6192, val_loss: 0.1020, train_mae: 0.3232, val_mae: 0.2707\n",
      "Epoch [6/1000] - train_loss: 72.9903, val_loss: 0.1017, train_mae: 0.2877, val_mae: 0.2702\n",
      "Epoch [7/1000] - train_loss: 68.5315, val_loss: 0.1014, train_mae: 0.2919, val_mae: 0.2696\n",
      "Epoch [8/1000] - train_loss: 64.2443, val_loss: 0.1010, train_mae: 0.3488, val_mae: 0.2690\n",
      "Epoch [9/1000] - train_loss: 60.0314, val_loss: 0.1007, train_mae: 0.3261, val_mae: 0.2685\n",
      "Epoch [10/1000] - train_loss: 55.9664, val_loss: 0.1005, train_mae: 0.2915, val_mae: 0.2679\n",
      "Epoch [11/1000] - train_loss: 52.0798, val_loss: 0.1002, train_mae: 0.3074, val_mae: 0.2674\n",
      "Epoch [12/1000] - train_loss: 48.3283, val_loss: 0.0999, train_mae: 0.3005, val_mae: 0.2667\n",
      "Epoch [13/1000] - train_loss: 44.7364, val_loss: 0.0996, train_mae: 0.2994, val_mae: 0.2660\n",
      "Epoch [14/1000] - train_loss: 41.3341, val_loss: 0.0993, train_mae: 0.3335, val_mae: 0.2656\n",
      "Epoch [15/1000] - train_loss: 37.9912, val_loss: 0.0991, train_mae: 0.2959, val_mae: 0.2656\n",
      "Epoch [16/1000] - train_loss: 34.8258, val_loss: 0.0989, train_mae: 0.2690, val_mae: 0.2656\n",
      "Epoch [17/1000] - train_loss: 31.8472, val_loss: 0.0988, train_mae: 0.2884, val_mae: 0.2656\n",
      "Epoch [18/1000] - train_loss: 28.9844, val_loss: 0.0987, train_mae: 0.2636, val_mae: 0.2656\n",
      "Epoch [19/1000] - train_loss: 26.2888, val_loss: 0.0986, train_mae: 0.2948, val_mae: 0.2656\n",
      "Epoch [20/1000] - train_loss: 23.7459, val_loss: 0.0985, train_mae: 0.3023, val_mae: 0.2656\n",
      "Epoch [21/1000] - train_loss: 21.3590, val_loss: 0.0984, train_mae: 0.3024, val_mae: 0.2656\n",
      "Epoch [22/1000] - train_loss: 19.1195, val_loss: 0.0983, train_mae: 0.3080, val_mae: 0.2656\n",
      "Epoch [23/1000] - train_loss: 16.9874, val_loss: 0.0982, train_mae: 0.2592, val_mae: 0.2656\n",
      "Epoch [24/1000] - train_loss: 15.0878, val_loss: 0.0982, train_mae: 0.3449, val_mae: 0.2656\n",
      "Epoch [25/1000] - train_loss: 13.2191, val_loss: 0.0982, train_mae: 0.2515, val_mae: 0.2656\n",
      "Epoch [26/1000] - train_loss: 11.6028, val_loss: 0.0981, train_mae: 0.3012, val_mae: 0.2656\n",
      "Epoch [27/1000] - train_loss: 10.0628, val_loss: 0.0981, train_mae: 0.2771, val_mae: 0.2656\n",
      "Epoch [28/1000] - train_loss: 8.7014, val_loss: 0.0981, train_mae: 0.2893, val_mae: 0.2656\n",
      "Epoch [29/1000] - train_loss: 7.4664, val_loss: 0.0981, train_mae: 0.2856, val_mae: 0.2656\n",
      "Epoch [30/1000] - train_loss: 6.4580, val_loss: 0.0982, train_mae: 0.3597, val_mae: 0.2656\n",
      "Epoch [31/1000] - train_loss: 5.5035, val_loss: 0.0982, train_mae: 0.3063, val_mae: 0.2656\n",
      "Epoch [32/1000] - train_loss: 4.7314, val_loss: 0.0983, train_mae: 0.3017, val_mae: 0.2656\n",
      "Epoch [33/1000] - train_loss: 4.0780, val_loss: 0.0983, train_mae: 0.2932, val_mae: 0.2656\n",
      "Epoch [34/1000] - train_loss: 3.6122, val_loss: 0.0984, train_mae: 0.3216, val_mae: 0.2656\n",
      "Epoch [35/1000] - train_loss: 3.2303, val_loss: 0.0985, train_mae: 0.2826, val_mae: 0.2656\n",
      "Epoch [36/1000] - train_loss: 3.0159, val_loss: 0.0986, train_mae: 0.2640, val_mae: 0.2656\n",
      "Epoch [37/1000] - train_loss: 2.9074, val_loss: 0.0987, train_mae: 0.3539, val_mae: 0.2656\n",
      "Epoch [38/1000] - train_loss: 2.6523, val_loss: 0.0988, train_mae: 0.2930, val_mae: 0.2656\n",
      "Epoch [39/1000] - train_loss: 2.4557, val_loss: 0.0989, train_mae: 0.3172, val_mae: 0.2656\n",
      "Epoch [40/1000] - train_loss: 2.3008, val_loss: 0.0991, train_mae: 0.2938, val_mae: 0.2656\n",
      "Epoch [41/1000] - train_loss: 2.1544, val_loss: 0.0992, train_mae: 0.2823, val_mae: 0.2656\n",
      "Epoch [42/1000] - train_loss: 2.0573, val_loss: 0.0994, train_mae: 0.3222, val_mae: 0.2656\n",
      "Epoch [43/1000] - train_loss: 1.9044, val_loss: 0.0995, train_mae: 0.2804, val_mae: 0.2656\n",
      "Epoch [44/1000] - train_loss: 1.8146, val_loss: 0.0995, train_mae: 0.3156, val_mae: 0.2656\n",
      "Epoch [45/1000] - train_loss: 1.6864, val_loss: 0.0996, train_mae: 0.2882, val_mae: 0.2656\n",
      "Epoch [46/1000] - train_loss: 1.5675, val_loss: 0.0997, train_mae: 0.2483, val_mae: 0.2656\n",
      "Epoch [47/1000] - train_loss: 1.5359, val_loss: 0.0997, train_mae: 0.3282, val_mae: 0.2656\n",
      "Epoch [48/1000] - train_loss: 1.3976, val_loss: 0.0997, train_mae: 0.2933, val_mae: 0.2656\n",
      "Epoch [49/1000] - train_loss: 1.3126, val_loss: 0.0998, train_mae: 0.3027, val_mae: 0.2656\n",
      "Epoch [50/1000] - train_loss: 1.2536, val_loss: 0.0999, train_mae: 0.3286, val_mae: 0.2656\n",
      "Epoch [51/1000] - train_loss: 1.1482, val_loss: 0.1001, train_mae: 0.2946, val_mae: 0.2656\n",
      "Epoch [52/1000] - train_loss: 1.0738, val_loss: 0.1002, train_mae: 0.2844, val_mae: 0.2656\n",
      "Epoch [53/1000] - train_loss: 1.0015, val_loss: 0.1003, train_mae: 0.2688, val_mae: 0.2656\n",
      "Epoch [54/1000] - train_loss: 0.9521, val_loss: 0.1004, train_mae: 0.2943, val_mae: 0.2656\n",
      "Epoch [55/1000] - train_loss: 0.8733, val_loss: 0.1004, train_mae: 0.2913, val_mae: 0.2656\n",
      "Epoch [56/1000] - train_loss: 0.8131, val_loss: 0.1004, train_mae: 0.2948, val_mae: 0.2656\n",
      "Epoch [57/1000] - train_loss: 0.7608, val_loss: 0.1004, train_mae: 0.3024, val_mae: 0.2656\n",
      "Epoch [58/1000] - train_loss: 0.7145, val_loss: 0.1005, train_mae: 0.3130, val_mae: 0.2656\n",
      "Epoch [59/1000] - train_loss: 0.6601, val_loss: 0.1006, train_mae: 0.3111, val_mae: 0.2658\n",
      "Epoch [60/1000] - train_loss: 0.6088, val_loss: 0.1008, train_mae: 0.2836, val_mae: 0.2662\n",
      "Epoch [61/1000] - train_loss: 0.5478, val_loss: 0.1009, train_mae: 0.2848, val_mae: 0.2663\n",
      "Epoch [62/1000] - train_loss: 0.4950, val_loss: 0.1010, train_mae: 0.2614, val_mae: 0.2665\n",
      "Epoch [63/1000] - train_loss: 0.4899, val_loss: 0.1011, train_mae: 0.3010, val_mae: 0.2667\n",
      "Epoch [64/1000] - train_loss: 0.4370, val_loss: 0.1013, train_mae: 0.2713, val_mae: 0.2670\n",
      "Epoch [65/1000] - train_loss: 0.4073, val_loss: 0.1014, train_mae: 0.2700, val_mae: 0.2672\n",
      "Epoch [66/1000] - train_loss: 0.3896, val_loss: 0.1014, train_mae: 0.2648, val_mae: 0.2674\n",
      "Epoch [67/1000] - train_loss: 0.4025, val_loss: 0.1016, train_mae: 0.3176, val_mae: 0.2675\n",
      "Epoch [68/1000] - train_loss: 0.3631, val_loss: 0.1017, train_mae: 0.2962, val_mae: 0.2679\n",
      "Epoch [69/1000] - train_loss: 0.3617, val_loss: 0.1019, train_mae: 0.2962, val_mae: 0.2681\n",
      "Epoch [70/1000] - train_loss: 0.3320, val_loss: 0.1020, train_mae: 0.2790, val_mae: 0.2682\n",
      "Epoch [71/1000] - train_loss: 0.3048, val_loss: 0.1020, train_mae: 0.2488, val_mae: 0.2683\n",
      "Epoch [72/1000] - train_loss: 0.3518, val_loss: 0.1021, train_mae: 0.3279, val_mae: 0.2684\n",
      "Epoch [73/1000] - train_loss: 0.3103, val_loss: 0.1022, train_mae: 0.2715, val_mae: 0.2685\n",
      "Epoch [74/1000] - train_loss: 0.3433, val_loss: 0.1023, train_mae: 0.3177, val_mae: 0.2687\n",
      "Epoch [75/1000] - train_loss: 0.3135, val_loss: 0.1024, train_mae: 0.2794, val_mae: 0.2689\n",
      "Epoch [76/1000] - train_loss: 0.3422, val_loss: 0.1025, train_mae: 0.2937, val_mae: 0.2690\n",
      "Epoch [77/1000] - train_loss: 0.2983, val_loss: 0.1025, train_mae: 0.2606, val_mae: 0.2690\n",
      "Epoch [78/1000] - train_loss: 0.3190, val_loss: 0.1024, train_mae: 0.2905, val_mae: 0.2689\n",
      "Epoch [79/1000] - train_loss: 0.3038, val_loss: 0.1023, train_mae: 0.2660, val_mae: 0.2688\n",
      "Epoch [80/1000] - train_loss: 0.3336, val_loss: 0.1022, train_mae: 0.3019, val_mae: 0.2686\n",
      "Epoch [81/1000] - train_loss: 0.2992, val_loss: 0.1021, train_mae: 0.2672, val_mae: 0.2685\n",
      "Epoch [82/1000] - train_loss: 0.3204, val_loss: 0.1021, train_mae: 0.2914, val_mae: 0.2684\n",
      "Epoch [83/1000] - train_loss: 0.3011, val_loss: 0.1020, train_mae: 0.2777, val_mae: 0.2683\n",
      "Epoch [84/1000] - train_loss: 0.3085, val_loss: 0.1020, train_mae: 0.2824, val_mae: 0.2682\n",
      "Epoch [85/1000] - train_loss: 0.3046, val_loss: 0.1020, train_mae: 0.2826, val_mae: 0.2683\n",
      "Epoch [86/1000] - train_loss: 0.3226, val_loss: 0.1021, train_mae: 0.2957, val_mae: 0.2684\n",
      "Epoch [87/1000] - train_loss: 0.3071, val_loss: 0.1022, train_mae: 0.2806, val_mae: 0.2685\n",
      "Epoch [88/1000] - train_loss: 0.3301, val_loss: 0.1022, train_mae: 0.3154, val_mae: 0.2686\n",
      "Epoch [89/1000] - train_loss: 0.3081, val_loss: 0.1023, train_mae: 0.2858, val_mae: 0.2687\n",
      "Epoch [90/1000] - train_loss: 0.3078, val_loss: 0.1023, train_mae: 0.2726, val_mae: 0.2688\n",
      "Epoch [91/1000] - train_loss: 0.3037, val_loss: 0.1024, train_mae: 0.2804, val_mae: 0.2689\n",
      "Epoch [92/1000] - train_loss: 0.2955, val_loss: 0.1025, train_mae: 0.2642, val_mae: 0.2691\n",
      "Epoch [93/1000] - train_loss: 0.2916, val_loss: 0.1026, train_mae: 0.2539, val_mae: 0.2692\n",
      "Epoch [94/1000] - train_loss: 0.3132, val_loss: 0.1026, train_mae: 0.2747, val_mae: 0.2692\n",
      "Epoch [95/1000] - train_loss: 0.2954, val_loss: 0.1026, train_mae: 0.2595, val_mae: 0.2692\n",
      "Epoch [96/1000] - train_loss: 0.2987, val_loss: 0.1026, train_mae: 0.2664, val_mae: 0.2691\n",
      "Epoch [97/1000] - train_loss: 0.3303, val_loss: 0.1025, train_mae: 0.2947, val_mae: 0.2691\n",
      "Epoch [98/1000] - train_loss: 0.3014, val_loss: 0.1025, train_mae: 0.2670, val_mae: 0.2690\n",
      "Epoch [99/1000] - train_loss: 0.3157, val_loss: 0.1025, train_mae: 0.2821, val_mae: 0.2690\n",
      "Epoch [100/1000] - train_loss: 0.3494, val_loss: 0.1025, train_mae: 0.3301, val_mae: 0.2691\n",
      "Epoch [101/1000] - train_loss: 0.3403, val_loss: 0.1026, train_mae: 0.3222, val_mae: 0.2692\n",
      "Epoch [102/1000] - train_loss: 0.3008, val_loss: 0.1027, train_mae: 0.2703, val_mae: 0.2694\n",
      "Epoch [103/1000] - train_loss: 0.3058, val_loss: 0.1028, train_mae: 0.2702, val_mae: 0.2696\n",
      "Epoch [104/1000] - train_loss: 0.3117, val_loss: 0.1030, train_mae: 0.2939, val_mae: 0.2698\n",
      "Epoch [105/1000] - train_loss: 0.2909, val_loss: 0.1032, train_mae: 0.2533, val_mae: 0.2700\n",
      "Epoch [106/1000] - train_loss: 0.2982, val_loss: 0.1032, train_mae: 0.2643, val_mae: 0.2701\n",
      "Epoch [107/1000] - train_loss: 0.3089, val_loss: 0.1033, train_mae: 0.2846, val_mae: 0.2703\n",
      "Epoch [108/1000] - train_loss: 0.3283, val_loss: 0.1035, train_mae: 0.2979, val_mae: 0.2704\n",
      "Epoch [109/1000] - train_loss: 0.3147, val_loss: 0.1036, train_mae: 0.2901, val_mae: 0.2707\n",
      "Epoch [110/1000] - train_loss: 0.2972, val_loss: 0.1038, train_mae: 0.2650, val_mae: 0.2709\n",
      "Epoch [111/1000] - train_loss: 0.3187, val_loss: 0.1039, train_mae: 0.2982, val_mae: 0.2710\n",
      "Epoch [112/1000] - train_loss: 0.2912, val_loss: 0.1040, train_mae: 0.2524, val_mae: 0.2711\n",
      "Epoch [113/1000] - train_loss: 0.2994, val_loss: 0.1040, train_mae: 0.2675, val_mae: 0.2711\n",
      "Epoch [114/1000] - train_loss: 0.3151, val_loss: 0.1040, train_mae: 0.2964, val_mae: 0.2712\n",
      "Epoch [115/1000] - train_loss: 0.3073, val_loss: 0.1041, train_mae: 0.2823, val_mae: 0.2713\n",
      "Epoch [116/1000] - train_loss: 0.3035, val_loss: 0.1042, train_mae: 0.2589, val_mae: 0.2714\n",
      "Epoch [117/1000] - train_loss: 0.2931, val_loss: 0.1043, train_mae: 0.2537, val_mae: 0.2715\n",
      "Epoch [118/1000] - train_loss: 0.2994, val_loss: 0.1043, train_mae: 0.2694, val_mae: 0.2715\n",
      "Epoch [119/1000] - train_loss: 0.3138, val_loss: 0.1043, train_mae: 0.2792, val_mae: 0.2715\n",
      "Epoch [120/1000] - train_loss: 0.3189, val_loss: 0.1042, train_mae: 0.3019, val_mae: 0.2714\n",
      "Epoch [121/1000] - train_loss: 0.2931, val_loss: 0.1043, train_mae: 0.2537, val_mae: 0.2715\n",
      "Epoch [122/1000] - train_loss: 0.3241, val_loss: 0.1043, train_mae: 0.2997, val_mae: 0.2715\n",
      "Epoch [123/1000] - train_loss: 0.3268, val_loss: 0.1042, train_mae: 0.2815, val_mae: 0.2715\n",
      "Epoch [124/1000] - train_loss: 0.3148, val_loss: 0.1042, train_mae: 0.2832, val_mae: 0.2714\n",
      "Epoch [125/1000] - train_loss: 0.3608, val_loss: 0.1041, train_mae: 0.3136, val_mae: 0.2713\n",
      "Epoch [126/1000] - train_loss: 0.2975, val_loss: 0.1040, train_mae: 0.2492, val_mae: 0.2711\n",
      "Epoch [127/1000] - train_loss: 0.3008, val_loss: 0.1039, train_mae: 0.2629, val_mae: 0.2710\n",
      "Epoch [128/1000] - train_loss: 0.2947, val_loss: 0.1038, train_mae: 0.2608, val_mae: 0.2709\n",
      "Epoch [129/1000] - train_loss: 0.2994, val_loss: 0.1037, train_mae: 0.2656, val_mae: 0.2708\n",
      "Epoch [130/1000] - train_loss: 0.2865, val_loss: 0.1037, train_mae: 0.2360, val_mae: 0.2708\n",
      "Epoch [131/1000] - train_loss: 0.3079, val_loss: 0.1037, train_mae: 0.2780, val_mae: 0.2708\n",
      "Epoch [132/1000] - train_loss: 0.2893, val_loss: 0.1037, train_mae: 0.2446, val_mae: 0.2708\n",
      "Epoch [133/1000] - train_loss: 0.3570, val_loss: 0.1037, train_mae: 0.3341, val_mae: 0.2707\n",
      "Epoch [134/1000] - train_loss: 0.3480, val_loss: 0.1035, train_mae: 0.3092, val_mae: 0.2705\n",
      "Epoch [135/1000] - train_loss: 0.3105, val_loss: 0.1033, train_mae: 0.2750, val_mae: 0.2703\n",
      "Epoch [136/1000] - train_loss: 0.3210, val_loss: 0.1032, train_mae: 0.3009, val_mae: 0.2701\n",
      "Epoch [137/1000] - train_loss: 0.2916, val_loss: 0.1033, train_mae: 0.2498, val_mae: 0.2702\n",
      "Epoch [138/1000] - train_loss: 0.3304, val_loss: 0.1033, train_mae: 0.3117, val_mae: 0.2702\n",
      "Epoch [139/1000] - train_loss: 0.2980, val_loss: 0.1033, train_mae: 0.2621, val_mae: 0.2702\n",
      "Epoch [140/1000] - train_loss: 0.3022, val_loss: 0.1033, train_mae: 0.2668, val_mae: 0.2702\n",
      "Epoch [141/1000] - train_loss: 0.3601, val_loss: 0.1034, train_mae: 0.3153, val_mae: 0.2703\n",
      "Epoch [142/1000] - train_loss: 0.3360, val_loss: 0.1034, train_mae: 0.3184, val_mae: 0.2703\n",
      "Epoch [143/1000] - train_loss: 0.3482, val_loss: 0.1033, train_mae: 0.3114, val_mae: 0.2702\n",
      "Epoch [144/1000] - train_loss: 0.3040, val_loss: 0.1032, train_mae: 0.2704, val_mae: 0.2701\n",
      "Epoch [145/1000] - train_loss: 0.3042, val_loss: 0.1032, train_mae: 0.2740, val_mae: 0.2700\n",
      "Epoch [146/1000] - train_loss: 0.2989, val_loss: 0.1032, train_mae: 0.2666, val_mae: 0.2700\n",
      "Epoch [147/1000] - train_loss: 0.3253, val_loss: 0.1032, train_mae: 0.2995, val_mae: 0.2701\n",
      "Epoch [148/1000] - train_loss: 0.2954, val_loss: 0.1033, train_mae: 0.2586, val_mae: 0.2702\n",
      "Epoch [149/1000] - train_loss: 0.2901, val_loss: 0.1033, train_mae: 0.2451, val_mae: 0.2703\n",
      "Epoch [150/1000] - train_loss: 0.3052, val_loss: 0.1034, train_mae: 0.2776, val_mae: 0.2703\n",
      "Epoch [151/1000] - train_loss: 0.2907, val_loss: 0.1034, train_mae: 0.2460, val_mae: 0.2704\n",
      "Epoch [152/1000] - train_loss: 0.3202, val_loss: 0.1035, train_mae: 0.2961, val_mae: 0.2705\n",
      "Epoch [153/1000] - train_loss: 0.3496, val_loss: 0.1037, train_mae: 0.3345, val_mae: 0.2708\n",
      "Epoch [154/1000] - train_loss: 0.3888, val_loss: 0.1038, train_mae: 0.3633, val_mae: 0.2709\n",
      "Epoch [155/1000] - train_loss: 0.3135, val_loss: 0.1039, train_mae: 0.2898, val_mae: 0.2710\n",
      "Epoch [156/1000] - train_loss: 0.3141, val_loss: 0.1040, train_mae: 0.2752, val_mae: 0.2712\n",
      "Epoch [157/1000] - train_loss: 0.3208, val_loss: 0.1040, train_mae: 0.2874, val_mae: 0.2711\n",
      "Epoch [158/1000] - train_loss: 0.3335, val_loss: 0.1039, train_mae: 0.3167, val_mae: 0.2711\n",
      "Epoch [159/1000] - train_loss: 0.2970, val_loss: 0.1040, train_mae: 0.2538, val_mae: 0.2712\n",
      "Epoch [160/1000] - train_loss: 0.3000, val_loss: 0.1041, train_mae: 0.2668, val_mae: 0.2713\n",
      "Epoch [161/1000] - train_loss: 0.3004, val_loss: 0.1042, train_mae: 0.2720, val_mae: 0.2714\n",
      "Epoch [162/1000] - train_loss: 0.2967, val_loss: 0.1043, train_mae: 0.2618, val_mae: 0.2715\n",
      "Epoch [163/1000] - train_loss: 0.3092, val_loss: 0.1043, train_mae: 0.2725, val_mae: 0.2716\n",
      "Epoch [164/1000] - train_loss: 0.3605, val_loss: 0.1044, train_mae: 0.3260, val_mae: 0.2717\n",
      "Epoch [165/1000] - train_loss: 0.3243, val_loss: 0.1044, train_mae: 0.3007, val_mae: 0.2717\n",
      "Epoch [166/1000] - train_loss: 0.3228, val_loss: 0.1045, train_mae: 0.3038, val_mae: 0.2718\n",
      "Epoch [167/1000] - train_loss: 0.3199, val_loss: 0.1046, train_mae: 0.3011, val_mae: 0.2719\n",
      "Epoch [168/1000] - train_loss: 0.3078, val_loss: 0.1047, train_mae: 0.2768, val_mae: 0.2720\n",
      "Epoch [169/1000] - train_loss: 0.2996, val_loss: 0.1047, train_mae: 0.2702, val_mae: 0.2720\n",
      "Epoch [170/1000] - train_loss: 0.2895, val_loss: 0.1047, train_mae: 0.2364, val_mae: 0.2720\n",
      "Epoch [171/1000] - train_loss: 0.3228, val_loss: 0.1047, train_mae: 0.3055, val_mae: 0.2720\n",
      "Epoch [172/1000] - train_loss: 0.3081, val_loss: 0.1048, train_mae: 0.2828, val_mae: 0.2722\n",
      "Epoch [173/1000] - train_loss: 0.3016, val_loss: 0.1049, train_mae: 0.2746, val_mae: 0.2723\n",
      "Epoch [174/1000] - train_loss: 0.2941, val_loss: 0.1050, train_mae: 0.2573, val_mae: 0.2725\n",
      "Epoch [175/1000] - train_loss: 0.3068, val_loss: 0.1051, train_mae: 0.2775, val_mae: 0.2725\n",
      "Epoch [176/1000] - train_loss: 0.3429, val_loss: 0.1050, train_mae: 0.3176, val_mae: 0.2724\n",
      "Epoch [177/1000] - train_loss: 0.3039, val_loss: 0.1049, train_mae: 0.2737, val_mae: 0.2722\n",
      "Epoch [178/1000] - train_loss: 0.2924, val_loss: 0.1047, train_mae: 0.2467, val_mae: 0.2721\n",
      "Epoch [179/1000] - train_loss: 0.3075, val_loss: 0.1047, train_mae: 0.2804, val_mae: 0.2720\n",
      "Epoch [180/1000] - train_loss: 0.3214, val_loss: 0.1045, train_mae: 0.2983, val_mae: 0.2718\n",
      "Epoch [181/1000] - train_loss: 0.3060, val_loss: 0.1043, train_mae: 0.2795, val_mae: 0.2716\n",
      "Epoch [182/1000] - train_loss: 0.3002, val_loss: 0.1041, train_mae: 0.2639, val_mae: 0.2713\n",
      "Epoch [183/1000] - train_loss: 0.3011, val_loss: 0.1040, train_mae: 0.2748, val_mae: 0.2712\n",
      "Epoch [184/1000] - train_loss: 0.3426, val_loss: 0.1039, train_mae: 0.3213, val_mae: 0.2711\n",
      "Epoch [185/1000] - train_loss: 0.3145, val_loss: 0.1039, train_mae: 0.2803, val_mae: 0.2711\n",
      "Epoch [186/1000] - train_loss: 0.3106, val_loss: 0.1039, train_mae: 0.2780, val_mae: 0.2710\n",
      "Epoch [187/1000] - train_loss: 0.3133, val_loss: 0.1037, train_mae: 0.2774, val_mae: 0.2708\n",
      "Epoch [188/1000] - train_loss: 0.3037, val_loss: 0.1035, train_mae: 0.2753, val_mae: 0.2705\n",
      "Epoch [189/1000] - train_loss: 0.3127, val_loss: 0.1035, train_mae: 0.2916, val_mae: 0.2705\n",
      "Epoch [190/1000] - train_loss: 0.2920, val_loss: 0.1036, train_mae: 0.2504, val_mae: 0.2706\n",
      "Epoch [191/1000] - train_loss: 0.2871, val_loss: 0.1036, train_mae: 0.2326, val_mae: 0.2707\n",
      "Epoch [192/1000] - train_loss: 0.2894, val_loss: 0.1036, train_mae: 0.2413, val_mae: 0.2706\n",
      "Epoch [193/1000] - train_loss: 0.3165, val_loss: 0.1036, train_mae: 0.2960, val_mae: 0.2706\n",
      "Epoch [194/1000] - train_loss: 0.3337, val_loss: 0.1037, train_mae: 0.3168, val_mae: 0.2708\n",
      "Epoch [195/1000] - train_loss: 0.3123, val_loss: 0.1039, train_mae: 0.2925, val_mae: 0.2710\n",
      "Epoch [196/1000] - train_loss: 0.3155, val_loss: 0.1041, train_mae: 0.2843, val_mae: 0.2712\n",
      "Epoch [197/1000] - train_loss: 0.3311, val_loss: 0.1042, train_mae: 0.3028, val_mae: 0.2715\n",
      "Epoch [198/1000] - train_loss: 0.3251, val_loss: 0.1043, train_mae: 0.3053, val_mae: 0.2716\n",
      "Epoch [199/1000] - train_loss: 0.3006, val_loss: 0.1045, train_mae: 0.2653, val_mae: 0.2718\n",
      "Epoch [200/1000] - train_loss: 0.3236, val_loss: 0.1046, train_mae: 0.2996, val_mae: 0.2719\n",
      "Epoch [201/1000] - train_loss: 0.3013, val_loss: 0.1046, train_mae: 0.2701, val_mae: 0.2719\n",
      "Epoch [202/1000] - train_loss: 0.3243, val_loss: 0.1046, train_mae: 0.2930, val_mae: 0.2719\n",
      "Epoch [203/1000] - train_loss: 0.3059, val_loss: 0.1045, train_mae: 0.2786, val_mae: 0.2717\n",
      "Epoch [204/1000] - train_loss: 0.2939, val_loss: 0.1044, train_mae: 0.2548, val_mae: 0.2717\n",
      "Epoch [205/1000] - train_loss: 0.3158, val_loss: 0.1045, train_mae: 0.2855, val_mae: 0.2718\n",
      "Epoch [206/1000] - train_loss: 0.2925, val_loss: 0.1045, train_mae: 0.2494, val_mae: 0.2717\n",
      "Epoch [207/1000] - train_loss: 0.3211, val_loss: 0.1045, train_mae: 0.2943, val_mae: 0.2718\n",
      "Epoch [208/1000] - train_loss: 0.3431, val_loss: 0.1044, train_mae: 0.3111, val_mae: 0.2717\n",
      "Epoch [209/1000] - train_loss: 0.3272, val_loss: 0.1043, train_mae: 0.3041, val_mae: 0.2716\n",
      "Epoch [210/1000] - train_loss: 0.3601, val_loss: 0.1043, train_mae: 0.3313, val_mae: 0.2716\n",
      "Epoch [211/1000] - train_loss: 0.3107, val_loss: 0.1044, train_mae: 0.2823, val_mae: 0.2716\n",
      "Epoch [212/1000] - train_loss: 0.2921, val_loss: 0.1044, train_mae: 0.2516, val_mae: 0.2717\n",
      "Epoch [213/1000] - train_loss: 0.2926, val_loss: 0.1045, train_mae: 0.2501, val_mae: 0.2718\n",
      "Epoch [214/1000] - train_loss: 0.3032, val_loss: 0.1046, train_mae: 0.2657, val_mae: 0.2719\n",
      "Epoch [215/1000] - train_loss: 0.2961, val_loss: 0.1047, train_mae: 0.2555, val_mae: 0.2720\n",
      "Epoch [216/1000] - train_loss: 0.2935, val_loss: 0.1047, train_mae: 0.2539, val_mae: 0.2721\n",
      "Epoch [217/1000] - train_loss: 0.3485, val_loss: 0.1048, train_mae: 0.3306, val_mae: 0.2721\n",
      "Epoch [218/1000] - train_loss: 0.3133, val_loss: 0.1047, train_mae: 0.2831, val_mae: 0.2721\n",
      "Epoch [219/1000] - train_loss: 0.3276, val_loss: 0.1046, train_mae: 0.3024, val_mae: 0.2719\n",
      "Epoch [220/1000] - train_loss: 0.3029, val_loss: 0.1045, train_mae: 0.2592, val_mae: 0.2719\n",
      "Epoch [221/1000] - train_loss: 0.3000, val_loss: 0.1046, train_mae: 0.2676, val_mae: 0.2719\n",
      "Epoch [222/1000] - train_loss: 0.2911, val_loss: 0.1045, train_mae: 0.2437, val_mae: 0.2718\n",
      "Epoch [223/1000] - train_loss: 0.2883, val_loss: 0.1045, train_mae: 0.2425, val_mae: 0.2718\n",
      "Epoch [224/1000] - train_loss: 0.3377, val_loss: 0.1045, train_mae: 0.3210, val_mae: 0.2717\n",
      "Epoch [225/1000] - train_loss: 0.3083, val_loss: 0.1045, train_mae: 0.2754, val_mae: 0.2718\n",
      "Epoch [226/1000] - train_loss: 0.2919, val_loss: 0.1044, train_mae: 0.2498, val_mae: 0.2717\n",
      "Epoch [227/1000] - train_loss: 0.3278, val_loss: 0.1044, train_mae: 0.3057, val_mae: 0.2717\n",
      "Epoch [228/1000] - train_loss: 0.2973, val_loss: 0.1045, train_mae: 0.2592, val_mae: 0.2718\n",
      "Epoch [229/1000] - train_loss: 0.3447, val_loss: 0.1046, train_mae: 0.3030, val_mae: 0.2719\n",
      "Epoch [230/1000] - train_loss: 0.3076, val_loss: 0.1045, train_mae: 0.2809, val_mae: 0.2718\n",
      "Epoch [231/1000] - train_loss: 0.3121, val_loss: 0.1044, train_mae: 0.2756, val_mae: 0.2716\n",
      "Epoch [232/1000] - train_loss: 0.3009, val_loss: 0.1042, train_mae: 0.2721, val_mae: 0.2714\n",
      "Epoch [233/1000] - train_loss: 0.2981, val_loss: 0.1040, train_mae: 0.2607, val_mae: 0.2712\n",
      "Epoch [234/1000] - train_loss: 0.2941, val_loss: 0.1039, train_mae: 0.2496, val_mae: 0.2710\n",
      "Epoch [235/1000] - train_loss: 0.2902, val_loss: 0.1037, train_mae: 0.2501, val_mae: 0.2708\n",
      "Epoch [236/1000] - train_loss: 0.2941, val_loss: 0.1036, train_mae: 0.2556, val_mae: 0.2707\n",
      "Epoch [237/1000] - train_loss: 0.3128, val_loss: 0.1036, train_mae: 0.2836, val_mae: 0.2706\n",
      "Epoch [238/1000] - train_loss: 0.3062, val_loss: 0.1035, train_mae: 0.2786, val_mae: 0.2705\n",
      "Epoch [239/1000] - train_loss: 0.3381, val_loss: 0.1035, train_mae: 0.3059, val_mae: 0.2705\n",
      "Epoch [240/1000] - train_loss: 0.2940, val_loss: 0.1035, train_mae: 0.2610, val_mae: 0.2705\n",
      "Epoch [241/1000] - train_loss: 0.2873, val_loss: 0.1035, train_mae: 0.2345, val_mae: 0.2704\n",
      "Epoch [242/1000] - train_loss: 0.2930, val_loss: 0.1035, train_mae: 0.2526, val_mae: 0.2704\n",
      "Epoch [243/1000] - train_loss: 0.2975, val_loss: 0.1034, train_mae: 0.2587, val_mae: 0.2704\n",
      "Epoch [244/1000] - train_loss: 0.3082, val_loss: 0.1035, train_mae: 0.2776, val_mae: 0.2706\n",
      "Epoch [245/1000] - train_loss: 0.3160, val_loss: 0.1037, train_mae: 0.2931, val_mae: 0.2707\n",
      "Epoch [246/1000] - train_loss: 0.3395, val_loss: 0.1037, train_mae: 0.3026, val_mae: 0.2708\n",
      "Epoch [247/1000] - train_loss: 0.3196, val_loss: 0.1036, train_mae: 0.2984, val_mae: 0.2706\n",
      "Epoch [248/1000] - train_loss: 0.3315, val_loss: 0.1035, train_mae: 0.3191, val_mae: 0.2705\n",
      "Epoch [249/1000] - train_loss: 0.3086, val_loss: 0.1035, train_mae: 0.2791, val_mae: 0.2705\n",
      "Epoch [250/1000] - train_loss: 0.3017, val_loss: 0.1037, train_mae: 0.2721, val_mae: 0.2707\n",
      "Epoch [251/1000] - train_loss: 0.3023, val_loss: 0.1037, train_mae: 0.2651, val_mae: 0.2708\n",
      "Epoch [252/1000] - train_loss: 0.3200, val_loss: 0.1038, train_mae: 0.2957, val_mae: 0.2708\n",
      "Epoch [253/1000] - train_loss: 0.3268, val_loss: 0.1039, train_mae: 0.2921, val_mae: 0.2710\n",
      "Epoch [254/1000] - train_loss: 0.3194, val_loss: 0.1041, train_mae: 0.2948, val_mae: 0.2713\n",
      "Epoch [255/1000] - train_loss: 0.3365, val_loss: 0.1042, train_mae: 0.3002, val_mae: 0.2715\n",
      "Epoch [256/1000] - train_loss: 0.2893, val_loss: 0.1042, train_mae: 0.2381, val_mae: 0.2714\n",
      "Epoch [257/1000] - train_loss: 0.3163, val_loss: 0.1042, train_mae: 0.2961, val_mae: 0.2714\n",
      "Epoch [258/1000] - train_loss: 0.3021, val_loss: 0.1042, train_mae: 0.2758, val_mae: 0.2715\n",
      "Epoch [259/1000] - train_loss: 0.2939, val_loss: 0.1042, train_mae: 0.2580, val_mae: 0.2714\n",
      "Epoch [260/1000] - train_loss: 0.3251, val_loss: 0.1041, train_mae: 0.2966, val_mae: 0.2713\n",
      "Epoch [261/1000] - train_loss: 0.3061, val_loss: 0.1039, train_mae: 0.2757, val_mae: 0.2711\n",
      "Epoch [262/1000] - train_loss: 0.3410, val_loss: 0.1039, train_mae: 0.3259, val_mae: 0.2710\n",
      "Epoch [263/1000] - train_loss: 0.3625, val_loss: 0.1039, train_mae: 0.3187, val_mae: 0.2710\n",
      "Epoch [264/1000] - train_loss: 0.3294, val_loss: 0.1037, train_mae: 0.3037, val_mae: 0.2708\n",
      "Epoch [265/1000] - train_loss: 0.3762, val_loss: 0.1037, train_mae: 0.3563, val_mae: 0.2708\n",
      "Epoch [266/1000] - train_loss: 0.3004, val_loss: 0.1037, train_mae: 0.2724, val_mae: 0.2708\n",
      "Epoch [267/1000] - train_loss: 0.2907, val_loss: 0.1037, train_mae: 0.2490, val_mae: 0.2708\n",
      "Epoch [268/1000] - train_loss: 0.3239, val_loss: 0.1037, train_mae: 0.2986, val_mae: 0.2708\n",
      "Epoch [269/1000] - train_loss: 0.2946, val_loss: 0.1038, train_mae: 0.2586, val_mae: 0.2709\n",
      "Epoch [270/1000] - train_loss: 0.3242, val_loss: 0.1039, train_mae: 0.3090, val_mae: 0.2710\n",
      "Epoch [271/1000] - train_loss: 0.2940, val_loss: 0.1040, train_mae: 0.2592, val_mae: 0.2712\n",
      "Epoch [272/1000] - train_loss: 0.3282, val_loss: 0.1040, train_mae: 0.3124, val_mae: 0.2712\n",
      "Epoch [273/1000] - train_loss: 0.3188, val_loss: 0.1041, train_mae: 0.2920, val_mae: 0.2713\n",
      "Epoch [274/1000] - train_loss: 0.3320, val_loss: 0.1043, train_mae: 0.3144, val_mae: 0.2716\n",
      "Epoch [275/1000] - train_loss: 0.3068, val_loss: 0.1045, train_mae: 0.2762, val_mae: 0.2718\n",
      "Epoch [276/1000] - train_loss: 0.3241, val_loss: 0.1048, train_mae: 0.2939, val_mae: 0.2721\n",
      "Epoch [277/1000] - train_loss: 0.3443, val_loss: 0.1048, train_mae: 0.3221, val_mae: 0.2722\n",
      "Epoch [278/1000] - train_loss: 0.3053, val_loss: 0.1049, train_mae: 0.2741, val_mae: 0.2723\n",
      "Epoch [279/1000] - train_loss: 0.3147, val_loss: 0.1051, train_mae: 0.2763, val_mae: 0.2725\n",
      "Epoch [280/1000] - train_loss: 0.3368, val_loss: 0.1050, train_mae: 0.3072, val_mae: 0.2724\n",
      "Epoch [281/1000] - train_loss: 0.3087, val_loss: 0.1049, train_mae: 0.2778, val_mae: 0.2723\n",
      "Epoch [282/1000] - train_loss: 0.3025, val_loss: 0.1047, train_mae: 0.2684, val_mae: 0.2721\n",
      "Epoch [283/1000] - train_loss: 0.2989, val_loss: 0.1046, train_mae: 0.2615, val_mae: 0.2720\n",
      "Epoch [284/1000] - train_loss: 0.2997, val_loss: 0.1045, train_mae: 0.2660, val_mae: 0.2718\n",
      "Epoch [285/1000] - train_loss: 0.2911, val_loss: 0.1045, train_mae: 0.2432, val_mae: 0.2717\n",
      "Epoch [286/1000] - train_loss: 0.2908, val_loss: 0.1044, train_mae: 0.2470, val_mae: 0.2716\n",
      "Epoch [287/1000] - train_loss: 0.3023, val_loss: 0.1043, train_mae: 0.2705, val_mae: 0.2715\n",
      "Epoch [288/1000] - train_loss: 0.3160, val_loss: 0.1043, train_mae: 0.2898, val_mae: 0.2715\n",
      "Epoch [289/1000] - train_loss: 0.3068, val_loss: 0.1043, train_mae: 0.2774, val_mae: 0.2716\n",
      "Epoch [290/1000] - train_loss: 0.3122, val_loss: 0.1045, train_mae: 0.2835, val_mae: 0.2717\n",
      "Epoch [291/1000] - train_loss: 0.3103, val_loss: 0.1046, train_mae: 0.2836, val_mae: 0.2719\n",
      "Epoch [292/1000] - train_loss: 0.3121, val_loss: 0.1045, train_mae: 0.2925, val_mae: 0.2718\n",
      "Epoch [293/1000] - train_loss: 0.2987, val_loss: 0.1044, train_mae: 0.2640, val_mae: 0.2717\n",
      "Epoch [294/1000] - train_loss: 0.3027, val_loss: 0.1043, train_mae: 0.2651, val_mae: 0.2716\n",
      "Epoch [295/1000] - train_loss: 0.3037, val_loss: 0.1043, train_mae: 0.2668, val_mae: 0.2715\n",
      "Epoch [296/1000] - train_loss: 0.3133, val_loss: 0.1042, train_mae: 0.2797, val_mae: 0.2715\n",
      "Epoch [297/1000] - train_loss: 0.3339, val_loss: 0.1041, train_mae: 0.3049, val_mae: 0.2713\n",
      "Epoch [298/1000] - train_loss: 0.3169, val_loss: 0.1041, train_mae: 0.2963, val_mae: 0.2712\n",
      "Epoch [299/1000] - train_loss: 0.3069, val_loss: 0.1041, train_mae: 0.2667, val_mae: 0.2712\n",
      "Epoch [300/1000] - train_loss: 0.3168, val_loss: 0.1041, train_mae: 0.2906, val_mae: 0.2713\n",
      "Epoch [301/1000] - train_loss: 0.2959, val_loss: 0.1041, train_mae: 0.2581, val_mae: 0.2713\n",
      "Epoch [302/1000] - train_loss: 0.2908, val_loss: 0.1042, train_mae: 0.2465, val_mae: 0.2714\n",
      "Epoch [303/1000] - train_loss: 0.3209, val_loss: 0.1042, train_mae: 0.2934, val_mae: 0.2714\n",
      "Epoch [304/1000] - train_loss: 0.2897, val_loss: 0.1044, train_mae: 0.2458, val_mae: 0.2716\n",
      "Epoch [305/1000] - train_loss: 0.2967, val_loss: 0.1045, train_mae: 0.2542, val_mae: 0.2717\n",
      "Epoch [306/1000] - train_loss: 0.3367, val_loss: 0.1045, train_mae: 0.3167, val_mae: 0.2718\n",
      "Epoch [307/1000] - train_loss: 0.3112, val_loss: 0.1047, train_mae: 0.2650, val_mae: 0.2720\n",
      "Epoch [308/1000] - train_loss: 0.3093, val_loss: 0.1048, train_mae: 0.2842, val_mae: 0.2721\n",
      "Epoch [309/1000] - train_loss: 0.2960, val_loss: 0.1049, train_mae: 0.2604, val_mae: 0.2723\n",
      "Epoch [310/1000] - train_loss: 0.2952, val_loss: 0.1050, train_mae: 0.2577, val_mae: 0.2724\n",
      "Epoch [311/1000] - train_loss: 0.3348, val_loss: 0.1050, train_mae: 0.3076, val_mae: 0.2725\n",
      "Epoch [312/1000] - train_loss: 0.3482, val_loss: 0.1051, train_mae: 0.3290, val_mae: 0.2726\n",
      "Epoch [313/1000] - train_loss: 0.3060, val_loss: 0.1052, train_mae: 0.2783, val_mae: 0.2726\n",
      "Epoch [314/1000] - train_loss: 0.3005, val_loss: 0.1052, train_mae: 0.2558, val_mae: 0.2727\n",
      "Epoch [315/1000] - train_loss: 0.3221, val_loss: 0.1053, train_mae: 0.3022, val_mae: 0.2727\n",
      "Epoch [316/1000] - train_loss: 0.2927, val_loss: 0.1054, train_mae: 0.2533, val_mae: 0.2729\n",
      "Epoch [317/1000] - train_loss: 0.3061, val_loss: 0.1055, train_mae: 0.2792, val_mae: 0.2730\n",
      "Epoch [318/1000] - train_loss: 0.3021, val_loss: 0.1055, train_mae: 0.2695, val_mae: 0.2730\n",
      "Epoch [319/1000] - train_loss: 0.2990, val_loss: 0.1054, train_mae: 0.2668, val_mae: 0.2729\n",
      "Epoch [320/1000] - train_loss: 0.2956, val_loss: 0.1053, train_mae: 0.2505, val_mae: 0.2728\n",
      "Epoch [321/1000] - train_loss: 0.2910, val_loss: 0.1053, train_mae: 0.2475, val_mae: 0.2727\n",
      "Epoch [322/1000] - train_loss: 0.2988, val_loss: 0.1052, train_mae: 0.2669, val_mae: 0.2726\n",
      "Epoch [323/1000] - train_loss: 0.2875, val_loss: 0.1051, train_mae: 0.2364, val_mae: 0.2725\n",
      "Epoch [324/1000] - train_loss: 0.3222, val_loss: 0.1050, train_mae: 0.2945, val_mae: 0.2724\n",
      "Epoch [325/1000] - train_loss: 0.3062, val_loss: 0.1050, train_mae: 0.2770, val_mae: 0.2724\n",
      "Epoch [326/1000] - train_loss: 0.3007, val_loss: 0.1051, train_mae: 0.2655, val_mae: 0.2725\n",
      "Epoch [327/1000] - train_loss: 0.3054, val_loss: 0.1052, train_mae: 0.2690, val_mae: 0.2726\n",
      "Epoch [328/1000] - train_loss: 0.3000, val_loss: 0.1051, train_mae: 0.2610, val_mae: 0.2725\n",
      "Epoch [329/1000] - train_loss: 0.2918, val_loss: 0.1051, train_mae: 0.2485, val_mae: 0.2725\n",
      "Epoch [330/1000] - train_loss: 0.3037, val_loss: 0.1052, train_mae: 0.2671, val_mae: 0.2726\n",
      "Epoch [331/1000] - train_loss: 0.3041, val_loss: 0.1052, train_mae: 0.2716, val_mae: 0.2727\n",
      "Epoch [332/1000] - train_loss: 0.3074, val_loss: 0.1053, train_mae: 0.2754, val_mae: 0.2728\n",
      "Epoch [333/1000] - train_loss: 0.3140, val_loss: 0.1055, train_mae: 0.2935, val_mae: 0.2729\n",
      "Epoch [334/1000] - train_loss: 0.3051, val_loss: 0.1056, train_mae: 0.2692, val_mae: 0.2731\n",
      "Epoch [335/1000] - train_loss: 0.3185, val_loss: 0.1057, train_mae: 0.2871, val_mae: 0.2732\n",
      "Epoch [336/1000] - train_loss: 0.2935, val_loss: 0.1057, train_mae: 0.2549, val_mae: 0.2733\n",
      "Epoch [337/1000] - train_loss: 0.2894, val_loss: 0.1058, train_mae: 0.2431, val_mae: 0.2733\n",
      "Epoch [338/1000] - train_loss: 0.3281, val_loss: 0.1058, train_mae: 0.3014, val_mae: 0.2734\n",
      "Epoch [339/1000] - train_loss: 0.3306, val_loss: 0.1060, train_mae: 0.3009, val_mae: 0.2736\n",
      "Epoch [340/1000] - train_loss: 0.3113, val_loss: 0.1063, train_mae: 0.2673, val_mae: 0.2739\n",
      "Epoch [341/1000] - train_loss: 0.3331, val_loss: 0.1063, train_mae: 0.3091, val_mae: 0.2739\n",
      "Epoch [342/1000] - train_loss: 0.3104, val_loss: 0.1065, train_mae: 0.2714, val_mae: 0.2741\n",
      "Epoch [343/1000] - train_loss: 0.3137, val_loss: 0.1066, train_mae: 0.2875, val_mae: 0.2742\n",
      "Epoch [344/1000] - train_loss: 0.3216, val_loss: 0.1067, train_mae: 0.2957, val_mae: 0.2743\n",
      "Epoch [345/1000] - train_loss: 0.3755, val_loss: 0.1067, train_mae: 0.3404, val_mae: 0.2744\n",
      "Epoch [346/1000] - train_loss: 0.2998, val_loss: 0.1066, train_mae: 0.2602, val_mae: 0.2743\n",
      "Epoch [347/1000] - train_loss: 0.3122, val_loss: 0.1065, train_mae: 0.2820, val_mae: 0.2742\n",
      "Epoch [348/1000] - train_loss: 0.3015, val_loss: 0.1066, train_mae: 0.2704, val_mae: 0.2742\n",
      "Epoch [349/1000] - train_loss: 0.3215, val_loss: 0.1066, train_mae: 0.2875, val_mae: 0.2742\n",
      "Epoch [350/1000] - train_loss: 0.2951, val_loss: 0.1067, train_mae: 0.2540, val_mae: 0.2743\n",
      "Epoch [351/1000] - train_loss: 0.3171, val_loss: 0.1066, train_mae: 0.2930, val_mae: 0.2743\n",
      "Epoch [352/1000] - train_loss: 0.3144, val_loss: 0.1066, train_mae: 0.2804, val_mae: 0.2743\n",
      "Epoch [353/1000] - train_loss: 0.3174, val_loss: 0.1067, train_mae: 0.2834, val_mae: 0.2744\n",
      "Epoch [354/1000] - train_loss: 0.3091, val_loss: 0.1067, train_mae: 0.2618, val_mae: 0.2743\n",
      "Epoch [355/1000] - train_loss: 0.3204, val_loss: 0.1067, train_mae: 0.2937, val_mae: 0.2743\n",
      "Epoch [356/1000] - train_loss: 0.3189, val_loss: 0.1067, train_mae: 0.2874, val_mae: 0.2743\n",
      "Epoch [357/1000] - train_loss: 0.3305, val_loss: 0.1067, train_mae: 0.3066, val_mae: 0.2744\n",
      "Epoch [358/1000] - train_loss: 0.3093, val_loss: 0.1067, train_mae: 0.2804, val_mae: 0.2743\n",
      "Epoch [359/1000] - train_loss: 0.3369, val_loss: 0.1066, train_mae: 0.2930, val_mae: 0.2743\n",
      "Epoch [360/1000] - train_loss: 0.3072, val_loss: 0.1066, train_mae: 0.2713, val_mae: 0.2742\n",
      "Epoch [361/1000] - train_loss: 0.2896, val_loss: 0.1066, train_mae: 0.2386, val_mae: 0.2742\n",
      "Epoch [362/1000] - train_loss: 0.3072, val_loss: 0.1065, train_mae: 0.2733, val_mae: 0.2742\n",
      "Epoch [363/1000] - train_loss: 0.2932, val_loss: 0.1065, train_mae: 0.2559, val_mae: 0.2742\n",
      "Epoch [364/1000] - train_loss: 0.3436, val_loss: 0.1065, train_mae: 0.3011, val_mae: 0.2741\n",
      "Epoch [365/1000] - train_loss: 0.3007, val_loss: 0.1063, train_mae: 0.2617, val_mae: 0.2740\n",
      "Epoch [366/1000] - train_loss: 0.2927, val_loss: 0.1063, train_mae: 0.2496, val_mae: 0.2739\n",
      "Epoch [367/1000] - train_loss: 0.2905, val_loss: 0.1062, train_mae: 0.2435, val_mae: 0.2738\n",
      "Epoch [368/1000] - train_loss: 0.3028, val_loss: 0.1061, train_mae: 0.2680, val_mae: 0.2737\n",
      "Epoch [369/1000] - train_loss: 0.3055, val_loss: 0.1060, train_mae: 0.2733, val_mae: 0.2736\n",
      "Epoch [370/1000] - train_loss: 0.3156, val_loss: 0.1059, train_mae: 0.2872, val_mae: 0.2734\n",
      "Epoch [371/1000] - train_loss: 0.3234, val_loss: 0.1058, train_mae: 0.3009, val_mae: 0.2734\n",
      "Epoch [372/1000] - train_loss: 0.3110, val_loss: 0.1059, train_mae: 0.2864, val_mae: 0.2734\n",
      "Epoch [373/1000] - train_loss: 0.3023, val_loss: 0.1059, train_mae: 0.2746, val_mae: 0.2735\n",
      "Epoch [374/1000] - train_loss: 0.3379, val_loss: 0.1059, train_mae: 0.3123, val_mae: 0.2734\n",
      "Epoch [375/1000] - train_loss: 0.3089, val_loss: 0.1057, train_mae: 0.2683, val_mae: 0.2732\n",
      "Epoch [376/1000] - train_loss: 0.2891, val_loss: 0.1056, train_mae: 0.2388, val_mae: 0.2731\n",
      "Epoch [377/1000] - train_loss: 0.3096, val_loss: 0.1054, train_mae: 0.2798, val_mae: 0.2729\n",
      "Epoch [378/1000] - train_loss: 0.3032, val_loss: 0.1054, train_mae: 0.2735, val_mae: 0.2729\n",
      "Epoch [379/1000] - train_loss: 0.3233, val_loss: 0.1053, train_mae: 0.2912, val_mae: 0.2728\n",
      "Epoch [380/1000] - train_loss: 0.3036, val_loss: 0.1052, train_mae: 0.2699, val_mae: 0.2726\n",
      "Epoch [381/1000] - train_loss: 0.3039, val_loss: 0.1051, train_mae: 0.2660, val_mae: 0.2726\n",
      "Epoch [382/1000] - train_loss: 0.3390, val_loss: 0.1052, train_mae: 0.3245, val_mae: 0.2726\n",
      "Epoch [383/1000] - train_loss: 0.3150, val_loss: 0.1053, train_mae: 0.2916, val_mae: 0.2728\n",
      "Epoch [384/1000] - train_loss: 0.2892, val_loss: 0.1054, train_mae: 0.2399, val_mae: 0.2728\n",
      "Epoch [385/1000] - train_loss: 0.3111, val_loss: 0.1053, train_mae: 0.2889, val_mae: 0.2728\n",
      "Epoch [386/1000] - train_loss: 0.3027, val_loss: 0.1052, train_mae: 0.2701, val_mae: 0.2727\n",
      "Epoch [387/1000] - train_loss: 0.3018, val_loss: 0.1052, train_mae: 0.2723, val_mae: 0.2726\n",
      "Epoch [388/1000] - train_loss: 0.2947, val_loss: 0.1051, train_mae: 0.2550, val_mae: 0.2726\n",
      "Epoch [389/1000] - train_loss: 0.3075, val_loss: 0.1050, train_mae: 0.2809, val_mae: 0.2724\n",
      "Epoch [390/1000] - train_loss: 0.2924, val_loss: 0.1050, train_mae: 0.2459, val_mae: 0.2724\n",
      "Epoch [391/1000] - train_loss: 0.3047, val_loss: 0.1049, train_mae: 0.2735, val_mae: 0.2723\n",
      "Epoch [392/1000] - train_loss: 0.2892, val_loss: 0.1049, train_mae: 0.2471, val_mae: 0.2722\n",
      "Epoch [393/1000] - train_loss: 0.3122, val_loss: 0.1048, train_mae: 0.2823, val_mae: 0.2721\n",
      "Epoch [394/1000] - train_loss: 0.3175, val_loss: 0.1047, train_mae: 0.3010, val_mae: 0.2720\n",
      "Epoch [395/1000] - train_loss: 0.3720, val_loss: 0.1047, train_mae: 0.3392, val_mae: 0.2721\n",
      "Epoch [396/1000] - train_loss: 0.3083, val_loss: 0.1047, train_mae: 0.2813, val_mae: 0.2721\n",
      "Epoch [397/1000] - train_loss: 0.3080, val_loss: 0.1048, train_mae: 0.2737, val_mae: 0.2722\n",
      "Epoch [398/1000] - train_loss: 0.2873, val_loss: 0.1048, train_mae: 0.2346, val_mae: 0.2722\n",
      "Epoch [399/1000] - train_loss: 0.3269, val_loss: 0.1048, train_mae: 0.2822, val_mae: 0.2722\n",
      "Epoch [400/1000] - train_loss: 0.3030, val_loss: 0.1046, train_mae: 0.2769, val_mae: 0.2720\n",
      "Epoch [401/1000] - train_loss: 0.2963, val_loss: 0.1045, train_mae: 0.2546, val_mae: 0.2718\n",
      "Epoch [402/1000] - train_loss: 0.3421, val_loss: 0.1045, train_mae: 0.3226, val_mae: 0.2718\n",
      "Epoch [403/1000] - train_loss: 0.3068, val_loss: 0.1046, train_mae: 0.2756, val_mae: 0.2719\n",
      "Epoch [404/1000] - train_loss: 0.3140, val_loss: 0.1046, train_mae: 0.2821, val_mae: 0.2719\n",
      "Epoch [405/1000] - train_loss: 0.2989, val_loss: 0.1045, train_mae: 0.2618, val_mae: 0.2718\n",
      "Epoch [406/1000] - train_loss: 0.2882, val_loss: 0.1044, train_mae: 0.2411, val_mae: 0.2716\n",
      "Epoch [407/1000] - train_loss: 0.3009, val_loss: 0.1043, train_mae: 0.2624, val_mae: 0.2715\n",
      "Epoch [408/1000] - train_loss: 0.3299, val_loss: 0.1042, train_mae: 0.3071, val_mae: 0.2714\n",
      "Epoch [409/1000] - train_loss: 0.2934, val_loss: 0.1042, train_mae: 0.2547, val_mae: 0.2714\n",
      "Epoch [410/1000] - train_loss: 0.3045, val_loss: 0.1043, train_mae: 0.2675, val_mae: 0.2715\n",
      "Epoch [411/1000] - train_loss: 0.3441, val_loss: 0.1043, train_mae: 0.3243, val_mae: 0.2715\n",
      "Epoch [412/1000] - train_loss: 0.3014, val_loss: 0.1043, train_mae: 0.2657, val_mae: 0.2715\n",
      "Epoch [413/1000] - train_loss: 0.3084, val_loss: 0.1044, train_mae: 0.2800, val_mae: 0.2717\n",
      "Epoch [414/1000] - train_loss: 0.3033, val_loss: 0.1046, train_mae: 0.2705, val_mae: 0.2719\n",
      "Epoch [415/1000] - train_loss: 0.3173, val_loss: 0.1047, train_mae: 0.2905, val_mae: 0.2721\n",
      "Epoch [416/1000] - train_loss: 0.3199, val_loss: 0.1048, train_mae: 0.2890, val_mae: 0.2721\n",
      "Epoch [417/1000] - train_loss: 0.3056, val_loss: 0.1047, train_mae: 0.2703, val_mae: 0.2721\n",
      "Epoch [418/1000] - train_loss: 0.3111, val_loss: 0.1047, train_mae: 0.2717, val_mae: 0.2720\n",
      "Epoch [419/1000] - train_loss: 0.3102, val_loss: 0.1045, train_mae: 0.2739, val_mae: 0.2718\n",
      "Epoch [420/1000] - train_loss: 0.2917, val_loss: 0.1043, train_mae: 0.2512, val_mae: 0.2716\n",
      "Epoch [421/1000] - train_loss: 0.3214, val_loss: 0.1042, train_mae: 0.3006, val_mae: 0.2714\n",
      "Epoch [422/1000] - train_loss: 0.3056, val_loss: 0.1040, train_mae: 0.2737, val_mae: 0.2712\n",
      "Epoch [423/1000] - train_loss: 0.3473, val_loss: 0.1039, train_mae: 0.3054, val_mae: 0.2710\n",
      "Epoch [424/1000] - train_loss: 0.3671, val_loss: 0.1036, train_mae: 0.3311, val_mae: 0.2707\n",
      "Epoch [425/1000] - train_loss: 0.3199, val_loss: 0.1033, train_mae: 0.2882, val_mae: 0.2703\n",
      "Epoch [426/1000] - train_loss: 0.3351, val_loss: 0.1031, train_mae: 0.3121, val_mae: 0.2699\n",
      "Epoch [427/1000] - train_loss: 0.2883, val_loss: 0.1030, train_mae: 0.2481, val_mae: 0.2698\n",
      "Epoch [428/1000] - train_loss: 0.3244, val_loss: 0.1029, train_mae: 0.3006, val_mae: 0.2696\n",
      "Epoch [429/1000] - train_loss: 0.3079, val_loss: 0.1027, train_mae: 0.2802, val_mae: 0.2694\n",
      "Epoch [430/1000] - train_loss: 0.2919, val_loss: 0.1027, train_mae: 0.2577, val_mae: 0.2693\n",
      "Epoch [431/1000] - train_loss: 0.3662, val_loss: 0.1026, train_mae: 0.3395, val_mae: 0.2693\n",
      "Epoch [432/1000] - train_loss: 0.3027, val_loss: 0.1026, train_mae: 0.2703, val_mae: 0.2693\n",
      "Epoch [433/1000] - train_loss: 0.3268, val_loss: 0.1027, train_mae: 0.2965, val_mae: 0.2694\n",
      "Epoch [434/1000] - train_loss: 0.3464, val_loss: 0.1028, train_mae: 0.3110, val_mae: 0.2696\n",
      "Epoch [435/1000] - train_loss: 0.3401, val_loss: 0.1028, train_mae: 0.2942, val_mae: 0.2695\n",
      "Epoch [436/1000] - train_loss: 0.3098, val_loss: 0.1028, train_mae: 0.2850, val_mae: 0.2694\n",
      "Epoch [437/1000] - train_loss: 0.3069, val_loss: 0.1028, train_mae: 0.2783, val_mae: 0.2695\n",
      "Epoch [438/1000] - train_loss: 0.3015, val_loss: 0.1029, train_mae: 0.2639, val_mae: 0.2697\n",
      "Epoch [439/1000] - train_loss: 0.3223, val_loss: 0.1030, train_mae: 0.3015, val_mae: 0.2698\n",
      "Epoch [440/1000] - train_loss: 0.3264, val_loss: 0.1032, train_mae: 0.3036, val_mae: 0.2700\n",
      "Epoch [441/1000] - train_loss: 0.2967, val_loss: 0.1033, train_mae: 0.2641, val_mae: 0.2703\n",
      "Epoch [442/1000] - train_loss: 0.2916, val_loss: 0.1034, train_mae: 0.2516, val_mae: 0.2703\n",
      "Epoch [443/1000] - train_loss: 0.3048, val_loss: 0.1033, train_mae: 0.2727, val_mae: 0.2703\n",
      "Epoch [444/1000] - train_loss: 0.3273, val_loss: 0.1033, train_mae: 0.2970, val_mae: 0.2703\n",
      "Epoch [445/1000] - train_loss: 0.3370, val_loss: 0.1033, train_mae: 0.3118, val_mae: 0.2703\n",
      "Epoch [446/1000] - train_loss: 0.3368, val_loss: 0.1034, train_mae: 0.3103, val_mae: 0.2703\n",
      "Epoch [447/1000] - train_loss: 0.3150, val_loss: 0.1034, train_mae: 0.2889, val_mae: 0.2704\n",
      "Epoch [448/1000] - train_loss: 0.2963, val_loss: 0.1034, train_mae: 0.2586, val_mae: 0.2703\n",
      "Epoch [449/1000] - train_loss: 0.3154, val_loss: 0.1034, train_mae: 0.2823, val_mae: 0.2703\n",
      "Epoch [450/1000] - train_loss: 0.3070, val_loss: 0.1033, train_mae: 0.2780, val_mae: 0.2703\n",
      "Epoch [451/1000] - train_loss: 0.3335, val_loss: 0.1034, train_mae: 0.3149, val_mae: 0.2704\n",
      "Epoch [452/1000] - train_loss: 0.3000, val_loss: 0.1035, train_mae: 0.2663, val_mae: 0.2705\n",
      "Epoch [453/1000] - train_loss: 0.3182, val_loss: 0.1036, train_mae: 0.2935, val_mae: 0.2707\n",
      "Epoch [454/1000] - train_loss: 0.3031, val_loss: 0.1038, train_mae: 0.2653, val_mae: 0.2708\n",
      "Epoch [455/1000] - train_loss: 0.3319, val_loss: 0.1039, train_mae: 0.3016, val_mae: 0.2710\n",
      "Epoch [456/1000] - train_loss: 0.3012, val_loss: 0.1041, train_mae: 0.2676, val_mae: 0.2713\n",
      "Epoch [457/1000] - train_loss: 0.3013, val_loss: 0.1043, train_mae: 0.2658, val_mae: 0.2715\n",
      "Epoch [458/1000] - train_loss: 0.2942, val_loss: 0.1045, train_mae: 0.2540, val_mae: 0.2718\n",
      "Epoch [459/1000] - train_loss: 0.2982, val_loss: 0.1046, train_mae: 0.2563, val_mae: 0.2719\n",
      "Epoch [460/1000] - train_loss: 0.3473, val_loss: 0.1046, train_mae: 0.3075, val_mae: 0.2719\n",
      "Epoch [461/1000] - train_loss: 0.3081, val_loss: 0.1045, train_mae: 0.2865, val_mae: 0.2718\n",
      "Epoch [462/1000] - train_loss: 0.3021, val_loss: 0.1045, train_mae: 0.2656, val_mae: 0.2717\n",
      "Epoch [463/1000] - train_loss: 0.3169, val_loss: 0.1045, train_mae: 0.2963, val_mae: 0.2718\n",
      "Epoch [464/1000] - train_loss: 0.3039, val_loss: 0.1046, train_mae: 0.2703, val_mae: 0.2719\n",
      "Epoch [465/1000] - train_loss: 0.2910, val_loss: 0.1047, train_mae: 0.2444, val_mae: 0.2721\n",
      "Epoch [466/1000] - train_loss: 0.3262, val_loss: 0.1048, train_mae: 0.3084, val_mae: 0.2722\n",
      "Epoch [467/1000] - train_loss: 0.3101, val_loss: 0.1049, train_mae: 0.2789, val_mae: 0.2723\n",
      "Epoch [468/1000] - train_loss: 0.3138, val_loss: 0.1051, train_mae: 0.2846, val_mae: 0.2725\n",
      "Epoch [469/1000] - train_loss: 0.2936, val_loss: 0.1052, train_mae: 0.2546, val_mae: 0.2727\n",
      "Epoch [470/1000] - train_loss: 0.2915, val_loss: 0.1052, train_mae: 0.2527, val_mae: 0.2727\n",
      "Epoch [471/1000] - train_loss: 0.3349, val_loss: 0.1052, train_mae: 0.3090, val_mae: 0.2727\n",
      "Epoch [472/1000] - train_loss: 0.2921, val_loss: 0.1054, train_mae: 0.2463, val_mae: 0.2729\n",
      "Epoch [473/1000] - train_loss: 0.3076, val_loss: 0.1054, train_mae: 0.2790, val_mae: 0.2729\n",
      "Epoch [474/1000] - train_loss: 0.3260, val_loss: 0.1055, train_mae: 0.2942, val_mae: 0.2729\n",
      "Epoch [475/1000] - train_loss: 0.2884, val_loss: 0.1054, train_mae: 0.2389, val_mae: 0.2729\n",
      "Epoch [476/1000] - train_loss: 0.3214, val_loss: 0.1053, train_mae: 0.2953, val_mae: 0.2728\n",
      "Epoch [477/1000] - train_loss: 0.3328, val_loss: 0.1052, train_mae: 0.3064, val_mae: 0.2727\n",
      "Epoch [478/1000] - train_loss: 0.3003, val_loss: 0.1051, train_mae: 0.2636, val_mae: 0.2725\n",
      "Epoch [479/1000] - train_loss: 0.3298, val_loss: 0.1050, train_mae: 0.2869, val_mae: 0.2724\n",
      "Epoch [480/1000] - train_loss: 0.3202, val_loss: 0.1049, train_mae: 0.2916, val_mae: 0.2722\n",
      "Epoch [481/1000] - train_loss: 0.3133, val_loss: 0.1049, train_mae: 0.2826, val_mae: 0.2722\n",
      "Epoch [482/1000] - train_loss: 0.2967, val_loss: 0.1049, train_mae: 0.2561, val_mae: 0.2723\n",
      "Epoch [483/1000] - train_loss: 0.2984, val_loss: 0.1049, train_mae: 0.2630, val_mae: 0.2723\n",
      "Epoch [484/1000] - train_loss: 0.3041, val_loss: 0.1049, train_mae: 0.2746, val_mae: 0.2723\n",
      "Epoch [485/1000] - train_loss: 0.3115, val_loss: 0.1049, train_mae: 0.2802, val_mae: 0.2723\n",
      "Epoch [486/1000] - train_loss: 0.2975, val_loss: 0.1049, train_mae: 0.2614, val_mae: 0.2723\n",
      "Epoch [487/1000] - train_loss: 0.2968, val_loss: 0.1049, train_mae: 0.2600, val_mae: 0.2723\n",
      "Epoch [488/1000] - train_loss: 0.3320, val_loss: 0.1049, train_mae: 0.2989, val_mae: 0.2723\n",
      "Epoch [489/1000] - train_loss: 0.2985, val_loss: 0.1049, train_mae: 0.2588, val_mae: 0.2723\n",
      "Epoch [490/1000] - train_loss: 0.3233, val_loss: 0.1049, train_mae: 0.2903, val_mae: 0.2723\n",
      "Epoch [491/1000] - train_loss: 0.3112, val_loss: 0.1050, train_mae: 0.2663, val_mae: 0.2724\n",
      "Epoch [492/1000] - train_loss: 0.3129, val_loss: 0.1051, train_mae: 0.2936, val_mae: 0.2725\n",
      "Epoch [493/1000] - train_loss: 0.3216, val_loss: 0.1052, train_mae: 0.2848, val_mae: 0.2726\n",
      "Epoch [494/1000] - train_loss: 0.3061, val_loss: 0.1052, train_mae: 0.2684, val_mae: 0.2727\n",
      "Epoch [495/1000] - train_loss: 0.3238, val_loss: 0.1052, train_mae: 0.3000, val_mae: 0.2727\n",
      "Epoch [496/1000] - train_loss: 0.3174, val_loss: 0.1050, train_mae: 0.2894, val_mae: 0.2725\n",
      "Epoch [497/1000] - train_loss: 0.3207, val_loss: 0.1048, train_mae: 0.2963, val_mae: 0.2721\n",
      "Epoch [498/1000] - train_loss: 0.3254, val_loss: 0.1046, train_mae: 0.3097, val_mae: 0.2719\n",
      "Epoch [499/1000] - train_loss: 0.3136, val_loss: 0.1045, train_mae: 0.2724, val_mae: 0.2718\n",
      "Epoch [500/1000] - train_loss: 0.2902, val_loss: 0.1043, train_mae: 0.2467, val_mae: 0.2716\n",
      "Epoch [501/1000] - train_loss: 0.2909, val_loss: 0.1043, train_mae: 0.2457, val_mae: 0.2715\n",
      "Epoch [502/1000] - train_loss: 0.2924, val_loss: 0.1042, train_mae: 0.2513, val_mae: 0.2714\n",
      "Epoch [503/1000] - train_loss: 0.3425, val_loss: 0.1042, train_mae: 0.2970, val_mae: 0.2714\n",
      "Epoch [504/1000] - train_loss: 0.2974, val_loss: 0.1041, train_mae: 0.2602, val_mae: 0.2713\n",
      "Epoch [505/1000] - train_loss: 0.2944, val_loss: 0.1040, train_mae: 0.2609, val_mae: 0.2712\n",
      "Epoch [506/1000] - train_loss: 0.3174, val_loss: 0.1038, train_mae: 0.2918, val_mae: 0.2709\n",
      "Epoch [507/1000] - train_loss: 0.2944, val_loss: 0.1037, train_mae: 0.2577, val_mae: 0.2707\n",
      "Epoch [508/1000] - train_loss: 0.2996, val_loss: 0.1035, train_mae: 0.2639, val_mae: 0.2705\n",
      "Epoch [509/1000] - train_loss: 0.3171, val_loss: 0.1035, train_mae: 0.2811, val_mae: 0.2705\n",
      "Epoch [510/1000] - train_loss: 0.3055, val_loss: 0.1036, train_mae: 0.2778, val_mae: 0.2706\n",
      "Epoch [511/1000] - train_loss: 0.3083, val_loss: 0.1038, train_mae: 0.2743, val_mae: 0.2709\n",
      "Epoch [512/1000] - train_loss: 0.3091, val_loss: 0.1039, train_mae: 0.2691, val_mae: 0.2710\n",
      "Epoch [513/1000] - train_loss: 0.3267, val_loss: 0.1039, train_mae: 0.3135, val_mae: 0.2711\n",
      "Epoch [514/1000] - train_loss: 0.2914, val_loss: 0.1039, train_mae: 0.2484, val_mae: 0.2711\n",
      "Epoch [515/1000] - train_loss: 0.2946, val_loss: 0.1039, train_mae: 0.2559, val_mae: 0.2711\n",
      "Epoch [516/1000] - train_loss: 0.3263, val_loss: 0.1039, train_mae: 0.3040, val_mae: 0.2711\n",
      "Epoch [517/1000] - train_loss: 0.3214, val_loss: 0.1038, train_mae: 0.2990, val_mae: 0.2710\n",
      "Epoch [518/1000] - train_loss: 0.3052, val_loss: 0.1039, train_mae: 0.2768, val_mae: 0.2710\n",
      "Epoch [519/1000] - train_loss: 0.2901, val_loss: 0.1040, train_mae: 0.2466, val_mae: 0.2711\n",
      "Epoch [520/1000] - train_loss: 0.2998, val_loss: 0.1040, train_mae: 0.2741, val_mae: 0.2711\n",
      "Epoch [521/1000] - train_loss: 0.2904, val_loss: 0.1040, train_mae: 0.2471, val_mae: 0.2711\n",
      "Epoch [522/1000] - train_loss: 0.3051, val_loss: 0.1039, train_mae: 0.2696, val_mae: 0.2711\n",
      "Epoch [523/1000] - train_loss: 0.3205, val_loss: 0.1039, train_mae: 0.2877, val_mae: 0.2710\n",
      "Epoch [524/1000] - train_loss: 0.2900, val_loss: 0.1038, train_mae: 0.2467, val_mae: 0.2710\n",
      "Epoch [525/1000] - train_loss: 0.3246, val_loss: 0.1038, train_mae: 0.3006, val_mae: 0.2709\n",
      "Epoch [526/1000] - train_loss: 0.3690, val_loss: 0.1036, train_mae: 0.3416, val_mae: 0.2706\n",
      "Epoch [527/1000] - train_loss: 0.2967, val_loss: 0.1034, train_mae: 0.2593, val_mae: 0.2703\n",
      "Epoch [528/1000] - train_loss: 0.3117, val_loss: 0.1033, train_mae: 0.2748, val_mae: 0.2703\n",
      "Epoch [529/1000] - train_loss: 0.2900, val_loss: 0.1034, train_mae: 0.2496, val_mae: 0.2703\n",
      "Epoch [530/1000] - train_loss: 0.3087, val_loss: 0.1034, train_mae: 0.2802, val_mae: 0.2703\n",
      "Epoch [531/1000] - train_loss: 0.3152, val_loss: 0.1033, train_mae: 0.2821, val_mae: 0.2703\n",
      "Epoch [532/1000] - train_loss: 0.3349, val_loss: 0.1034, train_mae: 0.3076, val_mae: 0.2703\n",
      "Epoch [533/1000] - train_loss: 0.3155, val_loss: 0.1033, train_mae: 0.2883, val_mae: 0.2702\n",
      "Epoch [534/1000] - train_loss: 0.2997, val_loss: 0.1033, train_mae: 0.2581, val_mae: 0.2703\n",
      "Epoch [535/1000] - train_loss: 0.2937, val_loss: 0.1033, train_mae: 0.2562, val_mae: 0.2702\n",
      "Epoch [536/1000] - train_loss: 0.2946, val_loss: 0.1033, train_mae: 0.2610, val_mae: 0.2702\n",
      "Epoch [537/1000] - train_loss: 0.3166, val_loss: 0.1032, train_mae: 0.2849, val_mae: 0.2701\n",
      "Epoch [538/1000] - train_loss: 0.3230, val_loss: 0.1033, train_mae: 0.2970, val_mae: 0.2702\n",
      "Epoch [539/1000] - train_loss: 0.3501, val_loss: 0.1033, train_mae: 0.3168, val_mae: 0.2702\n",
      "Epoch [540/1000] - train_loss: 0.2943, val_loss: 0.1033, train_mae: 0.2529, val_mae: 0.2702\n",
      "Epoch [541/1000] - train_loss: 0.3226, val_loss: 0.1033, train_mae: 0.2898, val_mae: 0.2702\n",
      "Epoch [542/1000] - train_loss: 0.3027, val_loss: 0.1032, train_mae: 0.2715, val_mae: 0.2701\n",
      "Epoch [543/1000] - train_loss: 0.3133, val_loss: 0.1032, train_mae: 0.2911, val_mae: 0.2701\n",
      "Epoch [544/1000] - train_loss: 0.2969, val_loss: 0.1033, train_mae: 0.2592, val_mae: 0.2702\n",
      "Epoch [545/1000] - train_loss: 0.3019, val_loss: 0.1034, train_mae: 0.2666, val_mae: 0.2703\n",
      "Epoch [546/1000] - train_loss: 0.3191, val_loss: 0.1034, train_mae: 0.2915, val_mae: 0.2703\n",
      "Epoch [547/1000] - train_loss: 0.3265, val_loss: 0.1033, train_mae: 0.3013, val_mae: 0.2702\n",
      "Epoch [548/1000] - train_loss: 0.3180, val_loss: 0.1033, train_mae: 0.2903, val_mae: 0.2702\n",
      "Epoch [549/1000] - train_loss: 0.2903, val_loss: 0.1034, train_mae: 0.2442, val_mae: 0.2704\n",
      "Epoch [550/1000] - train_loss: 0.3096, val_loss: 0.1035, train_mae: 0.2676, val_mae: 0.2705\n",
      "Epoch [551/1000] - train_loss: 0.2986, val_loss: 0.1034, train_mae: 0.2609, val_mae: 0.2704\n",
      "Epoch [552/1000] - train_loss: 0.3164, val_loss: 0.1034, train_mae: 0.2810, val_mae: 0.2704\n",
      "Epoch [553/1000] - train_loss: 0.3083, val_loss: 0.1033, train_mae: 0.2803, val_mae: 0.2702\n",
      "Epoch [554/1000] - train_loss: 0.3100, val_loss: 0.1033, train_mae: 0.2767, val_mae: 0.2702\n",
      "Epoch [555/1000] - train_loss: 0.3154, val_loss: 0.1032, train_mae: 0.2929, val_mae: 0.2701\n",
      "Epoch [556/1000] - train_loss: 0.2989, val_loss: 0.1032, train_mae: 0.2675, val_mae: 0.2700\n",
      "Epoch [557/1000] - train_loss: 0.2962, val_loss: 0.1031, train_mae: 0.2571, val_mae: 0.2699\n",
      "Epoch [558/1000] - train_loss: 0.3288, val_loss: 0.1031, train_mae: 0.3060, val_mae: 0.2700\n",
      "Epoch [559/1000] - train_loss: 0.3154, val_loss: 0.1031, train_mae: 0.2877, val_mae: 0.2700\n",
      "Epoch [560/1000] - train_loss: 0.3191, val_loss: 0.1031, train_mae: 0.2858, val_mae: 0.2699\n",
      "Epoch [561/1000] - train_loss: 0.3182, val_loss: 0.1030, train_mae: 0.2928, val_mae: 0.2698\n",
      "Epoch [562/1000] - train_loss: 0.2973, val_loss: 0.1030, train_mae: 0.2703, val_mae: 0.2698\n",
      "Epoch [563/1000] - train_loss: 0.3551, val_loss: 0.1030, train_mae: 0.3375, val_mae: 0.2698\n",
      "Epoch [564/1000] - train_loss: 0.3593, val_loss: 0.1030, train_mae: 0.3251, val_mae: 0.2698\n",
      "Epoch [565/1000] - train_loss: 0.3066, val_loss: 0.1030, train_mae: 0.2735, val_mae: 0.2697\n",
      "Epoch [566/1000] - train_loss: 0.3012, val_loss: 0.1029, train_mae: 0.2695, val_mae: 0.2697\n",
      "Epoch [567/1000] - train_loss: 0.3007, val_loss: 0.1030, train_mae: 0.2604, val_mae: 0.2698\n",
      "Epoch [568/1000] - train_loss: 0.3329, val_loss: 0.1031, train_mae: 0.3117, val_mae: 0.2699\n",
      "Epoch [569/1000] - train_loss: 0.3270, val_loss: 0.1031, train_mae: 0.2974, val_mae: 0.2700\n",
      "Epoch [570/1000] - train_loss: 0.3062, val_loss: 0.1031, train_mae: 0.2782, val_mae: 0.2699\n",
      "Epoch [571/1000] - train_loss: 0.3107, val_loss: 0.1031, train_mae: 0.2803, val_mae: 0.2699\n",
      "Epoch [572/1000] - train_loss: 0.3265, val_loss: 0.1030, train_mae: 0.3029, val_mae: 0.2699\n",
      "Epoch [573/1000] - train_loss: 0.3487, val_loss: 0.1031, train_mae: 0.3180, val_mae: 0.2700\n",
      "Epoch [574/1000] - train_loss: 0.2878, val_loss: 0.1030, train_mae: 0.2406, val_mae: 0.2699\n",
      "Epoch [575/1000] - train_loss: 0.3649, val_loss: 0.1030, train_mae: 0.3300, val_mae: 0.2698\n",
      "Epoch [576/1000] - train_loss: 0.3028, val_loss: 0.1029, train_mae: 0.2769, val_mae: 0.2696\n",
      "Epoch [577/1000] - train_loss: 0.3079, val_loss: 0.1028, train_mae: 0.2722, val_mae: 0.2694\n",
      "Epoch [578/1000] - train_loss: 0.2989, val_loss: 0.1026, train_mae: 0.2662, val_mae: 0.2692\n",
      "Epoch [579/1000] - train_loss: 0.3036, val_loss: 0.1025, train_mae: 0.2771, val_mae: 0.2691\n",
      "Epoch [580/1000] - train_loss: 0.2943, val_loss: 0.1025, train_mae: 0.2615, val_mae: 0.2690\n",
      "Epoch [581/1000] - train_loss: 0.3275, val_loss: 0.1025, train_mae: 0.3108, val_mae: 0.2690\n",
      "Epoch [582/1000] - train_loss: 0.3075, val_loss: 0.1025, train_mae: 0.2820, val_mae: 0.2691\n",
      "Epoch [583/1000] - train_loss: 0.3352, val_loss: 0.1026, train_mae: 0.3161, val_mae: 0.2692\n",
      "Epoch [584/1000] - train_loss: 0.2970, val_loss: 0.1028, train_mae: 0.2647, val_mae: 0.2694\n",
      "Epoch [585/1000] - train_loss: 0.3367, val_loss: 0.1029, train_mae: 0.3129, val_mae: 0.2696\n",
      "Epoch [586/1000] - train_loss: 0.3258, val_loss: 0.1030, train_mae: 0.3023, val_mae: 0.2698\n",
      "Epoch [587/1000] - train_loss: 0.3301, val_loss: 0.1031, train_mae: 0.3139, val_mae: 0.2700\n",
      "Epoch [588/1000] - train_loss: 0.3199, val_loss: 0.1034, train_mae: 0.3042, val_mae: 0.2703\n",
      "Epoch [589/1000] - train_loss: 0.3076, val_loss: 0.1036, train_mae: 0.2787, val_mae: 0.2707\n",
      "Epoch [590/1000] - train_loss: 0.3041, val_loss: 0.1039, train_mae: 0.2780, val_mae: 0.2710\n",
      "Epoch [591/1000] - train_loss: 0.2973, val_loss: 0.1041, train_mae: 0.2616, val_mae: 0.2712\n",
      "Epoch [592/1000] - train_loss: 0.2943, val_loss: 0.1042, train_mae: 0.2552, val_mae: 0.2714\n",
      "Epoch [593/1000] - train_loss: 0.3580, val_loss: 0.1043, train_mae: 0.3181, val_mae: 0.2715\n",
      "Epoch [594/1000] - train_loss: 0.3407, val_loss: 0.1042, train_mae: 0.3184, val_mae: 0.2714\n",
      "Epoch [595/1000] - train_loss: 0.2915, val_loss: 0.1041, train_mae: 0.2575, val_mae: 0.2712\n",
      "Epoch [596/1000] - train_loss: 0.3121, val_loss: 0.1039, train_mae: 0.2901, val_mae: 0.2711\n",
      "Epoch [597/1000] - train_loss: 0.3203, val_loss: 0.1038, train_mae: 0.2918, val_mae: 0.2709\n",
      "Epoch [598/1000] - train_loss: 0.2887, val_loss: 0.1037, train_mae: 0.2428, val_mae: 0.2707\n",
      "Epoch [599/1000] - train_loss: 0.3183, val_loss: 0.1035, train_mae: 0.2894, val_mae: 0.2705\n",
      "Epoch [600/1000] - train_loss: 0.3116, val_loss: 0.1033, train_mae: 0.2896, val_mae: 0.2703\n",
      "Epoch [601/1000] - train_loss: 0.2912, val_loss: 0.1032, train_mae: 0.2464, val_mae: 0.2701\n",
      "Epoch [602/1000] - train_loss: 0.3509, val_loss: 0.1031, train_mae: 0.3196, val_mae: 0.2700\n",
      "Epoch [603/1000] - train_loss: 0.3251, val_loss: 0.1030, train_mae: 0.2835, val_mae: 0.2698\n",
      "Epoch [604/1000] - train_loss: 0.3203, val_loss: 0.1029, train_mae: 0.2888, val_mae: 0.2697\n",
      "Epoch [605/1000] - train_loss: 0.3116, val_loss: 0.1029, train_mae: 0.2861, val_mae: 0.2697\n",
      "Epoch [606/1000] - train_loss: 0.3089, val_loss: 0.1028, train_mae: 0.2842, val_mae: 0.2695\n",
      "Epoch [607/1000] - train_loss: 0.3147, val_loss: 0.1027, train_mae: 0.2797, val_mae: 0.2694\n",
      "Epoch [608/1000] - train_loss: 0.3350, val_loss: 0.1027, train_mae: 0.3014, val_mae: 0.2694\n",
      "Epoch [609/1000] - train_loss: 0.3010, val_loss: 0.1027, train_mae: 0.2754, val_mae: 0.2694\n",
      "Epoch [610/1000] - train_loss: 0.2969, val_loss: 0.1027, train_mae: 0.2582, val_mae: 0.2693\n",
      "Epoch [611/1000] - train_loss: 0.2868, val_loss: 0.1027, train_mae: 0.2391, val_mae: 0.2694\n",
      "Epoch [612/1000] - train_loss: 0.3037, val_loss: 0.1027, train_mae: 0.2746, val_mae: 0.2694\n",
      "Epoch [613/1000] - train_loss: 0.3039, val_loss: 0.1028, train_mae: 0.2652, val_mae: 0.2695\n",
      "Epoch [614/1000] - train_loss: 0.3113, val_loss: 0.1029, train_mae: 0.2803, val_mae: 0.2697\n",
      "Epoch [615/1000] - train_loss: 0.2892, val_loss: 0.1031, train_mae: 0.2485, val_mae: 0.2699\n",
      "Epoch [616/1000] - train_loss: 0.3118, val_loss: 0.1032, train_mae: 0.2706, val_mae: 0.2700\n",
      "Epoch [617/1000] - train_loss: 0.3060, val_loss: 0.1032, train_mae: 0.2730, val_mae: 0.2700\n",
      "Epoch [618/1000] - train_loss: 0.2989, val_loss: 0.1031, train_mae: 0.2688, val_mae: 0.2700\n",
      "Epoch [619/1000] - train_loss: 0.3005, val_loss: 0.1031, train_mae: 0.2653, val_mae: 0.2699\n",
      "Epoch [620/1000] - train_loss: 0.3032, val_loss: 0.1031, train_mae: 0.2748, val_mae: 0.2699\n",
      "Epoch [621/1000] - train_loss: 0.3082, val_loss: 0.1031, train_mae: 0.2749, val_mae: 0.2699\n",
      "Epoch [622/1000] - train_loss: 0.3106, val_loss: 0.1032, train_mae: 0.2772, val_mae: 0.2700\n",
      "Epoch [623/1000] - train_loss: 0.3008, val_loss: 0.1033, train_mae: 0.2730, val_mae: 0.2702\n",
      "Epoch [624/1000] - train_loss: 0.3013, val_loss: 0.1033, train_mae: 0.2691, val_mae: 0.2703\n",
      "Epoch [625/1000] - train_loss: 0.3332, val_loss: 0.1034, train_mae: 0.3017, val_mae: 0.2703\n",
      "Epoch [626/1000] - train_loss: 0.3018, val_loss: 0.1033, train_mae: 0.2753, val_mae: 0.2702\n",
      "Epoch [627/1000] - train_loss: 0.3246, val_loss: 0.1033, train_mae: 0.2971, val_mae: 0.2702\n",
      "Epoch [628/1000] - train_loss: 0.3149, val_loss: 0.1034, train_mae: 0.2828, val_mae: 0.2703\n",
      "Epoch [629/1000] - train_loss: 0.2922, val_loss: 0.1036, train_mae: 0.2526, val_mae: 0.2706\n",
      "Epoch [630/1000] - train_loss: 0.3160, val_loss: 0.1036, train_mae: 0.2909, val_mae: 0.2707\n",
      "Epoch [631/1000] - train_loss: 0.3192, val_loss: 0.1038, train_mae: 0.2976, val_mae: 0.2708\n",
      "Epoch [632/1000] - train_loss: 0.3252, val_loss: 0.1038, train_mae: 0.2927, val_mae: 0.2709\n",
      "Epoch [633/1000] - train_loss: 0.2941, val_loss: 0.1038, train_mae: 0.2595, val_mae: 0.2709\n",
      "Epoch [634/1000] - train_loss: 0.3008, val_loss: 0.1038, train_mae: 0.2708, val_mae: 0.2709\n",
      "Epoch [635/1000] - train_loss: 0.2959, val_loss: 0.1038, train_mae: 0.2580, val_mae: 0.2708\n",
      "Epoch [636/1000] - train_loss: 0.3101, val_loss: 0.1038, train_mae: 0.2824, val_mae: 0.2709\n",
      "Epoch [637/1000] - train_loss: 0.3008, val_loss: 0.1039, train_mae: 0.2710, val_mae: 0.2710\n",
      "Epoch [638/1000] - train_loss: 0.2966, val_loss: 0.1040, train_mae: 0.2640, val_mae: 0.2711\n",
      "Epoch [639/1000] - train_loss: 0.2995, val_loss: 0.1040, train_mae: 0.2554, val_mae: 0.2712\n",
      "Epoch [640/1000] - train_loss: 0.3025, val_loss: 0.1040, train_mae: 0.2710, val_mae: 0.2712\n",
      "Epoch [641/1000] - train_loss: 0.3141, val_loss: 0.1040, train_mae: 0.2782, val_mae: 0.2712\n",
      "Epoch [642/1000] - train_loss: 0.3101, val_loss: 0.1041, train_mae: 0.2857, val_mae: 0.2713\n",
      "Epoch [643/1000] - train_loss: 0.3293, val_loss: 0.1041, train_mae: 0.2886, val_mae: 0.2713\n",
      "Epoch [644/1000] - train_loss: 0.2908, val_loss: 0.1041, train_mae: 0.2512, val_mae: 0.2713\n",
      "Epoch [645/1000] - train_loss: 0.3257, val_loss: 0.1041, train_mae: 0.2956, val_mae: 0.2713\n",
      "Epoch [646/1000] - train_loss: 0.3043, val_loss: 0.1041, train_mae: 0.2672, val_mae: 0.2712\n",
      "Epoch [647/1000] - train_loss: 0.3417, val_loss: 0.1041, train_mae: 0.3147, val_mae: 0.2713\n",
      "Epoch [648/1000] - train_loss: 0.2918, val_loss: 0.1042, train_mae: 0.2438, val_mae: 0.2714\n",
      "Epoch [649/1000] - train_loss: 0.2891, val_loss: 0.1042, train_mae: 0.2448, val_mae: 0.2714\n",
      "Epoch [650/1000] - train_loss: 0.3037, val_loss: 0.1043, train_mae: 0.2695, val_mae: 0.2715\n",
      "Epoch [651/1000] - train_loss: 0.2984, val_loss: 0.1044, train_mae: 0.2654, val_mae: 0.2716\n",
      "Epoch [652/1000] - train_loss: 0.2983, val_loss: 0.1045, train_mae: 0.2639, val_mae: 0.2718\n",
      "Epoch [653/1000] - train_loss: 0.3107, val_loss: 0.1045, train_mae: 0.2745, val_mae: 0.2718\n",
      "Epoch [654/1000] - train_loss: 0.3117, val_loss: 0.1046, train_mae: 0.2648, val_mae: 0.2720\n",
      "Epoch [655/1000] - train_loss: 0.2911, val_loss: 0.1047, train_mae: 0.2529, val_mae: 0.2721\n",
      "Epoch [656/1000] - train_loss: 0.3298, val_loss: 0.1048, train_mae: 0.3005, val_mae: 0.2721\n",
      "Epoch [657/1000] - train_loss: 0.2946, val_loss: 0.1048, train_mae: 0.2609, val_mae: 0.2721\n",
      "Epoch [658/1000] - train_loss: 0.2886, val_loss: 0.1048, train_mae: 0.2443, val_mae: 0.2721\n",
      "Epoch [659/1000] - train_loss: 0.3056, val_loss: 0.1048, train_mae: 0.2718, val_mae: 0.2721\n",
      "Epoch [660/1000] - train_loss: 0.3151, val_loss: 0.1047, train_mae: 0.2928, val_mae: 0.2721\n",
      "Epoch [661/1000] - train_loss: 0.3470, val_loss: 0.1048, train_mae: 0.3272, val_mae: 0.2722\n",
      "Epoch [662/1000] - train_loss: 0.3235, val_loss: 0.1050, train_mae: 0.2884, val_mae: 0.2724\n",
      "Epoch [663/1000] - train_loss: 0.2958, val_loss: 0.1051, train_mae: 0.2629, val_mae: 0.2725\n",
      "Epoch [664/1000] - train_loss: 0.3197, val_loss: 0.1052, train_mae: 0.2881, val_mae: 0.2726\n",
      "Epoch [665/1000] - train_loss: 0.3262, val_loss: 0.1052, train_mae: 0.2773, val_mae: 0.2726\n",
      "Epoch [666/1000] - train_loss: 0.3031, val_loss: 0.1050, train_mae: 0.2665, val_mae: 0.2725\n",
      "Epoch [667/1000] - train_loss: 0.2876, val_loss: 0.1049, train_mae: 0.2358, val_mae: 0.2723\n",
      "Epoch [668/1000] - train_loss: 0.2976, val_loss: 0.1048, train_mae: 0.2610, val_mae: 0.2721\n",
      "Epoch [669/1000] - train_loss: 0.2972, val_loss: 0.1046, train_mae: 0.2653, val_mae: 0.2720\n",
      "Epoch [670/1000] - train_loss: 0.3213, val_loss: 0.1045, train_mae: 0.2996, val_mae: 0.2718\n",
      "Epoch [671/1000] - train_loss: 0.3213, val_loss: 0.1043, train_mae: 0.2866, val_mae: 0.2715\n",
      "Epoch [672/1000] - train_loss: 0.3141, val_loss: 0.1042, train_mae: 0.2906, val_mae: 0.2714\n",
      "Epoch [673/1000] - train_loss: 0.3162, val_loss: 0.1041, train_mae: 0.2929, val_mae: 0.2712\n",
      "Epoch [674/1000] - train_loss: 0.3157, val_loss: 0.1039, train_mae: 0.2897, val_mae: 0.2711\n",
      "Epoch [675/1000] - train_loss: 0.3539, val_loss: 0.1038, train_mae: 0.3252, val_mae: 0.2708\n",
      "Epoch [676/1000] - train_loss: 0.3245, val_loss: 0.1035, train_mae: 0.2952, val_mae: 0.2705\n",
      "Epoch [677/1000] - train_loss: 0.3401, val_loss: 0.1032, train_mae: 0.2945, val_mae: 0.2701\n",
      "Epoch [678/1000] - train_loss: 0.3105, val_loss: 0.1029, train_mae: 0.2697, val_mae: 0.2697\n",
      "Epoch [679/1000] - train_loss: 0.3051, val_loss: 0.1028, train_mae: 0.2789, val_mae: 0.2695\n",
      "Epoch [680/1000] - train_loss: 0.3150, val_loss: 0.1028, train_mae: 0.2900, val_mae: 0.2694\n",
      "Epoch [681/1000] - train_loss: 0.3842, val_loss: 0.1028, train_mae: 0.3595, val_mae: 0.2696\n",
      "Epoch [682/1000] - train_loss: 0.3228, val_loss: 0.1029, train_mae: 0.2969, val_mae: 0.2696\n",
      "Epoch [683/1000] - train_loss: 0.3044, val_loss: 0.1030, train_mae: 0.2764, val_mae: 0.2698\n",
      "Epoch [684/1000] - train_loss: 0.3130, val_loss: 0.1031, train_mae: 0.2741, val_mae: 0.2700\n",
      "Epoch [685/1000] - train_loss: 0.3090, val_loss: 0.1031, train_mae: 0.2859, val_mae: 0.2700\n",
      "Epoch [686/1000] - train_loss: 0.3198, val_loss: 0.1031, train_mae: 0.2903, val_mae: 0.2700\n",
      "Epoch [687/1000] - train_loss: 0.3094, val_loss: 0.1033, train_mae: 0.2778, val_mae: 0.2702\n",
      "Epoch [688/1000] - train_loss: 0.3055, val_loss: 0.1035, train_mae: 0.2734, val_mae: 0.2705\n",
      "Epoch [689/1000] - train_loss: 0.3089, val_loss: 0.1037, train_mae: 0.2832, val_mae: 0.2707\n",
      "Epoch [690/1000] - train_loss: 0.3109, val_loss: 0.1038, train_mae: 0.2840, val_mae: 0.2709\n",
      "Epoch [691/1000] - train_loss: 0.3203, val_loss: 0.1039, train_mae: 0.2935, val_mae: 0.2711\n",
      "Epoch [692/1000] - train_loss: 0.2963, val_loss: 0.1042, train_mae: 0.2600, val_mae: 0.2714\n",
      "Epoch [693/1000] - train_loss: 0.3202, val_loss: 0.1044, train_mae: 0.3000, val_mae: 0.2716\n",
      "Epoch [694/1000] - train_loss: 0.3111, val_loss: 0.1045, train_mae: 0.2847, val_mae: 0.2718\n",
      "Epoch [695/1000] - train_loss: 0.3097, val_loss: 0.1046, train_mae: 0.2809, val_mae: 0.2720\n",
      "Epoch [696/1000] - train_loss: 0.3024, val_loss: 0.1048, train_mae: 0.2650, val_mae: 0.2721\n",
      "Epoch [697/1000] - train_loss: 0.3430, val_loss: 0.1048, train_mae: 0.3178, val_mae: 0.2722\n",
      "Epoch [698/1000] - train_loss: 0.3143, val_loss: 0.1048, train_mae: 0.2859, val_mae: 0.2722\n",
      "Epoch [699/1000] - train_loss: 0.3089, val_loss: 0.1049, train_mae: 0.2773, val_mae: 0.2723\n",
      "Epoch [700/1000] - train_loss: 0.3288, val_loss: 0.1049, train_mae: 0.2959, val_mae: 0.2723\n",
      "Epoch [701/1000] - train_loss: 0.3388, val_loss: 0.1051, train_mae: 0.3046, val_mae: 0.2725\n",
      "Epoch [702/1000] - train_loss: 0.3085, val_loss: 0.1052, train_mae: 0.2817, val_mae: 0.2727\n",
      "Epoch [703/1000] - train_loss: 0.2977, val_loss: 0.1054, train_mae: 0.2607, val_mae: 0.2728\n",
      "Epoch [704/1000] - train_loss: 0.3026, val_loss: 0.1055, train_mae: 0.2601, val_mae: 0.2730\n",
      "Epoch [705/1000] - train_loss: 0.3236, val_loss: 0.1056, train_mae: 0.3000, val_mae: 0.2731\n",
      "Epoch [706/1000] - train_loss: 0.3404, val_loss: 0.1058, train_mae: 0.3211, val_mae: 0.2734\n",
      "Epoch [707/1000] - train_loss: 0.3058, val_loss: 0.1062, train_mae: 0.2727, val_mae: 0.2738\n",
      "Epoch [708/1000] - train_loss: 0.2988, val_loss: 0.1064, train_mae: 0.2610, val_mae: 0.2741\n",
      "Epoch [709/1000] - train_loss: 0.2978, val_loss: 0.1066, train_mae: 0.2579, val_mae: 0.2742\n",
      "Epoch [710/1000] - train_loss: 0.3090, val_loss: 0.1067, train_mae: 0.2759, val_mae: 0.2743\n",
      "Epoch [711/1000] - train_loss: 0.3118, val_loss: 0.1068, train_mae: 0.2668, val_mae: 0.2744\n",
      "Epoch [712/1000] - train_loss: 0.3305, val_loss: 0.1067, train_mae: 0.2901, val_mae: 0.2744\n",
      "Epoch [713/1000] - train_loss: 0.2939, val_loss: 0.1066, train_mae: 0.2471, val_mae: 0.2742\n",
      "Epoch [714/1000] - train_loss: 0.3099, val_loss: 0.1065, train_mae: 0.2696, val_mae: 0.2742\n",
      "Epoch [715/1000] - train_loss: 0.3216, val_loss: 0.1063, train_mae: 0.2878, val_mae: 0.2739\n",
      "Epoch [716/1000] - train_loss: 0.2980, val_loss: 0.1061, train_mae: 0.2566, val_mae: 0.2737\n",
      "Epoch [717/1000] - train_loss: 0.3092, val_loss: 0.1060, train_mae: 0.2820, val_mae: 0.2736\n",
      "Epoch [718/1000] - train_loss: 0.3146, val_loss: 0.1059, train_mae: 0.2790, val_mae: 0.2735\n",
      "Epoch [719/1000] - train_loss: 0.3208, val_loss: 0.1057, train_mae: 0.2916, val_mae: 0.2733\n",
      "Epoch [720/1000] - train_loss: 0.2934, val_loss: 0.1056, train_mae: 0.2537, val_mae: 0.2731\n",
      "Epoch [721/1000] - train_loss: 0.3030, val_loss: 0.1054, train_mae: 0.2743, val_mae: 0.2728\n",
      "Epoch [722/1000] - train_loss: 0.3086, val_loss: 0.1052, train_mae: 0.2790, val_mae: 0.2726\n",
      "Epoch [723/1000] - train_loss: 0.3023, val_loss: 0.1050, train_mae: 0.2781, val_mae: 0.2724\n",
      "Epoch [724/1000] - train_loss: 0.2976, val_loss: 0.1049, train_mae: 0.2569, val_mae: 0.2723\n",
      "Epoch [725/1000] - train_loss: 0.3017, val_loss: 0.1050, train_mae: 0.2710, val_mae: 0.2724\n",
      "Epoch [726/1000] - train_loss: 0.3248, val_loss: 0.1050, train_mae: 0.2916, val_mae: 0.2724\n",
      "Epoch [727/1000] - train_loss: 0.3096, val_loss: 0.1050, train_mae: 0.2816, val_mae: 0.2724\n",
      "Epoch [728/1000] - train_loss: 0.2942, val_loss: 0.1051, train_mae: 0.2550, val_mae: 0.2725\n",
      "Epoch [729/1000] - train_loss: 0.3318, val_loss: 0.1051, train_mae: 0.2989, val_mae: 0.2726\n",
      "Epoch [730/1000] - train_loss: 0.3004, val_loss: 0.1053, train_mae: 0.2707, val_mae: 0.2728\n",
      "Epoch [731/1000] - train_loss: 0.2914, val_loss: 0.1054, train_mae: 0.2508, val_mae: 0.2729\n",
      "Epoch [732/1000] - train_loss: 0.2981, val_loss: 0.1054, train_mae: 0.2589, val_mae: 0.2728\n",
      "Epoch [733/1000] - train_loss: 0.3579, val_loss: 0.1053, train_mae: 0.3183, val_mae: 0.2728\n",
      "Epoch [734/1000] - train_loss: 0.3353, val_loss: 0.1052, train_mae: 0.3056, val_mae: 0.2726\n",
      "Epoch [735/1000] - train_loss: 0.3168, val_loss: 0.1050, train_mae: 0.2971, val_mae: 0.2724\n",
      "Epoch [736/1000] - train_loss: 0.3157, val_loss: 0.1048, train_mae: 0.2874, val_mae: 0.2722\n",
      "Epoch [737/1000] - train_loss: 0.2951, val_loss: 0.1048, train_mae: 0.2586, val_mae: 0.2722\n",
      "Epoch [738/1000] - train_loss: 0.3329, val_loss: 0.1049, train_mae: 0.3092, val_mae: 0.2723\n",
      "Epoch [739/1000] - train_loss: 0.2990, val_loss: 0.1050, train_mae: 0.2589, val_mae: 0.2725\n",
      "Epoch [740/1000] - train_loss: 0.3335, val_loss: 0.1051, train_mae: 0.2938, val_mae: 0.2726\n",
      "Epoch [741/1000] - train_loss: 0.3028, val_loss: 0.1050, train_mae: 0.2709, val_mae: 0.2725\n",
      "Epoch [742/1000] - train_loss: 0.3028, val_loss: 0.1050, train_mae: 0.2699, val_mae: 0.2724\n",
      "Epoch [743/1000] - train_loss: 0.3178, val_loss: 0.1049, train_mae: 0.2897, val_mae: 0.2722\n",
      "Epoch [744/1000] - train_loss: 0.3150, val_loss: 0.1048, train_mae: 0.2957, val_mae: 0.2722\n",
      "Epoch [745/1000] - train_loss: 0.2992, val_loss: 0.1049, train_mae: 0.2587, val_mae: 0.2723\n",
      "Epoch [746/1000] - train_loss: 0.2951, val_loss: 0.1049, train_mae: 0.2608, val_mae: 0.2723\n",
      "Epoch [747/1000] - train_loss: 0.3020, val_loss: 0.1049, train_mae: 0.2638, val_mae: 0.2723\n",
      "Epoch [748/1000] - train_loss: 0.3243, val_loss: 0.1050, train_mae: 0.2912, val_mae: 0.2724\n",
      "Epoch [749/1000] - train_loss: 0.3036, val_loss: 0.1050, train_mae: 0.2678, val_mae: 0.2724\n",
      "Timeout reached (10.004133224487305 sec)\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_maes, val_maes, best_state = train_loop(net, train_dataloader, val_dataloader, timeout=10, patience=5000, num_epochs=1000, l1_weight=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(n_neurons, h, w):\n",
    "    net = torch.nn.Sequential(torch.nn.Flatten(), \n",
    "                              torch.nn.Linear(3*h*w, n_neurons),     torch.nn.ReLU(), \n",
    "                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(n_neurons, 1))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## net6x50_1e-06l1 ########\n",
      "\n",
      "Epoch [1/1000] - train_loss: 0.1511, val_loss: 0.1093, train_mae: 0.3082, val_mae: 0.2794, val_max: 0.6718\n",
      "Epoch [2/1000] - train_loss: 0.1876, val_loss: 0.1024, train_mae: 0.3587, val_mae: 0.2712, val_max: 0.6303\n",
      "Epoch [3/1000] - train_loss: 0.1688, val_loss: 0.0989, train_mae: 0.3408, val_mae: 0.2661, val_max: 0.5880\n",
      "Epoch [4/1000] - train_loss: 0.1158, val_loss: 0.0994, train_mae: 0.2714, val_mae: 0.2662, val_max: 0.5429\n",
      "Epoch [5/1000] - train_loss: 0.1003, val_loss: 0.1049, train_mae: 0.2395, val_mae: 0.2724, val_max: 0.5560\n",
      "Epoch [6/1000] - train_loss: 0.1567, val_loss: 0.1128, train_mae: 0.3005, val_mae: 0.2806, val_max: 0.5970\n",
      "Epoch [7/1000] - train_loss: 0.1295, val_loss: 0.1181, train_mae: 0.2807, val_mae: 0.2886, val_max: 0.6192\n",
      "Epoch [8/1000] - train_loss: 0.1415, val_loss: 0.1248, train_mae: 0.3105, val_mae: 0.2977, val_max: 0.6418\n",
      "Epoch [9/1000] - train_loss: 0.1007, val_loss: 0.1220, train_mae: 0.2524, val_mae: 0.2942, val_max: 0.6339\n",
      "Epoch [10/1000] - train_loss: 0.0963, val_loss: 0.1166, train_mae: 0.2447, val_mae: 0.2872, val_max: 0.6165\n",
      "Epoch [11/1000] - train_loss: 0.1174, val_loss: 0.1145, train_mae: 0.2805, val_mae: 0.2844, val_max: 0.6102\n",
      "Epoch [12/1000] - train_loss: 0.0909, val_loss: 0.1138, train_mae: 0.2275, val_mae: 0.2837, val_max: 0.6095\n",
      "Epoch [13/1000] - train_loss: 0.1058, val_loss: 0.1120, train_mae: 0.2630, val_mae: 0.2815, val_max: 0.6055\n",
      "Stopping early (patience of 10 reached)\n",
      "Training completed\n",
      "\n",
      "######## net6x50_1e-05l1 ########\n",
      "\n",
      "Epoch [1/1000] - train_loss: 0.2296, val_loss: 0.1194, train_mae: 0.3953, val_mae: 0.2873, val_max: 0.7121\n",
      "Epoch [2/1000] - train_loss: 0.2014, val_loss: 0.1096, train_mae: 0.3514, val_mae: 0.2796, val_max: 0.6734\n",
      "Epoch [3/1000] - train_loss: 0.1684, val_loss: 0.1030, train_mae: 0.3395, val_mae: 0.2722, val_max: 0.6350\n",
      "Epoch [4/1000] - train_loss: 0.1456, val_loss: 0.0992, train_mae: 0.3026, val_mae: 0.2662, val_max: 0.5959\n",
      "Epoch [5/1000] - train_loss: 0.1234, val_loss: 0.0988, train_mae: 0.2705, val_mae: 0.2664, val_max: 0.5435\n",
      "Epoch [6/1000] - train_loss: 0.1284, val_loss: 0.1067, train_mae: 0.2931, val_mae: 0.2751, val_max: 0.5644\n",
      "Epoch [7/1000] - train_loss: 0.1226, val_loss: 0.1261, train_mae: 0.2713, val_mae: 0.3004, val_max: 0.6384\n",
      "Epoch [8/1000] - train_loss: 0.1124, val_loss: 0.1476, train_mae: 0.2552, val_mae: 0.3230, val_max: 0.6928\n",
      "Epoch [9/1000] - train_loss: 0.1181, val_loss: 0.1453, train_mae: 0.2597, val_mae: 0.3205, val_max: 0.6879\n",
      "Epoch [10/1000] - train_loss: 0.1216, val_loss: 0.1287, train_mae: 0.2713, val_mae: 0.3032, val_max: 0.6472\n",
      "Epoch [11/1000] - train_loss: 0.1204, val_loss: 0.1215, train_mae: 0.2618, val_mae: 0.2945, val_max: 0.6267\n",
      "Epoch [12/1000] - train_loss: 0.1125, val_loss: 0.1194, train_mae: 0.2650, val_mae: 0.2916, val_max: 0.6202\n",
      "Epoch [13/1000] - train_loss: 0.1463, val_loss: 0.1143, train_mae: 0.3008, val_mae: 0.2844, val_max: 0.6036\n",
      "Epoch [14/1000] - train_loss: 0.1128, val_loss: 0.1115, train_mae: 0.2629, val_mae: 0.2805, val_max: 0.5935\n",
      "Epoch [15/1000] - train_loss: 0.1100, val_loss: 0.1124, train_mae: 0.2582, val_mae: 0.2824, val_max: 0.5969\n",
      "Stopping early (patience of 10 reached)\n",
      "Training completed\n",
      "\n",
      "######## net6x50_0.0001l1 ########\n",
      "\n",
      "Epoch [1/1000] - train_loss: 0.2299, val_loss: 0.0991, train_mae: 0.2847, val_mae: 0.2656, val_max: 0.5965\n",
      "Epoch [2/1000] - train_loss: 0.2239, val_loss: 0.0998, train_mae: 0.3007, val_mae: 0.2660, val_max: 0.5326\n",
      "Epoch [3/1000] - train_loss: 0.2551, val_loss: 0.1072, train_mae: 0.3212, val_mae: 0.2750, val_max: 0.5712\n",
      "Epoch [4/1000] - train_loss: 0.2082, val_loss: 0.1130, train_mae: 0.2756, val_mae: 0.2818, val_max: 0.5987\n",
      "Epoch [5/1000] - train_loss: 0.1954, val_loss: 0.1118, train_mae: 0.2738, val_mae: 0.2798, val_max: 0.5942\n",
      "Epoch [6/1000] - train_loss: 0.2035, val_loss: 0.1106, train_mae: 0.2731, val_mae: 0.2782, val_max: 0.5895\n",
      "Epoch [7/1000] - train_loss: 0.1713, val_loss: 0.1083, train_mae: 0.2290, val_mae: 0.2762, val_max: 0.5797\n",
      "Epoch [8/1000] - train_loss: 0.1833, val_loss: 0.1073, train_mae: 0.2732, val_mae: 0.2752, val_max: 0.5755\n",
      "Epoch [9/1000] - train_loss: 0.1961, val_loss: 0.1067, train_mae: 0.2871, val_mae: 0.2745, val_max: 0.5733\n",
      "Epoch [10/1000] - train_loss: 0.1623, val_loss: 0.1051, train_mae: 0.2425, val_mae: 0.2728, val_max: 0.5662\n",
      "Epoch [11/1000] - train_loss: 0.2074, val_loss: 0.1058, train_mae: 0.3121, val_mae: 0.2735, val_max: 0.5713\n",
      "Stopping early (patience of 10 reached)\n",
      "Training completed\n",
      "\n",
      "######## net6x50_0.001l1 ########\n",
      "\n",
      "Epoch [1/1000] - train_loss: 1.2054, val_loss: 0.1364, train_mae: 0.4153, val_mae: 0.3029, val_max: 0.7615\n",
      "Epoch [2/1000] - train_loss: 1.1904, val_loss: 0.1227, train_mae: 0.4449, val_mae: 0.2897, val_max: 0.7226\n",
      "Epoch [3/1000] - train_loss: 1.1086, val_loss: 0.1120, train_mae: 0.4124, val_mae: 0.2819, val_max: 0.6834\n",
      "Epoch [4/1000] - train_loss: 1.0351, val_loss: 0.1041, train_mae: 0.3843, val_mae: 0.2737, val_max: 0.6428\n",
      "Epoch [5/1000] - train_loss: 0.9667, val_loss: 0.0992, train_mae: 0.3521, val_mae: 0.2658, val_max: 0.5961\n",
      "Epoch [6/1000] - train_loss: 0.8858, val_loss: 0.0987, train_mae: 0.2893, val_mae: 0.2658, val_max: 0.5458\n",
      "Epoch [7/1000] - train_loss: 0.8379, val_loss: 0.1029, train_mae: 0.2852, val_mae: 0.2696, val_max: 0.5460\n",
      "Epoch [8/1000] - train_loss: 0.7829, val_loss: 0.1089, train_mae: 0.2688, val_mae: 0.2766, val_max: 0.5808\n",
      "Epoch [9/1000] - train_loss: 0.7329, val_loss: 0.1166, train_mae: 0.2399, val_mae: 0.2865, val_max: 0.6132\n",
      "Epoch [10/1000] - train_loss: 0.7232, val_loss: 0.1213, train_mae: 0.2763, val_mae: 0.2930, val_max: 0.6294\n",
      "Epoch [11/1000] - train_loss: 0.6802, val_loss: 0.1220, train_mae: 0.2800, val_mae: 0.2939, val_max: 0.6317\n",
      "Epoch [12/1000] - train_loss: 0.6714, val_loss: 0.1231, train_mae: 0.2898, val_mae: 0.2953, val_max: 0.6351\n",
      "Epoch [13/1000] - train_loss: 0.6052, val_loss: 0.1183, train_mae: 0.2719, val_mae: 0.2890, val_max: 0.6193\n",
      "Epoch [14/1000] - train_loss: 0.5665, val_loss: 0.1147, train_mae: 0.2537, val_mae: 0.2836, val_max: 0.6060\n",
      "Epoch [15/1000] - train_loss: 0.5287, val_loss: 0.1135, train_mae: 0.2538, val_mae: 0.2818, val_max: 0.6013\n",
      "Epoch [16/1000] - train_loss: 0.4931, val_loss: 0.1136, train_mae: 0.2376, val_mae: 0.2819, val_max: 0.6017\n",
      "Stopping early (patience of 10 reached)\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "n_neurons = 50\n",
    "h = 2\n",
    "w = 2\n",
    "timeout = 10\n",
    "patience = 10\n",
    "num_epochs = 1000\n",
    "l1_weights = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "names = [f\"net6x50_{l1_weight}l1\" for l1_weight in l1_weights]\n",
    "\n",
    "for netname, l1_weight in zip(names, l1_weights):\n",
    "    print(f\"\\n######## {netname} ########\\n\")\n",
    "    net = make_nn(n_neurons, h, w)\n",
    "    train_losses, val_losses, train_maes, val_maes, val_maxs, best_state = train_loop(net, train_dataloader, val_dataloader, \n",
    "                                                                            timeout=timeout, patience=patience, num_epochs=num_epochs, \n",
    "                                                                            l1_weight=l1_weight)\n",
    "    torch.save(best_state, f'./l1_experiments/{netname}_best_state.pth')\n",
    "    torch.save(train_losses, f'./l1_experiments/{netname}_train_losses.pth')\n",
    "    torch.save(val_losses, f'./l1_experiments/{netname}_val_losses.pth')\n",
    "    torch.save(train_maes, f'./l1_experiments/{netname}_train_maes.pth')\n",
    "    torch.save(val_maes, f'./l1_experiments/{netname}_val_maes.pth')\n",
    "    torch.save(val_maxs, f'./l1_experiments/{netname}_val_maxs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_losses, './train_losses.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 2, 2)\n",
    "\n",
    "models = [net]\n",
    "onnx_files = ['net6x50_overfit.onnx']\n",
    "\n",
    "for model, filename in zip(models, onnx_files):\n",
    "    torch.onnx.export(model, dummy_input, './verification/models/' + filename, export_params=True, do_constant_folding=True, opset_version=7, input_names=['X'], output_names=['Y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcrown-fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
