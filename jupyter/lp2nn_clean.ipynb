{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import schedulefree\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../auto_LiRPA/\")\n",
    "import auto_LiRPA\n",
    "from auto_LiRPA.operators.gurobi_maxpool_lp import compute_maxpool_bias\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 160.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# now for a lot of neurons\n",
    "n_neurons = 100\n",
    "l = torch.randn(n_neurons, 1, 2, 2)\n",
    "u = torch.abs(torch.randn(n_neurons, 1, 2, 2)) + l\n",
    "alpha = torch.rand(n_neurons, 1, 2, 2)\n",
    "\n",
    "biases = compute_maxpool_bias(l, u, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bounds(l, u):\n",
    "    \"\"\"\n",
    "    Takes bounds tensors and normalizes them to [0, 1], s.t. smallest lower bound is mapped to 0\n",
    "    and largest upper bound is mapped to 1\n",
    "\n",
    "    args:\n",
    "        l (batch x channels x w x h) - concrete lower bounds\n",
    "        u (batch x channels x w x h) - concrete upper bounds\n",
    "\n",
    "    returns:\n",
    "        l_norm (batch x channels x w x h) - normalized concrete lower bounds\n",
    "        u_norm (batch x channels x w x h) - normalized concrete upper bounds\n",
    "    \"\"\"\n",
    "    lmin = l.flatten(-2).min(dim=-1)[0]\n",
    "    umax = u.flatten(-2).max(dim=-1)[0]\n",
    "    lmin = lmin.unsqueeze(1)\n",
    "    umax = umax.unsqueeze(1)\n",
    "\n",
    "    l_norm = (l.flatten(-2) - lmin) / (umax - lmin)\n",
    "    u_norm = (u.flatten(-2) - lmin) / (umax - lmin)\n",
    "    l_norm = l_norm.view(l.shape)\n",
    "    u_norm = u_norm.view(u.shape)\n",
    "\n",
    "    return l_norm, u_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_lower_bound(X):\n",
    "    \"\"\"\n",
    "    Sorts tensor of shape (n_neurons, 3, w, h) by concrete lower bounds (the first channel dim).\n",
    "    \"\"\"\n",
    "    _, ind_tensor = X.flatten(-2)[:,0].sort(dim=-1)\n",
    "    ind_tensor = ind_tensor.unsqueeze(1).expand(-1, X.size(1), -1)\n",
    "\n",
    "    return torch.gather(X.flatten(-2), dim=2, index=ind_tensor).view(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_neurons, h, w):\n",
    "    # since the normalized version suffices, just stick to that\n",
    "    x1 = torch.rand(n_neurons, 1, h, w)\n",
    "    x2 = torch.rand(n_neurons, 1, h, w)\n",
    "\n",
    "    l = torch.where(x1 <= x2, x1, x2)\n",
    "    u = torch.where(x1  > x2, x1, x2)\n",
    "    l, u = normalize_bounds(l, u)\n",
    "\n",
    "    alpha = torch.rand(n_neurons, 1, h, w)\n",
    "\n",
    "    biases = compute_maxpool_bias(l, u, alpha)\n",
    "\n",
    "    return l, u, alpha, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 145.46it/s]\n"
     ]
    }
   ],
   "source": [
    "l, u, alpha, biases = create_dataset(100, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((l, u, alpha), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has shape `(n_neurons, 3, h, w)` and\n",
    "- `X[:,0,:,:]` represents the lower bounds\n",
    "- `X[:,1,:,:]` represents the upper bounds\n",
    "- `X[:,2,:,:]` represents the slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0693],\n",
       "         [0.2860, 0.8869]],\n",
       "\n",
       "        [[0.0000, 0.0558],\n",
       "         [0.3411, 0.4535]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sorted = sort_by_lower_bound(X)\n",
    "X_sorted[:2, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2026, 1.0000],\n",
       "         [0.8710, 0.9930]],\n",
       "\n",
       "        [[0.0168, 0.5627],\n",
       "         [0.6591, 1.0000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sorted[:2, 1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(n_neurons_train, n_neurons_val, h, w, sort_by_lb=True):\n",
    "    l, u, alpha, bias = create_dataset(n_neurons_train, h, w)\n",
    "    X = torch.cat((l, u, alpha), dim=1)\n",
    "\n",
    "    if sort_by_lb:\n",
    "        X = sort_by_lower_bound(X)\n",
    "\n",
    "    dataset_train = TensorDataset(X, bias)\n",
    "\n",
    "    l, u, alpha, bias = create_dataset(n_neurons_val, h, w)\n",
    "    X = torch.cat((l, u, alpha), dim=1)\n",
    "\n",
    "    if sort_by_lb:\n",
    "        X = sort_by_lower_bound(X)\n",
    "        \n",
    "    dataset_val = TensorDataset(X, bias)\n",
    "\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATASET = False\n",
    "\n",
    "if CREATE_DATASET:\n",
    "    ds_train, ds_val = create_tensor_dataset(100, 10, 2, 2)\n",
    "    torch.save(ds_train, './datasets/maxpool2x2_train_clean.pth')\n",
    "    torch.save(ds_val, './datasets/maxpool2x2_val_clean.pth')\n",
    "else:\n",
    "    ds_train = torch.load('./datasets/maxpool2x2_train_clean.pth')\n",
    "    ds_val   = torch.load('./datasets/maxpool2x2_val_clean.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(ds_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(net, train_dataloader, val_dataloader, patience=10, num_epochs=100, timeout=60, lossfun='mse', opt='adam'):\n",
    "    if lossfun == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif lossfun == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    else:\n",
    "        raise ValueError('Unknown loss function!')\n",
    "    \n",
    "    if opt == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "    elif opt == 'schedulefree':\n",
    "        optimizer = schedulefree.AdamWScheduleFree(net.parameters(), lr=0.0025)\n",
    "    else:\n",
    "        raise ValueError('Uknown optimizer!')\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    train_maes = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_cnt = 0\n",
    "    t_start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        t_cur = time.time()\n",
    "        if t_cur - t_start > timeout:\n",
    "            print(f\"Timeout reached ({t_cur - t_start} sec)\")\n",
    "            break \n",
    "        \n",
    "        net.train()\n",
    "\n",
    "        if opt == 'schedulefree':\n",
    "            optimizer.train()\n",
    "\n",
    "        train_loss = 0.\n",
    "        train_mae = 0.\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            y_hat = net(batch_X)\n",
    "            loss = criterion(y_hat, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_mae += torch.abs(y_hat - batch_y).mean().item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_mae /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_maes.append(train_mae)\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        if opt == 'schedulefree':\n",
    "            optimizer.eval()\n",
    "            \n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_dataloader:\n",
    "                y_hat = net(batch_X)\n",
    "                loss = criterion(y_hat, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += torch.abs(y_hat - batch_y).mean().item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_mae /= len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_maes.append(val_mae)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, train_mae: {train_mae:.4f}, val_mae: {val_mae:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_cnt = 0\n",
    "            best_net_state = net.state_dict()\n",
    "        else:\n",
    "            early_stopping_cnt += 1\n",
    "            if early_stopping_cnt >= patience:\n",
    "                print(f\"Stopping early (patience of {patience} reached)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    return train_losses, val_losses, train_maes, val_maes, best_net_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 50\n",
    "h = 2\n",
    "w = 2\n",
    "net = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(3*h*w, n_neurons),     torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - train_loss: 0.0180, val_loss: 0.0214, train_mae: 0.1086, val_mae: 0.1224\n",
      "Epoch [2/1000] - train_loss: 0.0137, val_loss: 0.0159, train_mae: 0.0965, val_mae: 0.0979\n",
      "Epoch [3/1000] - train_loss: 0.0095, val_loss: 0.0206, train_mae: 0.0831, val_mae: 0.1136\n",
      "Epoch [4/1000] - train_loss: 0.0053, val_loss: 0.0129, train_mae: 0.0591, val_mae: 0.0988\n",
      "Epoch [5/1000] - train_loss: 0.0083, val_loss: 0.0110, train_mae: 0.0757, val_mae: 0.0874\n",
      "Epoch [6/1000] - train_loss: 0.0068, val_loss: 0.0128, train_mae: 0.0678, val_mae: 0.0863\n",
      "Epoch [7/1000] - train_loss: 0.0033, val_loss: 0.0098, train_mae: 0.0453, val_mae: 0.0856\n",
      "Epoch [8/1000] - train_loss: 0.0045, val_loss: 0.0088, train_mae: 0.0541, val_mae: 0.0783\n",
      "Epoch [9/1000] - train_loss: 0.0035, val_loss: 0.0100, train_mae: 0.0486, val_mae: 0.0782\n",
      "Epoch [10/1000] - train_loss: 0.0029, val_loss: 0.0094, train_mae: 0.0419, val_mae: 0.0819\n",
      "Epoch [11/1000] - train_loss: 0.0030, val_loss: 0.0105, train_mae: 0.0429, val_mae: 0.0842\n",
      "Epoch [12/1000] - train_loss: 0.0041, val_loss: 0.0107, train_mae: 0.0497, val_mae: 0.0846\n",
      "Epoch [13/1000] - train_loss: 0.0027, val_loss: 0.0098, train_mae: 0.0392, val_mae: 0.0825\n",
      "Epoch [14/1000] - train_loss: 0.0032, val_loss: 0.0108, train_mae: 0.0459, val_mae: 0.0795\n",
      "Epoch [15/1000] - train_loss: 0.0039, val_loss: 0.0098, train_mae: 0.0513, val_mae: 0.0811\n",
      "Epoch [16/1000] - train_loss: 0.0026, val_loss: 0.0096, train_mae: 0.0381, val_mae: 0.0829\n",
      "Epoch [17/1000] - train_loss: 0.0029, val_loss: 0.0103, train_mae: 0.0424, val_mae: 0.0804\n",
      "Epoch [18/1000] - train_loss: 0.0026, val_loss: 0.0088, train_mae: 0.0379, val_mae: 0.0792\n",
      "Epoch [19/1000] - train_loss: 0.0035, val_loss: 0.0085, train_mae: 0.0448, val_mae: 0.0778\n",
      "Epoch [20/1000] - train_loss: 0.0028, val_loss: 0.0086, train_mae: 0.0428, val_mae: 0.0779\n",
      "Epoch [21/1000] - train_loss: 0.0037, val_loss: 0.0086, train_mae: 0.0454, val_mae: 0.0794\n",
      "Epoch [22/1000] - train_loss: 0.0024, val_loss: 0.0088, train_mae: 0.0371, val_mae: 0.0810\n",
      "Epoch [23/1000] - train_loss: 0.0025, val_loss: 0.0095, train_mae: 0.0408, val_mae: 0.0848\n",
      "Epoch [24/1000] - train_loss: 0.0030, val_loss: 0.0104, train_mae: 0.0424, val_mae: 0.0868\n",
      "Epoch [25/1000] - train_loss: 0.0028, val_loss: 0.0102, train_mae: 0.0388, val_mae: 0.0866\n",
      "Epoch [26/1000] - train_loss: 0.0026, val_loss: 0.0087, train_mae: 0.0414, val_mae: 0.0820\n",
      "Epoch [27/1000] - train_loss: 0.0024, val_loss: 0.0072, train_mae: 0.0406, val_mae: 0.0743\n",
      "Epoch [28/1000] - train_loss: 0.0025, val_loss: 0.0072, train_mae: 0.0395, val_mae: 0.0748\n",
      "Epoch [29/1000] - train_loss: 0.0022, val_loss: 0.0083, train_mae: 0.0373, val_mae: 0.0720\n",
      "Epoch [30/1000] - train_loss: 0.0028, val_loss: 0.0076, train_mae: 0.0411, val_mae: 0.0741\n",
      "Epoch [31/1000] - train_loss: 0.0029, val_loss: 0.0084, train_mae: 0.0441, val_mae: 0.0752\n",
      "Epoch [32/1000] - train_loss: 0.0024, val_loss: 0.0089, train_mae: 0.0395, val_mae: 0.0777\n",
      "Epoch [33/1000] - train_loss: 0.0027, val_loss: 0.0080, train_mae: 0.0427, val_mae: 0.0733\n",
      "Epoch [34/1000] - train_loss: 0.0030, val_loss: 0.0076, train_mae: 0.0425, val_mae: 0.0728\n",
      "Epoch [35/1000] - train_loss: 0.0021, val_loss: 0.0075, train_mae: 0.0361, val_mae: 0.0733\n",
      "Epoch [36/1000] - train_loss: 0.0018, val_loss: 0.0088, train_mae: 0.0327, val_mae: 0.0758\n",
      "Epoch [37/1000] - train_loss: 0.0024, val_loss: 0.0088, train_mae: 0.0384, val_mae: 0.0781\n",
      "Epoch [38/1000] - train_loss: 0.0031, val_loss: 0.0080, train_mae: 0.0442, val_mae: 0.0731\n",
      "Epoch [39/1000] - train_loss: 0.0021, val_loss: 0.0068, train_mae: 0.0365, val_mae: 0.0678\n",
      "Epoch [40/1000] - train_loss: 0.0022, val_loss: 0.0066, train_mae: 0.0376, val_mae: 0.0655\n",
      "Epoch [41/1000] - train_loss: 0.0022, val_loss: 0.0074, train_mae: 0.0389, val_mae: 0.0727\n",
      "Epoch [42/1000] - train_loss: 0.0022, val_loss: 0.0084, train_mae: 0.0383, val_mae: 0.0687\n",
      "Epoch [43/1000] - train_loss: 0.0019, val_loss: 0.0068, train_mae: 0.0351, val_mae: 0.0689\n",
      "Epoch [44/1000] - train_loss: 0.0026, val_loss: 0.0073, train_mae: 0.0417, val_mae: 0.0655\n",
      "Epoch [45/1000] - train_loss: 0.0025, val_loss: 0.0065, train_mae: 0.0403, val_mae: 0.0652\n",
      "Epoch [46/1000] - train_loss: 0.0024, val_loss: 0.0074, train_mae: 0.0377, val_mae: 0.0690\n",
      "Epoch [47/1000] - train_loss: 0.0019, val_loss: 0.0069, train_mae: 0.0337, val_mae: 0.0673\n",
      "Epoch [48/1000] - train_loss: 0.0021, val_loss: 0.0064, train_mae: 0.0366, val_mae: 0.0684\n",
      "Epoch [49/1000] - train_loss: 0.0026, val_loss: 0.0095, train_mae: 0.0415, val_mae: 0.0719\n",
      "Epoch [50/1000] - train_loss: 0.0040, val_loss: 0.0116, train_mae: 0.0524, val_mae: 0.0976\n",
      "Epoch [51/1000] - train_loss: 0.0044, val_loss: 0.0116, train_mae: 0.0532, val_mae: 0.0811\n",
      "Epoch [52/1000] - train_loss: 0.0035, val_loss: 0.0091, train_mae: 0.0514, val_mae: 0.0788\n",
      "Epoch [53/1000] - train_loss: 0.0022, val_loss: 0.0073, train_mae: 0.0389, val_mae: 0.0681\n",
      "Epoch [54/1000] - train_loss: 0.0016, val_loss: 0.0055, train_mae: 0.0346, val_mae: 0.0605\n",
      "Epoch [55/1000] - train_loss: 0.0015, val_loss: 0.0047, train_mae: 0.0279, val_mae: 0.0532\n",
      "Epoch [56/1000] - train_loss: 0.0015, val_loss: 0.0052, train_mae: 0.0314, val_mae: 0.0572\n",
      "Epoch [57/1000] - train_loss: 0.0016, val_loss: 0.0063, train_mae: 0.0338, val_mae: 0.0653\n",
      "Epoch [58/1000] - train_loss: 0.0017, val_loss: 0.0067, train_mae: 0.0332, val_mae: 0.0687\n",
      "Epoch [59/1000] - train_loss: 0.0016, val_loss: 0.0066, train_mae: 0.0334, val_mae: 0.0690\n",
      "Epoch [60/1000] - train_loss: 0.0019, val_loss: 0.0062, train_mae: 0.0354, val_mae: 0.0666\n",
      "Epoch [61/1000] - train_loss: 0.0015, val_loss: 0.0054, train_mae: 0.0319, val_mae: 0.0631\n",
      "Epoch [62/1000] - train_loss: 0.0013, val_loss: 0.0059, train_mae: 0.0291, val_mae: 0.0630\n",
      "Epoch [63/1000] - train_loss: 0.0012, val_loss: 0.0064, train_mae: 0.0284, val_mae: 0.0676\n",
      "Epoch [64/1000] - train_loss: 0.0018, val_loss: 0.0071, train_mae: 0.0335, val_mae: 0.0686\n",
      "Epoch [65/1000] - train_loss: 0.0015, val_loss: 0.0060, train_mae: 0.0320, val_mae: 0.0654\n",
      "Epoch [66/1000] - train_loss: 0.0012, val_loss: 0.0057, train_mae: 0.0282, val_mae: 0.0610\n",
      "Epoch [67/1000] - train_loss: 0.0015, val_loss: 0.0058, train_mae: 0.0322, val_mae: 0.0649\n",
      "Epoch [68/1000] - train_loss: 0.0016, val_loss: 0.0082, train_mae: 0.0316, val_mae: 0.0669\n",
      "Epoch [69/1000] - train_loss: 0.0016, val_loss: 0.0069, train_mae: 0.0313, val_mae: 0.0705\n",
      "Epoch [70/1000] - train_loss: 0.0017, val_loss: 0.0086, train_mae: 0.0310, val_mae: 0.0692\n",
      "Epoch [71/1000] - train_loss: 0.0020, val_loss: 0.0061, train_mae: 0.0356, val_mae: 0.0666\n",
      "Epoch [72/1000] - train_loss: 0.0017, val_loss: 0.0055, train_mae: 0.0334, val_mae: 0.0607\n",
      "Epoch [73/1000] - train_loss: 0.0016, val_loss: 0.0055, train_mae: 0.0321, val_mae: 0.0618\n",
      "Epoch [74/1000] - train_loss: 0.0016, val_loss: 0.0058, train_mae: 0.0325, val_mae: 0.0636\n",
      "Epoch [75/1000] - train_loss: 0.0013, val_loss: 0.0058, train_mae: 0.0309, val_mae: 0.0640\n",
      "Epoch [76/1000] - train_loss: 0.0012, val_loss: 0.0055, train_mae: 0.0283, val_mae: 0.0622\n",
      "Epoch [77/1000] - train_loss: 0.0012, val_loss: 0.0048, train_mae: 0.0292, val_mae: 0.0572\n",
      "Epoch [78/1000] - train_loss: 0.0013, val_loss: 0.0044, train_mae: 0.0289, val_mae: 0.0554\n",
      "Epoch [79/1000] - train_loss: 0.0010, val_loss: 0.0043, train_mae: 0.0259, val_mae: 0.0561\n",
      "Epoch [80/1000] - train_loss: 0.0013, val_loss: 0.0050, train_mae: 0.0292, val_mae: 0.0609\n",
      "Epoch [81/1000] - train_loss: 0.0011, val_loss: 0.0058, train_mae: 0.0270, val_mae: 0.0621\n",
      "Epoch [82/1000] - train_loss: 0.0010, val_loss: 0.0051, train_mae: 0.0243, val_mae: 0.0604\n",
      "Epoch [83/1000] - train_loss: 0.0013, val_loss: 0.0050, train_mae: 0.0294, val_mae: 0.0596\n",
      "Epoch [84/1000] - train_loss: 0.0012, val_loss: 0.0058, train_mae: 0.0274, val_mae: 0.0630\n",
      "Epoch [85/1000] - train_loss: 0.0018, val_loss: 0.0053, train_mae: 0.0345, val_mae: 0.0604\n",
      "Epoch [86/1000] - train_loss: 0.0012, val_loss: 0.0059, train_mae: 0.0294, val_mae: 0.0641\n",
      "Epoch [87/1000] - train_loss: 0.0012, val_loss: 0.0053, train_mae: 0.0266, val_mae: 0.0619\n",
      "Epoch [88/1000] - train_loss: 0.0013, val_loss: 0.0050, train_mae: 0.0302, val_mae: 0.0614\n",
      "Epoch [89/1000] - train_loss: 0.0010, val_loss: 0.0068, train_mae: 0.0245, val_mae: 0.0673\n",
      "Epoch [90/1000] - train_loss: 0.0012, val_loss: 0.0059, train_mae: 0.0286, val_mae: 0.0639\n",
      "Epoch [91/1000] - train_loss: 0.0011, val_loss: 0.0059, train_mae: 0.0255, val_mae: 0.0630\n",
      "Epoch [92/1000] - train_loss: 0.0013, val_loss: 0.0044, train_mae: 0.0309, val_mae: 0.0551\n",
      "Epoch [93/1000] - train_loss: 0.0012, val_loss: 0.0050, train_mae: 0.0274, val_mae: 0.0572\n",
      "Epoch [94/1000] - train_loss: 0.0012, val_loss: 0.0062, train_mae: 0.0279, val_mae: 0.0624\n",
      "Epoch [95/1000] - train_loss: 0.0016, val_loss: 0.0064, train_mae: 0.0321, val_mae: 0.0632\n",
      "Epoch [96/1000] - train_loss: 0.0014, val_loss: 0.0054, train_mae: 0.0286, val_mae: 0.0587\n",
      "Epoch [97/1000] - train_loss: 0.0011, val_loss: 0.0051, train_mae: 0.0273, val_mae: 0.0570\n",
      "Epoch [98/1000] - train_loss: 0.0010, val_loss: 0.0057, train_mae: 0.0260, val_mae: 0.0603\n",
      "Epoch [99/1000] - train_loss: 0.0014, val_loss: 0.0062, train_mae: 0.0299, val_mae: 0.0630\n",
      "Epoch [100/1000] - train_loss: 0.0013, val_loss: 0.0081, train_mae: 0.0298, val_mae: 0.0733\n",
      "Epoch [101/1000] - train_loss: 0.0013, val_loss: 0.0072, train_mae: 0.0314, val_mae: 0.0729\n",
      "Epoch [102/1000] - train_loss: 0.0009, val_loss: 0.0059, train_mae: 0.0244, val_mae: 0.0639\n",
      "Epoch [103/1000] - train_loss: 0.0012, val_loss: 0.0050, train_mae: 0.0285, val_mae: 0.0602\n",
      "Epoch [104/1000] - train_loss: 0.0011, val_loss: 0.0051, train_mae: 0.0260, val_mae: 0.0592\n",
      "Epoch [105/1000] - train_loss: 0.0020, val_loss: 0.0050, train_mae: 0.0358, val_mae: 0.0593\n",
      "Epoch [106/1000] - train_loss: 0.0010, val_loss: 0.0054, train_mae: 0.0240, val_mae: 0.0598\n",
      "Epoch [107/1000] - train_loss: 0.0011, val_loss: 0.0052, train_mae: 0.0258, val_mae: 0.0627\n",
      "Epoch [108/1000] - train_loss: 0.0012, val_loss: 0.0050, train_mae: 0.0271, val_mae: 0.0571\n",
      "Epoch [109/1000] - train_loss: 0.0015, val_loss: 0.0045, train_mae: 0.0315, val_mae: 0.0558\n",
      "Epoch [110/1000] - train_loss: 0.0014, val_loss: 0.0048, train_mae: 0.0314, val_mae: 0.0493\n",
      "Epoch [111/1000] - train_loss: 0.0023, val_loss: 0.0072, train_mae: 0.0395, val_mae: 0.0695\n",
      "Epoch [112/1000] - train_loss: 0.0019, val_loss: 0.0053, train_mae: 0.0375, val_mae: 0.0605\n",
      "Epoch [113/1000] - train_loss: 0.0010, val_loss: 0.0057, train_mae: 0.0266, val_mae: 0.0620\n",
      "Epoch [114/1000] - train_loss: 0.0012, val_loss: 0.0052, train_mae: 0.0272, val_mae: 0.0585\n",
      "Epoch [115/1000] - train_loss: 0.0010, val_loss: 0.0051, train_mae: 0.0251, val_mae: 0.0580\n",
      "Epoch [116/1000] - train_loss: 0.0012, val_loss: 0.0063, train_mae: 0.0266, val_mae: 0.0622\n",
      "Epoch [117/1000] - train_loss: 0.0017, val_loss: 0.0060, train_mae: 0.0307, val_mae: 0.0608\n",
      "Epoch [118/1000] - train_loss: 0.0012, val_loss: 0.0072, train_mae: 0.0272, val_mae: 0.0690\n",
      "Epoch [119/1000] - train_loss: 0.0012, val_loss: 0.0063, train_mae: 0.0290, val_mae: 0.0636\n",
      "Epoch [120/1000] - train_loss: 0.0009, val_loss: 0.0049, train_mae: 0.0244, val_mae: 0.0567\n",
      "Epoch [121/1000] - train_loss: 0.0009, val_loss: 0.0051, train_mae: 0.0223, val_mae: 0.0591\n",
      "Epoch [122/1000] - train_loss: 0.0008, val_loss: 0.0050, train_mae: 0.0227, val_mae: 0.0539\n",
      "Epoch [123/1000] - train_loss: 0.0010, val_loss: 0.0050, train_mae: 0.0266, val_mae: 0.0547\n",
      "Epoch [124/1000] - train_loss: 0.0010, val_loss: 0.0050, train_mae: 0.0240, val_mae: 0.0562\n",
      "Epoch [125/1000] - train_loss: 0.0007, val_loss: 0.0048, train_mae: 0.0205, val_mae: 0.0564\n",
      "Epoch [126/1000] - train_loss: 0.0007, val_loss: 0.0042, train_mae: 0.0224, val_mae: 0.0533\n",
      "Epoch [127/1000] - train_loss: 0.0006, val_loss: 0.0046, train_mae: 0.0197, val_mae: 0.0578\n",
      "Epoch [128/1000] - train_loss: 0.0007, val_loss: 0.0056, train_mae: 0.0213, val_mae: 0.0644\n",
      "Epoch [129/1000] - train_loss: 0.0007, val_loss: 0.0054, train_mae: 0.0213, val_mae: 0.0608\n",
      "Epoch [130/1000] - train_loss: 0.0008, val_loss: 0.0053, train_mae: 0.0249, val_mae: 0.0626\n",
      "Epoch [131/1000] - train_loss: 0.0007, val_loss: 0.0051, train_mae: 0.0214, val_mae: 0.0606\n",
      "Epoch [132/1000] - train_loss: 0.0009, val_loss: 0.0045, train_mae: 0.0245, val_mae: 0.0546\n",
      "Epoch [133/1000] - train_loss: 0.0006, val_loss: 0.0048, train_mae: 0.0189, val_mae: 0.0579\n",
      "Epoch [134/1000] - train_loss: 0.0007, val_loss: 0.0047, train_mae: 0.0198, val_mae: 0.0524\n",
      "Epoch [135/1000] - train_loss: 0.0008, val_loss: 0.0075, train_mae: 0.0205, val_mae: 0.0685\n",
      "Epoch [136/1000] - train_loss: 0.0013, val_loss: 0.0065, train_mae: 0.0277, val_mae: 0.0649\n",
      "Epoch [137/1000] - train_loss: 0.0014, val_loss: 0.0079, train_mae: 0.0315, val_mae: 0.0690\n",
      "Epoch [138/1000] - train_loss: 0.0017, val_loss: 0.0041, train_mae: 0.0336, val_mae: 0.0531\n",
      "Epoch [139/1000] - train_loss: 0.0012, val_loss: 0.0033, train_mae: 0.0275, val_mae: 0.0492\n",
      "Epoch [140/1000] - train_loss: 0.0009, val_loss: 0.0040, train_mae: 0.0235, val_mae: 0.0556\n",
      "Epoch [141/1000] - train_loss: 0.0009, val_loss: 0.0049, train_mae: 0.0245, val_mae: 0.0604\n",
      "Epoch [142/1000] - train_loss: 0.0010, val_loss: 0.0049, train_mae: 0.0248, val_mae: 0.0598\n",
      "Epoch [143/1000] - train_loss: 0.0006, val_loss: 0.0044, train_mae: 0.0195, val_mae: 0.0579\n",
      "Epoch [144/1000] - train_loss: 0.0006, val_loss: 0.0043, train_mae: 0.0201, val_mae: 0.0568\n",
      "Epoch [145/1000] - train_loss: 0.0006, val_loss: 0.0045, train_mae: 0.0208, val_mae: 0.0542\n",
      "Epoch [146/1000] - train_loss: 0.0009, val_loss: 0.0056, train_mae: 0.0230, val_mae: 0.0621\n",
      "Epoch [147/1000] - train_loss: 0.0007, val_loss: 0.0045, train_mae: 0.0216, val_mae: 0.0566\n",
      "Epoch [148/1000] - train_loss: 0.0008, val_loss: 0.0052, train_mae: 0.0218, val_mae: 0.0610\n",
      "Epoch [149/1000] - train_loss: 0.0006, val_loss: 0.0042, train_mae: 0.0204, val_mae: 0.0538\n",
      "Epoch [150/1000] - train_loss: 0.0006, val_loss: 0.0041, train_mae: 0.0185, val_mae: 0.0547\n",
      "Epoch [151/1000] - train_loss: 0.0006, val_loss: 0.0043, train_mae: 0.0204, val_mae: 0.0550\n",
      "Epoch [152/1000] - train_loss: 0.0005, val_loss: 0.0050, train_mae: 0.0185, val_mae: 0.0568\n",
      "Epoch [153/1000] - train_loss: 0.0007, val_loss: 0.0064, train_mae: 0.0201, val_mae: 0.0656\n",
      "Epoch [154/1000] - train_loss: 0.0009, val_loss: 0.0057, train_mae: 0.0241, val_mae: 0.0602\n",
      "Epoch [155/1000] - train_loss: 0.0013, val_loss: 0.0058, train_mae: 0.0305, val_mae: 0.0649\n",
      "Epoch [156/1000] - train_loss: 0.0012, val_loss: 0.0065, train_mae: 0.0283, val_mae: 0.0657\n",
      "Epoch [157/1000] - train_loss: 0.0017, val_loss: 0.0044, train_mae: 0.0350, val_mae: 0.0531\n",
      "Epoch [158/1000] - train_loss: 0.0007, val_loss: 0.0043, train_mae: 0.0220, val_mae: 0.0525\n",
      "Epoch [159/1000] - train_loss: 0.0008, val_loss: 0.0065, train_mae: 0.0232, val_mae: 0.0649\n",
      "Epoch [160/1000] - train_loss: 0.0008, val_loss: 0.0063, train_mae: 0.0223, val_mae: 0.0647\n",
      "Epoch [161/1000] - train_loss: 0.0015, val_loss: 0.0065, train_mae: 0.0326, val_mae: 0.0672\n",
      "Epoch [162/1000] - train_loss: 0.0013, val_loss: 0.0051, train_mae: 0.0305, val_mae: 0.0611\n",
      "Epoch [163/1000] - train_loss: 0.0011, val_loss: 0.0043, train_mae: 0.0272, val_mae: 0.0489\n",
      "Epoch [164/1000] - train_loss: 0.0014, val_loss: 0.0042, train_mae: 0.0306, val_mae: 0.0572\n",
      "Epoch [165/1000] - train_loss: 0.0006, val_loss: 0.0049, train_mae: 0.0193, val_mae: 0.0592\n",
      "Epoch [166/1000] - train_loss: 0.0006, val_loss: 0.0052, train_mae: 0.0192, val_mae: 0.0562\n",
      "Epoch [167/1000] - train_loss: 0.0008, val_loss: 0.0052, train_mae: 0.0235, val_mae: 0.0608\n",
      "Epoch [168/1000] - train_loss: 0.0006, val_loss: 0.0041, train_mae: 0.0192, val_mae: 0.0514\n",
      "Epoch [169/1000] - train_loss: 0.0006, val_loss: 0.0049, train_mae: 0.0210, val_mae: 0.0610\n",
      "Epoch [170/1000] - train_loss: 0.0007, val_loss: 0.0056, train_mae: 0.0192, val_mae: 0.0658\n",
      "Epoch [171/1000] - train_loss: 0.0006, val_loss: 0.0052, train_mae: 0.0188, val_mae: 0.0608\n",
      "Epoch [172/1000] - train_loss: 0.0007, val_loss: 0.0048, train_mae: 0.0203, val_mae: 0.0613\n",
      "Epoch [173/1000] - train_loss: 0.0005, val_loss: 0.0044, train_mae: 0.0172, val_mae: 0.0586\n",
      "Epoch [174/1000] - train_loss: 0.0006, val_loss: 0.0045, train_mae: 0.0193, val_mae: 0.0593\n",
      "Epoch [175/1000] - train_loss: 0.0004, val_loss: 0.0044, train_mae: 0.0156, val_mae: 0.0558\n",
      "Epoch [176/1000] - train_loss: 0.0005, val_loss: 0.0045, train_mae: 0.0185, val_mae: 0.0586\n",
      "Epoch [177/1000] - train_loss: 0.0004, val_loss: 0.0043, train_mae: 0.0175, val_mae: 0.0578\n",
      "Epoch [178/1000] - train_loss: 0.0007, val_loss: 0.0042, train_mae: 0.0203, val_mae: 0.0571\n",
      "Epoch [179/1000] - train_loss: 0.0006, val_loss: 0.0043, train_mae: 0.0186, val_mae: 0.0579\n",
      "Epoch [180/1000] - train_loss: 0.0007, val_loss: 0.0050, train_mae: 0.0202, val_mae: 0.0616\n",
      "Epoch [181/1000] - train_loss: 0.0005, val_loss: 0.0046, train_mae: 0.0179, val_mae: 0.0585\n",
      "Epoch [182/1000] - train_loss: 0.0004, val_loss: 0.0047, train_mae: 0.0156, val_mae: 0.0582\n",
      "Epoch [183/1000] - train_loss: 0.0004, val_loss: 0.0038, train_mae: 0.0169, val_mae: 0.0501\n",
      "Epoch [184/1000] - train_loss: 0.0005, val_loss: 0.0041, train_mae: 0.0162, val_mae: 0.0549\n",
      "Epoch [185/1000] - train_loss: 0.0004, val_loss: 0.0040, train_mae: 0.0162, val_mae: 0.0535\n",
      "Epoch [186/1000] - train_loss: 0.0004, val_loss: 0.0043, train_mae: 0.0155, val_mae: 0.0575\n",
      "Epoch [187/1000] - train_loss: 0.0004, val_loss: 0.0040, train_mae: 0.0149, val_mae: 0.0552\n",
      "Epoch [188/1000] - train_loss: 0.0004, val_loss: 0.0047, train_mae: 0.0153, val_mae: 0.0587\n",
      "Epoch [189/1000] - train_loss: 0.0004, val_loss: 0.0040, train_mae: 0.0167, val_mae: 0.0483\n",
      "Epoch [190/1000] - train_loss: 0.0008, val_loss: 0.0043, train_mae: 0.0221, val_mae: 0.0557\n",
      "Epoch [191/1000] - train_loss: 0.0004, val_loss: 0.0054, train_mae: 0.0166, val_mae: 0.0625\n",
      "Epoch [192/1000] - train_loss: 0.0005, val_loss: 0.0046, train_mae: 0.0184, val_mae: 0.0582\n",
      "Epoch [193/1000] - train_loss: 0.0004, val_loss: 0.0044, train_mae: 0.0153, val_mae: 0.0572\n",
      "Epoch [194/1000] - train_loss: 0.0004, val_loss: 0.0037, train_mae: 0.0152, val_mae: 0.0520\n",
      "Epoch [195/1000] - train_loss: 0.0004, val_loss: 0.0043, train_mae: 0.0172, val_mae: 0.0582\n",
      "Epoch [196/1000] - train_loss: 0.0005, val_loss: 0.0044, train_mae: 0.0178, val_mae: 0.0587\n",
      "Epoch [197/1000] - train_loss: 0.0004, val_loss: 0.0038, train_mae: 0.0171, val_mae: 0.0510\n",
      "Epoch [198/1000] - train_loss: 0.0008, val_loss: 0.0038, train_mae: 0.0227, val_mae: 0.0535\n",
      "Epoch [199/1000] - train_loss: 0.0005, val_loss: 0.0049, train_mae: 0.0159, val_mae: 0.0594\n",
      "Epoch [200/1000] - train_loss: 0.0006, val_loss: 0.0042, train_mae: 0.0200, val_mae: 0.0500\n",
      "Epoch [201/1000] - train_loss: 0.0009, val_loss: 0.0038, train_mae: 0.0243, val_mae: 0.0508\n",
      "Epoch [202/1000] - train_loss: 0.0009, val_loss: 0.0053, train_mae: 0.0234, val_mae: 0.0630\n",
      "Epoch [203/1000] - train_loss: 0.0007, val_loss: 0.0040, train_mae: 0.0212, val_mae: 0.0541\n",
      "Epoch [204/1000] - train_loss: 0.0008, val_loss: 0.0045, train_mae: 0.0226, val_mae: 0.0491\n",
      "Epoch [205/1000] - train_loss: 0.0019, val_loss: 0.0043, train_mae: 0.0353, val_mae: 0.0553\n",
      "Epoch [206/1000] - train_loss: 0.0014, val_loss: 0.0072, train_mae: 0.0303, val_mae: 0.0729\n",
      "Epoch [207/1000] - train_loss: 0.0007, val_loss: 0.0050, train_mae: 0.0223, val_mae: 0.0602\n",
      "Epoch [208/1000] - train_loss: 0.0007, val_loss: 0.0061, train_mae: 0.0208, val_mae: 0.0621\n",
      "Epoch [209/1000] - train_loss: 0.0011, val_loss: 0.0039, train_mae: 0.0259, val_mae: 0.0546\n",
      "Epoch [210/1000] - train_loss: 0.0004, val_loss: 0.0062, train_mae: 0.0166, val_mae: 0.0678\n",
      "Epoch [211/1000] - train_loss: 0.0006, val_loss: 0.0051, train_mae: 0.0207, val_mae: 0.0617\n",
      "Epoch [212/1000] - train_loss: 0.0005, val_loss: 0.0038, train_mae: 0.0182, val_mae: 0.0527\n",
      "Epoch [213/1000] - train_loss: 0.0005, val_loss: 0.0044, train_mae: 0.0167, val_mae: 0.0554\n",
      "Epoch [214/1000] - train_loss: 0.0007, val_loss: 0.0039, train_mae: 0.0208, val_mae: 0.0523\n",
      "Epoch [215/1000] - train_loss: 0.0005, val_loss: 0.0042, train_mae: 0.0175, val_mae: 0.0568\n",
      "Epoch [216/1000] - train_loss: 0.0005, val_loss: 0.0049, train_mae: 0.0188, val_mae: 0.0586\n",
      "Epoch [217/1000] - train_loss: 0.0006, val_loss: 0.0043, train_mae: 0.0200, val_mae: 0.0564\n",
      "Epoch [218/1000] - train_loss: 0.0005, val_loss: 0.0049, train_mae: 0.0186, val_mae: 0.0626\n",
      "Epoch [219/1000] - train_loss: 0.0005, val_loss: 0.0052, train_mae: 0.0176, val_mae: 0.0637\n",
      "Epoch [220/1000] - train_loss: 0.0004, val_loss: 0.0038, train_mae: 0.0159, val_mae: 0.0550\n",
      "Epoch [221/1000] - train_loss: 0.0003, val_loss: 0.0048, train_mae: 0.0136, val_mae: 0.0592\n",
      "Epoch [222/1000] - train_loss: 0.0004, val_loss: 0.0042, train_mae: 0.0165, val_mae: 0.0571\n",
      "Epoch [223/1000] - train_loss: 0.0003, val_loss: 0.0039, train_mae: 0.0142, val_mae: 0.0517\n",
      "Epoch [224/1000] - train_loss: 0.0006, val_loss: 0.0059, train_mae: 0.0191, val_mae: 0.0637\n",
      "Epoch [225/1000] - train_loss: 0.0007, val_loss: 0.0037, train_mae: 0.0225, val_mae: 0.0512\n",
      "Epoch [226/1000] - train_loss: 0.0006, val_loss: 0.0042, train_mae: 0.0191, val_mae: 0.0574\n",
      "Epoch [227/1000] - train_loss: 0.0005, val_loss: 0.0068, train_mae: 0.0189, val_mae: 0.0669\n",
      "Epoch [228/1000] - train_loss: 0.0013, val_loss: 0.0037, train_mae: 0.0309, val_mae: 0.0499\n",
      "Epoch [229/1000] - train_loss: 0.0008, val_loss: 0.0039, train_mae: 0.0240, val_mae: 0.0550\n",
      "Epoch [230/1000] - train_loss: 0.0005, val_loss: 0.0064, train_mae: 0.0175, val_mae: 0.0659\n",
      "Epoch [231/1000] - train_loss: 0.0007, val_loss: 0.0062, train_mae: 0.0214, val_mae: 0.0650\n",
      "Epoch [232/1000] - train_loss: 0.0015, val_loss: 0.0065, train_mae: 0.0326, val_mae: 0.0685\n",
      "Epoch [233/1000] - train_loss: 0.0007, val_loss: 0.0040, train_mae: 0.0206, val_mae: 0.0558\n",
      "Epoch [234/1000] - train_loss: 0.0005, val_loss: 0.0036, train_mae: 0.0188, val_mae: 0.0491\n",
      "Epoch [235/1000] - train_loss: 0.0007, val_loss: 0.0068, train_mae: 0.0200, val_mae: 0.0673\n",
      "Epoch [236/1000] - train_loss: 0.0012, val_loss: 0.0049, train_mae: 0.0303, val_mae: 0.0532\n",
      "Epoch [237/1000] - train_loss: 0.0010, val_loss: 0.0049, train_mae: 0.0251, val_mae: 0.0611\n",
      "Epoch [238/1000] - train_loss: 0.0008, val_loss: 0.0040, train_mae: 0.0221, val_mae: 0.0540\n",
      "Epoch [239/1000] - train_loss: 0.0005, val_loss: 0.0037, train_mae: 0.0190, val_mae: 0.0492\n",
      "Epoch [240/1000] - train_loss: 0.0008, val_loss: 0.0051, train_mae: 0.0217, val_mae: 0.0601\n",
      "Epoch [241/1000] - train_loss: 0.0006, val_loss: 0.0056, train_mae: 0.0181, val_mae: 0.0660\n",
      "Epoch [242/1000] - train_loss: 0.0005, val_loss: 0.0049, train_mae: 0.0183, val_mae: 0.0617\n",
      "Epoch [243/1000] - train_loss: 0.0003, val_loss: 0.0050, train_mae: 0.0129, val_mae: 0.0625\n",
      "Epoch [244/1000] - train_loss: 0.0003, val_loss: 0.0040, train_mae: 0.0132, val_mae: 0.0559\n",
      "Epoch [245/1000] - train_loss: 0.0003, val_loss: 0.0039, train_mae: 0.0148, val_mae: 0.0552\n",
      "Epoch [246/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0123, val_mae: 0.0583\n",
      "Epoch [247/1000] - train_loss: 0.0003, val_loss: 0.0047, train_mae: 0.0130, val_mae: 0.0589\n",
      "Epoch [248/1000] - train_loss: 0.0008, val_loss: 0.0048, train_mae: 0.0235, val_mae: 0.0601\n",
      "Epoch [249/1000] - train_loss: 0.0007, val_loss: 0.0056, train_mae: 0.0209, val_mae: 0.0640\n",
      "Epoch [250/1000] - train_loss: 0.0007, val_loss: 0.0048, train_mae: 0.0220, val_mae: 0.0597\n",
      "Epoch [251/1000] - train_loss: 0.0005, val_loss: 0.0038, train_mae: 0.0198, val_mae: 0.0496\n",
      "Epoch [252/1000] - train_loss: 0.0007, val_loss: 0.0043, train_mae: 0.0205, val_mae: 0.0567\n",
      "Epoch [253/1000] - train_loss: 0.0003, val_loss: 0.0042, train_mae: 0.0151, val_mae: 0.0569\n",
      "Epoch [254/1000] - train_loss: 0.0004, val_loss: 0.0055, train_mae: 0.0170, val_mae: 0.0645\n",
      "Epoch [255/1000] - train_loss: 0.0003, val_loss: 0.0053, train_mae: 0.0146, val_mae: 0.0643\n",
      "Epoch [256/1000] - train_loss: 0.0003, val_loss: 0.0050, train_mae: 0.0138, val_mae: 0.0622\n",
      "Epoch [257/1000] - train_loss: 0.0003, val_loss: 0.0056, train_mae: 0.0125, val_mae: 0.0622\n",
      "Epoch [258/1000] - train_loss: 0.0004, val_loss: 0.0044, train_mae: 0.0166, val_mae: 0.0558\n",
      "Epoch [259/1000] - train_loss: 0.0008, val_loss: 0.0045, train_mae: 0.0226, val_mae: 0.0585\n",
      "Epoch [260/1000] - train_loss: 0.0008, val_loss: 0.0044, train_mae: 0.0224, val_mae: 0.0573\n",
      "Epoch [261/1000] - train_loss: 0.0004, val_loss: 0.0039, train_mae: 0.0152, val_mae: 0.0539\n",
      "Epoch [262/1000] - train_loss: 0.0003, val_loss: 0.0038, train_mae: 0.0157, val_mae: 0.0521\n",
      "Epoch [263/1000] - train_loss: 0.0004, val_loss: 0.0040, train_mae: 0.0158, val_mae: 0.0552\n",
      "Epoch [264/1000] - train_loss: 0.0003, val_loss: 0.0042, train_mae: 0.0138, val_mae: 0.0546\n",
      "Epoch [265/1000] - train_loss: 0.0004, val_loss: 0.0046, train_mae: 0.0148, val_mae: 0.0598\n",
      "Epoch [266/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0121, val_mae: 0.0618\n",
      "Epoch [267/1000] - train_loss: 0.0002, val_loss: 0.0055, train_mae: 0.0119, val_mae: 0.0634\n",
      "Epoch [268/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0126, val_mae: 0.0585\n",
      "Epoch [269/1000] - train_loss: 0.0003, val_loss: 0.0049, train_mae: 0.0141, val_mae: 0.0592\n",
      "Epoch [270/1000] - train_loss: 0.0002, val_loss: 0.0040, train_mae: 0.0118, val_mae: 0.0537\n",
      "Epoch [271/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0120, val_mae: 0.0591\n",
      "Epoch [272/1000] - train_loss: 0.0003, val_loss: 0.0040, train_mae: 0.0128, val_mae: 0.0554\n",
      "Epoch [273/1000] - train_loss: 0.0003, val_loss: 0.0041, train_mae: 0.0139, val_mae: 0.0565\n",
      "Epoch [274/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0123, val_mae: 0.0584\n",
      "Epoch [275/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0128, val_mae: 0.0597\n",
      "Epoch [276/1000] - train_loss: 0.0004, val_loss: 0.0047, train_mae: 0.0165, val_mae: 0.0598\n",
      "Epoch [277/1000] - train_loss: 0.0003, val_loss: 0.0049, train_mae: 0.0138, val_mae: 0.0623\n",
      "Epoch [278/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0115, val_mae: 0.0631\n",
      "Epoch [279/1000] - train_loss: 0.0002, val_loss: 0.0053, train_mae: 0.0125, val_mae: 0.0638\n",
      "Epoch [280/1000] - train_loss: 0.0004, val_loss: 0.0045, train_mae: 0.0165, val_mae: 0.0603\n",
      "Epoch [281/1000] - train_loss: 0.0003, val_loss: 0.0046, train_mae: 0.0146, val_mae: 0.0581\n",
      "Epoch [282/1000] - train_loss: 0.0003, val_loss: 0.0064, train_mae: 0.0135, val_mae: 0.0681\n",
      "Epoch [283/1000] - train_loss: 0.0005, val_loss: 0.0049, train_mae: 0.0175, val_mae: 0.0617\n",
      "Epoch [284/1000] - train_loss: 0.0004, val_loss: 0.0042, train_mae: 0.0170, val_mae: 0.0571\n",
      "Epoch [285/1000] - train_loss: 0.0003, val_loss: 0.0052, train_mae: 0.0119, val_mae: 0.0626\n",
      "Epoch [286/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0105, val_mae: 0.0602\n",
      "Epoch [287/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0109, val_mae: 0.0599\n",
      "Epoch [288/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0118, val_mae: 0.0543\n",
      "Epoch [289/1000] - train_loss: 0.0003, val_loss: 0.0039, train_mae: 0.0137, val_mae: 0.0555\n",
      "Epoch [290/1000] - train_loss: 0.0003, val_loss: 0.0051, train_mae: 0.0135, val_mae: 0.0624\n",
      "Epoch [291/1000] - train_loss: 0.0002, val_loss: 0.0042, train_mae: 0.0123, val_mae: 0.0582\n",
      "Epoch [292/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0095, val_mae: 0.0586\n",
      "Epoch [293/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0100, val_mae: 0.0597\n",
      "Epoch [294/1000] - train_loss: 0.0003, val_loss: 0.0047, train_mae: 0.0122, val_mae: 0.0598\n",
      "Epoch [295/1000] - train_loss: 0.0003, val_loss: 0.0045, train_mae: 0.0134, val_mae: 0.0588\n",
      "Epoch [296/1000] - train_loss: 0.0002, val_loss: 0.0041, train_mae: 0.0100, val_mae: 0.0553\n",
      "Epoch [297/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0119, val_mae: 0.0601\n",
      "Epoch [298/1000] - train_loss: 0.0004, val_loss: 0.0044, train_mae: 0.0159, val_mae: 0.0590\n",
      "Epoch [299/1000] - train_loss: 0.0003, val_loss: 0.0049, train_mae: 0.0122, val_mae: 0.0573\n",
      "Epoch [300/1000] - train_loss: 0.0006, val_loss: 0.0051, train_mae: 0.0191, val_mae: 0.0629\n",
      "Epoch [301/1000] - train_loss: 0.0003, val_loss: 0.0041, train_mae: 0.0133, val_mae: 0.0575\n",
      "Epoch [302/1000] - train_loss: 0.0002, val_loss: 0.0035, train_mae: 0.0109, val_mae: 0.0526\n",
      "Epoch [303/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0125, val_mae: 0.0579\n",
      "Epoch [304/1000] - train_loss: 0.0002, val_loss: 0.0051, train_mae: 0.0104, val_mae: 0.0620\n",
      "Epoch [305/1000] - train_loss: 0.0002, val_loss: 0.0043, train_mae: 0.0117, val_mae: 0.0562\n",
      "Epoch [306/1000] - train_loss: 0.0003, val_loss: 0.0039, train_mae: 0.0123, val_mae: 0.0543\n",
      "Epoch [307/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0119, val_mae: 0.0597\n",
      "Epoch [308/1000] - train_loss: 0.0002, val_loss: 0.0053, train_mae: 0.0094, val_mae: 0.0647\n",
      "Epoch [309/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0108, val_mae: 0.0595\n",
      "Epoch [310/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0105, val_mae: 0.0573\n",
      "Epoch [311/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0090, val_mae: 0.0566\n",
      "Epoch [312/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0111, val_mae: 0.0547\n",
      "Epoch [313/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0102, val_mae: 0.0527\n",
      "Epoch [314/1000] - train_loss: 0.0004, val_loss: 0.0045, train_mae: 0.0171, val_mae: 0.0588\n",
      "Epoch [315/1000] - train_loss: 0.0003, val_loss: 0.0055, train_mae: 0.0131, val_mae: 0.0631\n",
      "Epoch [316/1000] - train_loss: 0.0003, val_loss: 0.0045, train_mae: 0.0149, val_mae: 0.0594\n",
      "Epoch [317/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0119, val_mae: 0.0605\n",
      "Epoch [318/1000] - train_loss: 0.0001, val_loss: 0.0039, train_mae: 0.0088, val_mae: 0.0559\n",
      "Epoch [319/1000] - train_loss: 0.0001, val_loss: 0.0039, train_mae: 0.0098, val_mae: 0.0560\n",
      "Epoch [320/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0084, val_mae: 0.0588\n",
      "Epoch [321/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0076, val_mae: 0.0610\n",
      "Epoch [322/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0104, val_mae: 0.0582\n",
      "Epoch [323/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0105, val_mae: 0.0527\n",
      "Epoch [324/1000] - train_loss: 0.0003, val_loss: 0.0045, train_mae: 0.0136, val_mae: 0.0591\n",
      "Epoch [325/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0095, val_mae: 0.0604\n",
      "Epoch [326/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0086, val_mae: 0.0576\n",
      "Epoch [327/1000] - train_loss: 0.0002, val_loss: 0.0041, train_mae: 0.0095, val_mae: 0.0561\n",
      "Epoch [328/1000] - train_loss: 0.0001, val_loss: 0.0039, train_mae: 0.0085, val_mae: 0.0544\n",
      "Epoch [329/1000] - train_loss: 0.0002, val_loss: 0.0042, train_mae: 0.0108, val_mae: 0.0575\n",
      "Epoch [330/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0090, val_mae: 0.0600\n",
      "Epoch [331/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0091, val_mae: 0.0608\n",
      "Epoch [332/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0077, val_mae: 0.0561\n",
      "Epoch [333/1000] - train_loss: 0.0002, val_loss: 0.0040, train_mae: 0.0113, val_mae: 0.0549\n",
      "Epoch [334/1000] - train_loss: 0.0003, val_loss: 0.0042, train_mae: 0.0133, val_mae: 0.0577\n",
      "Epoch [335/1000] - train_loss: 0.0002, val_loss: 0.0038, train_mae: 0.0112, val_mae: 0.0552\n",
      "Epoch [336/1000] - train_loss: 0.0002, val_loss: 0.0038, train_mae: 0.0118, val_mae: 0.0546\n",
      "Epoch [337/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0082, val_mae: 0.0556\n",
      "Epoch [338/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0092, val_mae: 0.0574\n",
      "Epoch [339/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0093, val_mae: 0.0567\n",
      "Epoch [340/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0097, val_mae: 0.0575\n",
      "Epoch [341/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0101, val_mae: 0.0563\n",
      "Epoch [342/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0103, val_mae: 0.0587\n",
      "Epoch [343/1000] - train_loss: 0.0003, val_loss: 0.0036, train_mae: 0.0148, val_mae: 0.0518\n",
      "Epoch [344/1000] - train_loss: 0.0003, val_loss: 0.0036, train_mae: 0.0145, val_mae: 0.0530\n",
      "Epoch [345/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0094, val_mae: 0.0604\n",
      "Epoch [346/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0115, val_mae: 0.0587\n",
      "Epoch [347/1000] - train_loss: 0.0002, val_loss: 0.0040, train_mae: 0.0099, val_mae: 0.0553\n",
      "Epoch [348/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0088, val_mae: 0.0600\n",
      "Epoch [349/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0097, val_mae: 0.0601\n",
      "Epoch [350/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0098, val_mae: 0.0599\n",
      "Epoch [351/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0104, val_mae: 0.0552\n",
      "Epoch [352/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0094, val_mae: 0.0577\n",
      "Epoch [353/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0093, val_mae: 0.0574\n",
      "Epoch [354/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0091, val_mae: 0.0548\n",
      "Epoch [355/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0093, val_mae: 0.0549\n",
      "Epoch [356/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0096, val_mae: 0.0585\n",
      "Epoch [357/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0088, val_mae: 0.0581\n",
      "Epoch [358/1000] - train_loss: 0.0001, val_loss: 0.0038, train_mae: 0.0085, val_mae: 0.0547\n",
      "Epoch [359/1000] - train_loss: 0.0001, val_loss: 0.0039, train_mae: 0.0094, val_mae: 0.0550\n",
      "Epoch [360/1000] - train_loss: 0.0002, val_loss: 0.0043, train_mae: 0.0113, val_mae: 0.0582\n",
      "Epoch [361/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0085, val_mae: 0.0591\n",
      "Epoch [362/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0091, val_mae: 0.0573\n",
      "Epoch [363/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0086, val_mae: 0.0550\n",
      "Epoch [364/1000] - train_loss: 0.0002, val_loss: 0.0037, train_mae: 0.0100, val_mae: 0.0506\n",
      "Epoch [365/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0137, val_mae: 0.0552\n",
      "Epoch [366/1000] - train_loss: 0.0003, val_loss: 0.0040, train_mae: 0.0150, val_mae: 0.0539\n",
      "Epoch [367/1000] - train_loss: 0.0002, val_loss: 0.0039, train_mae: 0.0128, val_mae: 0.0543\n",
      "Epoch [368/1000] - train_loss: 0.0002, val_loss: 0.0042, train_mae: 0.0110, val_mae: 0.0570\n",
      "Epoch [369/1000] - train_loss: 0.0002, val_loss: 0.0043, train_mae: 0.0111, val_mae: 0.0576\n",
      "Epoch [370/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0081, val_mae: 0.0596\n",
      "Epoch [371/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0117, val_mae: 0.0598\n",
      "Epoch [372/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0100, val_mae: 0.0563\n",
      "Epoch [373/1000] - train_loss: 0.0003, val_loss: 0.0041, train_mae: 0.0152, val_mae: 0.0560\n",
      "Epoch [374/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0123, val_mae: 0.0609\n",
      "Epoch [375/1000] - train_loss: 0.0002, val_loss: 0.0052, train_mae: 0.0100, val_mae: 0.0620\n",
      "Epoch [376/1000] - train_loss: 0.0002, val_loss: 0.0037, train_mae: 0.0114, val_mae: 0.0506\n",
      "Epoch [377/1000] - train_loss: 0.0003, val_loss: 0.0034, train_mae: 0.0129, val_mae: 0.0493\n",
      "Epoch [378/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0129, val_mae: 0.0584\n",
      "Epoch [379/1000] - train_loss: 0.0003, val_loss: 0.0064, train_mae: 0.0131, val_mae: 0.0675\n",
      "Epoch [380/1000] - train_loss: 0.0005, val_loss: 0.0040, train_mae: 0.0190, val_mae: 0.0535\n",
      "Epoch [381/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0146, val_mae: 0.0555\n",
      "Epoch [382/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0110, val_mae: 0.0635\n",
      "Epoch [383/1000] - train_loss: 0.0004, val_loss: 0.0050, train_mae: 0.0153, val_mae: 0.0617\n",
      "Epoch [384/1000] - train_loss: 0.0002, val_loss: 0.0042, train_mae: 0.0110, val_mae: 0.0578\n",
      "Epoch [385/1000] - train_loss: 0.0001, val_loss: 0.0040, train_mae: 0.0074, val_mae: 0.0557\n",
      "Epoch [386/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0089, val_mae: 0.0578\n",
      "Epoch [387/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0113, val_mae: 0.0582\n",
      "Epoch [388/1000] - train_loss: 0.0002, val_loss: 0.0042, train_mae: 0.0104, val_mae: 0.0537\n",
      "Epoch [389/1000] - train_loss: 0.0002, val_loss: 0.0034, train_mae: 0.0120, val_mae: 0.0494\n",
      "Epoch [390/1000] - train_loss: 0.0002, val_loss: 0.0035, train_mae: 0.0121, val_mae: 0.0510\n",
      "Epoch [391/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0091, val_mae: 0.0586\n",
      "Epoch [392/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0118, val_mae: 0.0641\n",
      "Epoch [393/1000] - train_loss: 0.0004, val_loss: 0.0046, train_mae: 0.0160, val_mae: 0.0585\n",
      "Epoch [394/1000] - train_loss: 0.0003, val_loss: 0.0033, train_mae: 0.0139, val_mae: 0.0499\n",
      "Epoch [395/1000] - train_loss: 0.0002, val_loss: 0.0037, train_mae: 0.0111, val_mae: 0.0518\n",
      "Epoch [396/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0097, val_mae: 0.0580\n",
      "Epoch [397/1000] - train_loss: 0.0002, val_loss: 0.0041, train_mae: 0.0125, val_mae: 0.0544\n",
      "Epoch [398/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0101, val_mae: 0.0546\n",
      "Epoch [399/1000] - train_loss: 0.0007, val_loss: 0.0046, train_mae: 0.0233, val_mae: 0.0592\n",
      "Epoch [400/1000] - train_loss: 0.0006, val_loss: 0.0070, train_mae: 0.0191, val_mae: 0.0686\n",
      "Epoch [401/1000] - train_loss: 0.0007, val_loss: 0.0051, train_mae: 0.0233, val_mae: 0.0610\n",
      "Epoch [402/1000] - train_loss: 0.0009, val_loss: 0.0041, train_mae: 0.0242, val_mae: 0.0546\n",
      "Epoch [403/1000] - train_loss: 0.0008, val_loss: 0.0055, train_mae: 0.0195, val_mae: 0.0554\n",
      "Epoch [404/1000] - train_loss: 0.0018, val_loss: 0.0080, train_mae: 0.0354, val_mae: 0.0751\n",
      "Epoch [405/1000] - train_loss: 0.0010, val_loss: 0.0073, train_mae: 0.0236, val_mae: 0.0655\n",
      "Epoch [406/1000] - train_loss: 0.0007, val_loss: 0.0063, train_mae: 0.0224, val_mae: 0.0591\n",
      "Epoch [407/1000] - train_loss: 0.0006, val_loss: 0.0064, train_mae: 0.0199, val_mae: 0.0635\n",
      "Epoch [408/1000] - train_loss: 0.0004, val_loss: 0.0060, train_mae: 0.0155, val_mae: 0.0636\n",
      "Epoch [409/1000] - train_loss: 0.0012, val_loss: 0.0055, train_mae: 0.0291, val_mae: 0.0661\n",
      "Epoch [410/1000] - train_loss: 0.0010, val_loss: 0.0097, train_mae: 0.0259, val_mae: 0.0789\n",
      "Epoch [411/1000] - train_loss: 0.0011, val_loss: 0.0071, train_mae: 0.0270, val_mae: 0.0700\n",
      "Epoch [412/1000] - train_loss: 0.0008, val_loss: 0.0059, train_mae: 0.0220, val_mae: 0.0658\n",
      "Epoch [413/1000] - train_loss: 0.0002, val_loss: 0.0052, train_mae: 0.0125, val_mae: 0.0629\n",
      "Epoch [414/1000] - train_loss: 0.0003, val_loss: 0.0067, train_mae: 0.0148, val_mae: 0.0695\n",
      "Epoch [415/1000] - train_loss: 0.0004, val_loss: 0.0056, train_mae: 0.0147, val_mae: 0.0645\n",
      "Epoch [416/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0108, val_mae: 0.0571\n",
      "Epoch [417/1000] - train_loss: 0.0002, val_loss: 0.0041, train_mae: 0.0094, val_mae: 0.0552\n",
      "Epoch [418/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0079, val_mae: 0.0599\n",
      "Epoch [419/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0071, val_mae: 0.0633\n",
      "Epoch [420/1000] - train_loss: 0.0002, val_loss: 0.0058, train_mae: 0.0108, val_mae: 0.0635\n",
      "Epoch [421/1000] - train_loss: 0.0004, val_loss: 0.0051, train_mae: 0.0165, val_mae: 0.0588\n",
      "Epoch [422/1000] - train_loss: 0.0003, val_loss: 0.0058, train_mae: 0.0150, val_mae: 0.0657\n",
      "Epoch [423/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0116, val_mae: 0.0603\n",
      "Epoch [424/1000] - train_loss: 0.0002, val_loss: 0.0040, train_mae: 0.0102, val_mae: 0.0535\n",
      "Epoch [425/1000] - train_loss: 0.0003, val_loss: 0.0051, train_mae: 0.0136, val_mae: 0.0625\n",
      "Epoch [426/1000] - train_loss: 0.0002, val_loss: 0.0065, train_mae: 0.0123, val_mae: 0.0697\n",
      "Epoch [427/1000] - train_loss: 0.0003, val_loss: 0.0048, train_mae: 0.0148, val_mae: 0.0598\n",
      "Epoch [428/1000] - train_loss: 0.0002, val_loss: 0.0043, train_mae: 0.0120, val_mae: 0.0543\n",
      "Epoch [429/1000] - train_loss: 0.0003, val_loss: 0.0058, train_mae: 0.0143, val_mae: 0.0660\n",
      "Epoch [430/1000] - train_loss: 0.0002, val_loss: 0.0053, train_mae: 0.0114, val_mae: 0.0635\n",
      "Epoch [431/1000] - train_loss: 0.0002, val_loss: 0.0040, train_mae: 0.0103, val_mae: 0.0541\n",
      "Epoch [432/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0090, val_mae: 0.0550\n",
      "Epoch [433/1000] - train_loss: 0.0001, val_loss: 0.0057, train_mae: 0.0087, val_mae: 0.0652\n",
      "Epoch [434/1000] - train_loss: 0.0002, val_loss: 0.0054, train_mae: 0.0125, val_mae: 0.0640\n",
      "Epoch [435/1000] - train_loss: 0.0002, val_loss: 0.0051, train_mae: 0.0102, val_mae: 0.0610\n",
      "Epoch [436/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0074, val_mae: 0.0588\n",
      "Epoch [437/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0080, val_mae: 0.0587\n",
      "Epoch [438/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0086, val_mae: 0.0579\n",
      "Epoch [439/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0079, val_mae: 0.0560\n",
      "Epoch [440/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0076, val_mae: 0.0564\n",
      "Epoch [441/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0086, val_mae: 0.0578\n",
      "Epoch [442/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0068, val_mae: 0.0586\n",
      "Epoch [443/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0080, val_mae: 0.0618\n",
      "Epoch [444/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0105, val_mae: 0.0610\n",
      "Epoch [445/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0088, val_mae: 0.0572\n",
      "Epoch [446/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0067, val_mae: 0.0590\n",
      "Epoch [447/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0085, val_mae: 0.0650\n",
      "Epoch [448/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0089, val_mae: 0.0626\n",
      "Epoch [449/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0067, val_mae: 0.0564\n",
      "Epoch [450/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0069, val_mae: 0.0592\n",
      "Epoch [451/1000] - train_loss: 0.0003, val_loss: 0.0049, train_mae: 0.0131, val_mae: 0.0608\n",
      "Epoch [452/1000] - train_loss: 0.0004, val_loss: 0.0048, train_mae: 0.0164, val_mae: 0.0585\n",
      "Epoch [453/1000] - train_loss: 0.0003, val_loss: 0.0053, train_mae: 0.0156, val_mae: 0.0604\n",
      "Epoch [454/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0123, val_mae: 0.0605\n",
      "Epoch [455/1000] - train_loss: 0.0005, val_loss: 0.0042, train_mae: 0.0179, val_mae: 0.0551\n",
      "Epoch [456/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0077, val_mae: 0.0552\n",
      "Epoch [457/1000] - train_loss: 0.0003, val_loss: 0.0042, train_mae: 0.0126, val_mae: 0.0573\n",
      "Epoch [458/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0093, val_mae: 0.0612\n",
      "Epoch [459/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0105, val_mae: 0.0618\n",
      "Epoch [460/1000] - train_loss: 0.0002, val_loss: 0.0051, train_mae: 0.0100, val_mae: 0.0629\n",
      "Epoch [461/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0088, val_mae: 0.0635\n",
      "Epoch [462/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0074, val_mae: 0.0629\n",
      "Epoch [463/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0099, val_mae: 0.0599\n",
      "Epoch [464/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0100, val_mae: 0.0562\n",
      "Epoch [465/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0093, val_mae: 0.0562\n",
      "Epoch [466/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0069, val_mae: 0.0579\n",
      "Epoch [467/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0065, val_mae: 0.0591\n",
      "Epoch [468/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0058, val_mae: 0.0583\n",
      "Epoch [469/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0055, val_mae: 0.0579\n",
      "Epoch [470/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0058, val_mae: 0.0588\n",
      "Epoch [471/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0061, val_mae: 0.0604\n",
      "Epoch [472/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0055, val_mae: 0.0586\n",
      "Epoch [473/1000] - train_loss: 0.0000, val_loss: 0.0045, train_mae: 0.0052, val_mae: 0.0589\n",
      "Epoch [474/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0068, val_mae: 0.0595\n",
      "Epoch [475/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0069, val_mae: 0.0604\n",
      "Epoch [476/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0072, val_mae: 0.0569\n",
      "Epoch [477/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0077, val_mae: 0.0576\n",
      "Epoch [478/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0067, val_mae: 0.0632\n",
      "Epoch [479/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0060, val_mae: 0.0598\n",
      "Epoch [480/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0059, val_mae: 0.0555\n",
      "Epoch [481/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0063, val_mae: 0.0553\n",
      "Epoch [482/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0076, val_mae: 0.0588\n",
      "Epoch [483/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0069, val_mae: 0.0595\n",
      "Epoch [484/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0077, val_mae: 0.0583\n",
      "Epoch [485/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0079, val_mae: 0.0553\n",
      "Epoch [486/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0070, val_mae: 0.0555\n",
      "Epoch [487/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0062, val_mae: 0.0605\n",
      "Epoch [488/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0063, val_mae: 0.0598\n",
      "Epoch [489/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0052, val_mae: 0.0565\n",
      "Epoch [490/1000] - train_loss: 0.0000, val_loss: 0.0045, train_mae: 0.0049, val_mae: 0.0575\n",
      "Epoch [491/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0047, val_mae: 0.0612\n",
      "Epoch [492/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0067, val_mae: 0.0604\n",
      "Epoch [493/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0056, val_mae: 0.0568\n",
      "Epoch [494/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0081, val_mae: 0.0584\n",
      "Epoch [495/1000] - train_loss: 0.0002, val_loss: 0.0052, train_mae: 0.0111, val_mae: 0.0619\n",
      "Epoch [496/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0083, val_mae: 0.0612\n",
      "Epoch [497/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0115, val_mae: 0.0575\n",
      "Epoch [498/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0086, val_mae: 0.0568\n",
      "Epoch [499/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0107, val_mae: 0.0583\n",
      "Epoch [500/1000] - train_loss: 0.0001, val_loss: 0.0059, train_mae: 0.0084, val_mae: 0.0638\n",
      "Epoch [501/1000] - train_loss: 0.0003, val_loss: 0.0050, train_mae: 0.0144, val_mae: 0.0605\n",
      "Epoch [502/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0113, val_mae: 0.0582\n",
      "Epoch [503/1000] - train_loss: 0.0001, val_loss: 0.0038, train_mae: 0.0100, val_mae: 0.0530\n",
      "Epoch [504/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0111, val_mae: 0.0556\n",
      "Epoch [505/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0127, val_mae: 0.0576\n",
      "Epoch [506/1000] - train_loss: 0.0002, val_loss: 0.0043, train_mae: 0.0105, val_mae: 0.0556\n",
      "Epoch [507/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0067, val_mae: 0.0609\n",
      "Epoch [508/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0071, val_mae: 0.0619\n",
      "Epoch [509/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0094, val_mae: 0.0580\n",
      "Epoch [510/1000] - train_loss: 0.0003, val_loss: 0.0048, train_mae: 0.0137, val_mae: 0.0595\n",
      "Epoch [511/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0099, val_mae: 0.0577\n",
      "Epoch [512/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0127, val_mae: 0.0581\n",
      "Epoch [513/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0108, val_mae: 0.0591\n",
      "Epoch [514/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0086, val_mae: 0.0608\n",
      "Epoch [515/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0121, val_mae: 0.0612\n",
      "Epoch [516/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0095, val_mae: 0.0565\n",
      "Epoch [517/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0059, val_mae: 0.0570\n",
      "Epoch [518/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0068, val_mae: 0.0576\n",
      "Epoch [519/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0065, val_mae: 0.0567\n",
      "Epoch [520/1000] - train_loss: 0.0000, val_loss: 0.0044, train_mae: 0.0048, val_mae: 0.0570\n",
      "Epoch [521/1000] - train_loss: 0.0000, val_loss: 0.0045, train_mae: 0.0050, val_mae: 0.0583\n",
      "Epoch [522/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0053, val_mae: 0.0603\n",
      "Epoch [523/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0060, val_mae: 0.0631\n",
      "Epoch [524/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0086, val_mae: 0.0598\n",
      "Epoch [525/1000] - train_loss: 0.0000, val_loss: 0.0042, train_mae: 0.0047, val_mae: 0.0551\n",
      "Epoch [526/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0073, val_mae: 0.0589\n",
      "Epoch [527/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0075, val_mae: 0.0624\n",
      "Epoch [528/1000] - train_loss: 0.0001, val_loss: 0.0044, train_mae: 0.0064, val_mae: 0.0585\n",
      "Epoch [529/1000] - train_loss: 0.0001, val_loss: 0.0042, train_mae: 0.0058, val_mae: 0.0552\n",
      "Epoch [530/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0076, val_mae: 0.0588\n",
      "Epoch [531/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0105, val_mae: 0.0615\n",
      "Epoch [532/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0080, val_mae: 0.0581\n",
      "Epoch [533/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0079, val_mae: 0.0624\n",
      "Epoch [534/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0072, val_mae: 0.0605\n",
      "Epoch [535/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0115, val_mae: 0.0578\n",
      "Epoch [536/1000] - train_loss: 0.0001, val_loss: 0.0041, train_mae: 0.0089, val_mae: 0.0551\n",
      "Epoch [537/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0096, val_mae: 0.0585\n",
      "Epoch [538/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0057, val_mae: 0.0576\n",
      "Epoch [539/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0057, val_mae: 0.0568\n",
      "Epoch [540/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0051, val_mae: 0.0587\n",
      "Epoch [541/1000] - train_loss: 0.0000, val_loss: 0.0046, train_mae: 0.0052, val_mae: 0.0582\n",
      "Epoch [542/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0057, val_mae: 0.0607\n",
      "Epoch [543/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0117, val_mae: 0.0617\n",
      "Epoch [544/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0090, val_mae: 0.0577\n",
      "Epoch [545/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0088, val_mae: 0.0603\n",
      "Epoch [546/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0077, val_mae: 0.0614\n",
      "Epoch [547/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0067, val_mae: 0.0617\n",
      "Epoch [548/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0089, val_mae: 0.0580\n",
      "Epoch [549/1000] - train_loss: 0.0001, val_loss: 0.0047, train_mae: 0.0070, val_mae: 0.0579\n",
      "Epoch [550/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0062, val_mae: 0.0573\n",
      "Epoch [551/1000] - train_loss: 0.0001, val_loss: 0.0043, train_mae: 0.0058, val_mae: 0.0562\n",
      "Epoch [552/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0066, val_mae: 0.0621\n",
      "Epoch [553/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0070, val_mae: 0.0621\n",
      "Epoch [554/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0075, val_mae: 0.0605\n",
      "Epoch [555/1000] - train_loss: 0.0003, val_loss: 0.0049, train_mae: 0.0136, val_mae: 0.0620\n",
      "Epoch [556/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0110, val_mae: 0.0654\n",
      "Epoch [557/1000] - train_loss: 0.0001, val_loss: 0.0046, train_mae: 0.0082, val_mae: 0.0572\n",
      "Epoch [558/1000] - train_loss: 0.0002, val_loss: 0.0060, train_mae: 0.0121, val_mae: 0.0671\n",
      "Epoch [559/1000] - train_loss: 0.0001, val_loss: 0.0075, train_mae: 0.0099, val_mae: 0.0746\n",
      "Epoch [560/1000] - train_loss: 0.0003, val_loss: 0.0065, train_mae: 0.0142, val_mae: 0.0675\n",
      "Epoch [561/1000] - train_loss: 0.0003, val_loss: 0.0052, train_mae: 0.0164, val_mae: 0.0591\n",
      "Epoch [562/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0132, val_mae: 0.0599\n",
      "Epoch [563/1000] - train_loss: 0.0003, val_loss: 0.0043, train_mae: 0.0142, val_mae: 0.0540\n",
      "Epoch [564/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0121, val_mae: 0.0605\n",
      "Epoch [565/1000] - train_loss: 0.0008, val_loss: 0.0078, train_mae: 0.0243, val_mae: 0.0739\n",
      "Epoch [566/1000] - train_loss: 0.0004, val_loss: 0.0057, train_mae: 0.0164, val_mae: 0.0635\n",
      "Epoch [567/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0115, val_mae: 0.0563\n",
      "Epoch [568/1000] - train_loss: 0.0002, val_loss: 0.0062, train_mae: 0.0123, val_mae: 0.0645\n",
      "Epoch [569/1000] - train_loss: 0.0004, val_loss: 0.0079, train_mae: 0.0173, val_mae: 0.0728\n",
      "Epoch [570/1000] - train_loss: 0.0009, val_loss: 0.0049, train_mae: 0.0259, val_mae: 0.0590\n",
      "Epoch [571/1000] - train_loss: 0.0005, val_loss: 0.0044, train_mae: 0.0174, val_mae: 0.0575\n",
      "Epoch [572/1000] - train_loss: 0.0006, val_loss: 0.0062, train_mae: 0.0203, val_mae: 0.0672\n",
      "Epoch [573/1000] - train_loss: 0.0004, val_loss: 0.0076, train_mae: 0.0156, val_mae: 0.0716\n",
      "Epoch [574/1000] - train_loss: 0.0005, val_loss: 0.0053, train_mae: 0.0200, val_mae: 0.0582\n",
      "Epoch [575/1000] - train_loss: 0.0006, val_loss: 0.0049, train_mae: 0.0218, val_mae: 0.0575\n",
      "Epoch [576/1000] - train_loss: 0.0003, val_loss: 0.0052, train_mae: 0.0154, val_mae: 0.0619\n",
      "Epoch [577/1000] - train_loss: 0.0002, val_loss: 0.0055, train_mae: 0.0114, val_mae: 0.0638\n",
      "Epoch [578/1000] - train_loss: 0.0002, val_loss: 0.0045, train_mae: 0.0116, val_mae: 0.0556\n",
      "Epoch [579/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0086, val_mae: 0.0554\n",
      "Epoch [580/1000] - train_loss: 0.0001, val_loss: 0.0057, train_mae: 0.0070, val_mae: 0.0632\n",
      "Epoch [581/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0071, val_mae: 0.0652\n",
      "Epoch [582/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0065, val_mae: 0.0632\n",
      "Epoch [583/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0080, val_mae: 0.0583\n",
      "Epoch [584/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0074, val_mae: 0.0623\n",
      "Epoch [585/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0065, val_mae: 0.0588\n",
      "Epoch [586/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0099, val_mae: 0.0596\n",
      "Epoch [587/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0064, val_mae: 0.0615\n",
      "Epoch [588/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0057, val_mae: 0.0627\n",
      "Epoch [589/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0064, val_mae: 0.0593\n",
      "Epoch [590/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0083, val_mae: 0.0583\n",
      "Epoch [591/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0068, val_mae: 0.0572\n",
      "Epoch [592/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0059, val_mae: 0.0594\n",
      "Epoch [593/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0043, val_mae: 0.0631\n",
      "Epoch [594/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0053, val_mae: 0.0603\n",
      "Epoch [595/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0052, val_mae: 0.0573\n",
      "Epoch [596/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0069, val_mae: 0.0612\n",
      "Epoch [597/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0058, val_mae: 0.0633\n",
      "Epoch [598/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0053, val_mae: 0.0609\n",
      "Epoch [599/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0042, val_mae: 0.0579\n",
      "Epoch [600/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0043, val_mae: 0.0599\n",
      "Epoch [601/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0042, val_mae: 0.0631\n",
      "Epoch [602/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0062, val_mae: 0.0582\n",
      "Epoch [603/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0041, val_mae: 0.0586\n",
      "Epoch [604/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0050, val_mae: 0.0612\n",
      "Epoch [605/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0040, val_mae: 0.0605\n",
      "Epoch [606/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0048, val_mae: 0.0609\n",
      "Epoch [607/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0066, val_mae: 0.0621\n",
      "Epoch [608/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0071, val_mae: 0.0598\n",
      "Epoch [609/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0048, val_mae: 0.0595\n",
      "Epoch [610/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0046, val_mae: 0.0587\n",
      "Epoch [611/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0063, val_mae: 0.0598\n",
      "Epoch [612/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0062, val_mae: 0.0615\n",
      "Epoch [613/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0049, val_mae: 0.0602\n",
      "Epoch [614/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0043, val_mae: 0.0608\n",
      "Epoch [615/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0038, val_mae: 0.0604\n",
      "Epoch [616/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0035, val_mae: 0.0599\n",
      "Epoch [617/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0037, val_mae: 0.0612\n",
      "Epoch [618/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0040, val_mae: 0.0611\n",
      "Epoch [619/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0062, val_mae: 0.0612\n",
      "Epoch [620/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0055, val_mae: 0.0602\n",
      "Epoch [621/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0044, val_mae: 0.0598\n",
      "Epoch [622/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0045, val_mae: 0.0621\n",
      "Epoch [623/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0041, val_mae: 0.0624\n",
      "Epoch [624/1000] - train_loss: 0.0000, val_loss: 0.0046, train_mae: 0.0047, val_mae: 0.0573\n",
      "Epoch [625/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0062, val_mae: 0.0587\n",
      "Epoch [626/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0067, val_mae: 0.0618\n",
      "Epoch [627/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0060, val_mae: 0.0637\n",
      "Epoch [628/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0055, val_mae: 0.0608\n",
      "Epoch [629/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0059, val_mae: 0.0605\n",
      "Epoch [630/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0038, val_mae: 0.0593\n",
      "Epoch [631/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0045, val_mae: 0.0575\n",
      "Epoch [632/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0056, val_mae: 0.0601\n",
      "Epoch [633/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0039, val_mae: 0.0607\n",
      "Epoch [634/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0044, val_mae: 0.0588\n",
      "Epoch [635/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0044, val_mae: 0.0587\n",
      "Epoch [636/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0040, val_mae: 0.0610\n",
      "Epoch [637/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0033, val_mae: 0.0599\n",
      "Epoch [638/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0034, val_mae: 0.0592\n",
      "Epoch [639/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0025, val_mae: 0.0611\n",
      "Epoch [640/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0034, val_mae: 0.0603\n",
      "Epoch [641/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0040, val_mae: 0.0590\n",
      "Epoch [642/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0050, val_mae: 0.0613\n",
      "Epoch [643/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0046, val_mae: 0.0625\n",
      "Epoch [644/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0040, val_mae: 0.0600\n",
      "Epoch [645/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0037, val_mae: 0.0609\n",
      "Epoch [646/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0036, val_mae: 0.0620\n",
      "Epoch [647/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0040, val_mae: 0.0607\n",
      "Epoch [648/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0035, val_mae: 0.0590\n",
      "Epoch [649/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0039, val_mae: 0.0591\n",
      "Epoch [650/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0052, val_mae: 0.0598\n",
      "Epoch [651/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0053, val_mae: 0.0597\n",
      "Epoch [652/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0043, val_mae: 0.0607\n",
      "Epoch [653/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0066, val_mae: 0.0625\n",
      "Epoch [654/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0052, val_mae: 0.0595\n",
      "Epoch [655/1000] - train_loss: 0.0000, val_loss: 0.0046, train_mae: 0.0048, val_mae: 0.0579\n",
      "Epoch [656/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0040, val_mae: 0.0597\n",
      "Epoch [657/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0086, val_mae: 0.0599\n",
      "Epoch [658/1000] - train_loss: 0.0001, val_loss: 0.0045, train_mae: 0.0089, val_mae: 0.0587\n",
      "Epoch [659/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0076, val_mae: 0.0607\n",
      "Epoch [660/1000] - train_loss: 0.0002, val_loss: 0.0058, train_mae: 0.0106, val_mae: 0.0652\n",
      "Epoch [661/1000] - train_loss: 0.0002, val_loss: 0.0061, train_mae: 0.0131, val_mae: 0.0672\n",
      "Epoch [662/1000] - train_loss: 0.0003, val_loss: 0.0051, train_mae: 0.0147, val_mae: 0.0621\n",
      "Epoch [663/1000] - train_loss: 0.0002, val_loss: 0.0051, train_mae: 0.0114, val_mae: 0.0603\n",
      "Epoch [664/1000] - train_loss: 0.0002, val_loss: 0.0055, train_mae: 0.0100, val_mae: 0.0609\n",
      "Epoch [665/1000] - train_loss: 0.0002, val_loss: 0.0050, train_mae: 0.0116, val_mae: 0.0577\n",
      "Epoch [666/1000] - train_loss: 0.0002, val_loss: 0.0046, train_mae: 0.0106, val_mae: 0.0569\n",
      "Epoch [667/1000] - train_loss: 0.0002, val_loss: 0.0053, train_mae: 0.0108, val_mae: 0.0610\n",
      "Epoch [668/1000] - train_loss: 0.0002, val_loss: 0.0053, train_mae: 0.0099, val_mae: 0.0604\n",
      "Epoch [669/1000] - train_loss: 0.0005, val_loss: 0.0045, train_mae: 0.0185, val_mae: 0.0546\n",
      "Epoch [670/1000] - train_loss: 0.0002, val_loss: 0.0052, train_mae: 0.0121, val_mae: 0.0578\n",
      "Epoch [671/1000] - train_loss: 0.0004, val_loss: 0.0042, train_mae: 0.0163, val_mae: 0.0539\n",
      "Epoch [672/1000] - train_loss: 0.0003, val_loss: 0.0055, train_mae: 0.0161, val_mae: 0.0599\n",
      "Epoch [673/1000] - train_loss: 0.0005, val_loss: 0.0068, train_mae: 0.0177, val_mae: 0.0656\n",
      "Epoch [674/1000] - train_loss: 0.0004, val_loss: 0.0049, train_mae: 0.0161, val_mae: 0.0581\n",
      "Epoch [675/1000] - train_loss: 0.0003, val_loss: 0.0061, train_mae: 0.0146, val_mae: 0.0639\n",
      "Epoch [676/1000] - train_loss: 0.0007, val_loss: 0.0072, train_mae: 0.0219, val_mae: 0.0733\n",
      "Epoch [677/1000] - train_loss: 0.0008, val_loss: 0.0048, train_mae: 0.0224, val_mae: 0.0545\n",
      "Epoch [678/1000] - train_loss: 0.0010, val_loss: 0.0054, train_mae: 0.0274, val_mae: 0.0546\n",
      "Epoch [679/1000] - train_loss: 0.0008, val_loss: 0.0061, train_mae: 0.0232, val_mae: 0.0648\n",
      "Epoch [680/1000] - train_loss: 0.0006, val_loss: 0.0061, train_mae: 0.0203, val_mae: 0.0695\n",
      "Epoch [681/1000] - train_loss: 0.0002, val_loss: 0.0073, train_mae: 0.0124, val_mae: 0.0717\n",
      "Epoch [682/1000] - train_loss: 0.0003, val_loss: 0.0065, train_mae: 0.0144, val_mae: 0.0688\n",
      "Epoch [683/1000] - train_loss: 0.0003, val_loss: 0.0064, train_mae: 0.0147, val_mae: 0.0686\n",
      "Epoch [684/1000] - train_loss: 0.0003, val_loss: 0.0054, train_mae: 0.0154, val_mae: 0.0609\n",
      "Epoch [685/1000] - train_loss: 0.0007, val_loss: 0.0059, train_mae: 0.0216, val_mae: 0.0605\n",
      "Epoch [686/1000] - train_loss: 0.0009, val_loss: 0.0058, train_mae: 0.0264, val_mae: 0.0627\n",
      "Epoch [687/1000] - train_loss: 0.0005, val_loss: 0.0061, train_mae: 0.0157, val_mae: 0.0677\n",
      "Epoch [688/1000] - train_loss: 0.0002, val_loss: 0.0049, train_mae: 0.0116, val_mae: 0.0604\n",
      "Epoch [689/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0118, val_mae: 0.0581\n",
      "Epoch [690/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0093, val_mae: 0.0605\n",
      "Epoch [691/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0098, val_mae: 0.0629\n",
      "Epoch [692/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0095, val_mae: 0.0629\n",
      "Epoch [693/1000] - train_loss: 0.0002, val_loss: 0.0047, train_mae: 0.0106, val_mae: 0.0585\n",
      "Epoch [694/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0059, val_mae: 0.0598\n",
      "Epoch [695/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0063, val_mae: 0.0610\n",
      "Epoch [696/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0065, val_mae: 0.0593\n",
      "Epoch [697/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0044, val_mae: 0.0628\n",
      "Epoch [698/1000] - train_loss: 0.0000, val_loss: 0.0057, train_mae: 0.0054, val_mae: 0.0652\n",
      "Epoch [699/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0078, val_mae: 0.0610\n",
      "Epoch [700/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0070, val_mae: 0.0587\n",
      "Epoch [701/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0062, val_mae: 0.0602\n",
      "Epoch [702/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0061, val_mae: 0.0618\n",
      "Epoch [703/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0051, val_mae: 0.0624\n",
      "Epoch [704/1000] - train_loss: 0.0001, val_loss: 0.0059, train_mae: 0.0082, val_mae: 0.0661\n",
      "Epoch [705/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0084, val_mae: 0.0647\n",
      "Epoch [706/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0068, val_mae: 0.0600\n",
      "Epoch [707/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0068, val_mae: 0.0623\n",
      "Epoch [708/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0053, val_mae: 0.0608\n",
      "Epoch [709/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0043, val_mae: 0.0595\n",
      "Epoch [710/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0045, val_mae: 0.0619\n",
      "Epoch [711/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0061, val_mae: 0.0630\n",
      "Epoch [712/1000] - train_loss: 0.0001, val_loss: 0.0048, train_mae: 0.0062, val_mae: 0.0584\n",
      "Epoch [713/1000] - train_loss: 0.0001, val_loss: 0.0049, train_mae: 0.0058, val_mae: 0.0592\n",
      "Epoch [714/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0059, val_mae: 0.0603\n",
      "Epoch [715/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0059, val_mae: 0.0658\n",
      "Epoch [716/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0074, val_mae: 0.0651\n",
      "Epoch [717/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0044, val_mae: 0.0630\n",
      "Epoch [718/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0045, val_mae: 0.0618\n",
      "Epoch [719/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0050, val_mae: 0.0625\n",
      "Epoch [720/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0043, val_mae: 0.0623\n",
      "Epoch [721/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0039, val_mae: 0.0613\n",
      "Epoch [722/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0031, val_mae: 0.0623\n",
      "Epoch [723/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0031, val_mae: 0.0628\n",
      "Epoch [724/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0041, val_mae: 0.0616\n",
      "Epoch [725/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0043, val_mae: 0.0625\n",
      "Epoch [726/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0031, val_mae: 0.0613\n",
      "Epoch [727/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0033, val_mae: 0.0597\n",
      "Epoch [728/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0055, val_mae: 0.0632\n",
      "Epoch [729/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0044, val_mae: 0.0633\n",
      "Epoch [730/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0046, val_mae: 0.0616\n",
      "Epoch [731/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0061, val_mae: 0.0610\n",
      "Epoch [732/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0057, val_mae: 0.0620\n",
      "Epoch [733/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0046, val_mae: 0.0606\n",
      "Epoch [734/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0043, val_mae: 0.0610\n",
      "Epoch [735/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0068, val_mae: 0.0604\n",
      "Epoch [736/1000] - train_loss: 0.0000, val_loss: 0.0047, train_mae: 0.0056, val_mae: 0.0586\n",
      "Epoch [737/1000] - train_loss: 0.0001, val_loss: 0.0056, train_mae: 0.0057, val_mae: 0.0645\n",
      "Epoch [738/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0060, val_mae: 0.0639\n",
      "Epoch [739/1000] - train_loss: 0.0000, val_loss: 0.0048, train_mae: 0.0043, val_mae: 0.0599\n",
      "Epoch [740/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0057, val_mae: 0.0601\n",
      "Epoch [741/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0076, val_mae: 0.0627\n",
      "Epoch [742/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0071, val_mae: 0.0615\n",
      "Epoch [743/1000] - train_loss: 0.0001, val_loss: 0.0052, train_mae: 0.0067, val_mae: 0.0626\n",
      "Epoch [744/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0073, val_mae: 0.0631\n",
      "Epoch [745/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0060, val_mae: 0.0666\n",
      "Epoch [746/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0055, val_mae: 0.0638\n",
      "Epoch [747/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0043, val_mae: 0.0612\n",
      "Epoch [748/1000] - train_loss: 0.0000, val_loss: 0.0050, train_mae: 0.0051, val_mae: 0.0615\n",
      "Epoch [749/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0044, val_mae: 0.0625\n",
      "Epoch [750/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0049, val_mae: 0.0629\n",
      "Epoch [751/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0078, val_mae: 0.0613\n",
      "Epoch [752/1000] - train_loss: 0.0002, val_loss: 0.0048, train_mae: 0.0101, val_mae: 0.0565\n",
      "Epoch [753/1000] - train_loss: 0.0003, val_loss: 0.0059, train_mae: 0.0158, val_mae: 0.0669\n",
      "Epoch [754/1000] - train_loss: 0.0002, val_loss: 0.0060, train_mae: 0.0113, val_mae: 0.0656\n",
      "Epoch [755/1000] - train_loss: 0.0002, val_loss: 0.0058, train_mae: 0.0115, val_mae: 0.0638\n",
      "Epoch [756/1000] - train_loss: 0.0002, val_loss: 0.0056, train_mae: 0.0116, val_mae: 0.0616\n",
      "Epoch [757/1000] - train_loss: 0.0002, val_loss: 0.0062, train_mae: 0.0105, val_mae: 0.0666\n",
      "Epoch [758/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0075, val_mae: 0.0657\n",
      "Epoch [759/1000] - train_loss: 0.0001, val_loss: 0.0051, train_mae: 0.0072, val_mae: 0.0621\n",
      "Epoch [760/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0044, val_mae: 0.0604\n",
      "Epoch [761/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0058, val_mae: 0.0625\n",
      "Epoch [762/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0045, val_mae: 0.0614\n",
      "Epoch [763/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0044, val_mae: 0.0618\n",
      "Epoch [764/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0043, val_mae: 0.0616\n",
      "Epoch [765/1000] - train_loss: 0.0000, val_loss: 0.0049, train_mae: 0.0032, val_mae: 0.0594\n",
      "Epoch [766/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0049, val_mae: 0.0629\n",
      "Epoch [767/1000] - train_loss: 0.0001, val_loss: 0.0065, train_mae: 0.0079, val_mae: 0.0690\n",
      "Epoch [768/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0100, val_mae: 0.0629\n",
      "Epoch [769/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0049, val_mae: 0.0606\n",
      "Epoch [770/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0047, val_mae: 0.0628\n",
      "Epoch [771/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0038, val_mae: 0.0628\n",
      "Epoch [772/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0037, val_mae: 0.0610\n",
      "Epoch [773/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0038, val_mae: 0.0610\n",
      "Epoch [774/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0032, val_mae: 0.0646\n",
      "Epoch [775/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0031, val_mae: 0.0644\n",
      "Epoch [776/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0034, val_mae: 0.0631\n",
      "Epoch [777/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0037, val_mae: 0.0637\n",
      "Epoch [778/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0040, val_mae: 0.0623\n",
      "Epoch [779/1000] - train_loss: 0.0000, val_loss: 0.0051, train_mae: 0.0040, val_mae: 0.0605\n",
      "Epoch [780/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0058, val_mae: 0.0615\n",
      "Epoch [781/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0075, val_mae: 0.0619\n",
      "Epoch [782/1000] - train_loss: 0.0001, val_loss: 0.0057, train_mae: 0.0084, val_mae: 0.0613\n",
      "Epoch [783/1000] - train_loss: 0.0004, val_loss: 0.0054, train_mae: 0.0165, val_mae: 0.0605\n",
      "Epoch [784/1000] - train_loss: 0.0005, val_loss: 0.0046, train_mae: 0.0191, val_mae: 0.0541\n",
      "Epoch [785/1000] - train_loss: 0.0005, val_loss: 0.0063, train_mae: 0.0176, val_mae: 0.0672\n",
      "Epoch [786/1000] - train_loss: 0.0007, val_loss: 0.0048, train_mae: 0.0204, val_mae: 0.0559\n",
      "Epoch [787/1000] - train_loss: 0.0004, val_loss: 0.0046, train_mae: 0.0139, val_mae: 0.0579\n",
      "Epoch [788/1000] - train_loss: 0.0006, val_loss: 0.0038, train_mae: 0.0187, val_mae: 0.0524\n",
      "Epoch [789/1000] - train_loss: 0.0003, val_loss: 0.0040, train_mae: 0.0138, val_mae: 0.0557\n",
      "Epoch [790/1000] - train_loss: 0.0002, val_loss: 0.0044, train_mae: 0.0100, val_mae: 0.0587\n",
      "Epoch [791/1000] - train_loss: 0.0002, val_loss: 0.0054, train_mae: 0.0117, val_mae: 0.0605\n",
      "Epoch [792/1000] - train_loss: 0.0003, val_loss: 0.0060, train_mae: 0.0127, val_mae: 0.0668\n",
      "Epoch [793/1000] - train_loss: 0.0001, val_loss: 0.0056, train_mae: 0.0093, val_mae: 0.0644\n",
      "Epoch [794/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0088, val_mae: 0.0637\n",
      "Epoch [795/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0068, val_mae: 0.0613\n",
      "Epoch [796/1000] - train_loss: 0.0001, val_loss: 0.0059, train_mae: 0.0067, val_mae: 0.0656\n",
      "Epoch [797/1000] - train_loss: 0.0000, val_loss: 0.0060, train_mae: 0.0053, val_mae: 0.0666\n",
      "Epoch [798/1000] - train_loss: 0.0000, val_loss: 0.0065, train_mae: 0.0052, val_mae: 0.0694\n",
      "Epoch [799/1000] - train_loss: 0.0001, val_loss: 0.0067, train_mae: 0.0092, val_mae: 0.0702\n",
      "Epoch [800/1000] - train_loss: 0.0003, val_loss: 0.0057, train_mae: 0.0145, val_mae: 0.0645\n",
      "Epoch [801/1000] - train_loss: 0.0002, val_loss: 0.0060, train_mae: 0.0114, val_mae: 0.0624\n",
      "Epoch [802/1000] - train_loss: 0.0002, val_loss: 0.0058, train_mae: 0.0118, val_mae: 0.0607\n",
      "Epoch [803/1000] - train_loss: 0.0001, val_loss: 0.0060, train_mae: 0.0089, val_mae: 0.0647\n",
      "Epoch [804/1000] - train_loss: 0.0001, val_loss: 0.0064, train_mae: 0.0063, val_mae: 0.0672\n",
      "Epoch [805/1000] - train_loss: 0.0001, val_loss: 0.0061, train_mae: 0.0061, val_mae: 0.0658\n",
      "Epoch [806/1000] - train_loss: 0.0001, val_loss: 0.0055, train_mae: 0.0053, val_mae: 0.0624\n",
      "Epoch [807/1000] - train_loss: 0.0001, val_loss: 0.0057, train_mae: 0.0057, val_mae: 0.0640\n",
      "Epoch [808/1000] - train_loss: 0.0000, val_loss: 0.0057, train_mae: 0.0056, val_mae: 0.0646\n",
      "Epoch [809/1000] - train_loss: 0.0001, val_loss: 0.0058, train_mae: 0.0061, val_mae: 0.0650\n",
      "Epoch [810/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0054, val_mae: 0.0616\n",
      "Epoch [811/1000] - train_loss: 0.0000, val_loss: 0.0061, train_mae: 0.0042, val_mae: 0.0666\n",
      "Epoch [812/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0053, val_mae: 0.0635\n",
      "Epoch [813/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0041, val_mae: 0.0625\n",
      "Epoch [814/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0041, val_mae: 0.0625\n",
      "Epoch [815/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0039, val_mae: 0.0605\n",
      "Epoch [816/1000] - train_loss: 0.0001, val_loss: 0.0054, train_mae: 0.0064, val_mae: 0.0614\n",
      "Epoch [817/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0062, val_mae: 0.0606\n",
      "Epoch [818/1000] - train_loss: 0.0000, val_loss: 0.0059, train_mae: 0.0045, val_mae: 0.0646\n",
      "Epoch [819/1000] - train_loss: 0.0001, val_loss: 0.0063, train_mae: 0.0077, val_mae: 0.0670\n",
      "Epoch [820/1000] - train_loss: 0.0001, val_loss: 0.0056, train_mae: 0.0067, val_mae: 0.0636\n",
      "Epoch [821/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0037, val_mae: 0.0612\n",
      "Epoch [822/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0043, val_mae: 0.0618\n",
      "Epoch [823/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0047, val_mae: 0.0612\n",
      "Epoch [824/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0041, val_mae: 0.0614\n",
      "Epoch [825/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0029, val_mae: 0.0635\n",
      "Epoch [826/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0033, val_mae: 0.0617\n",
      "Epoch [827/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0035, val_mae: 0.0616\n",
      "Epoch [828/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0027, val_mae: 0.0635\n",
      "Epoch [829/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0032, val_mae: 0.0628\n",
      "Epoch [830/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0027, val_mae: 0.0627\n",
      "Epoch [831/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0023, val_mae: 0.0631\n",
      "Epoch [832/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0025, val_mae: 0.0618\n",
      "Epoch [833/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0030, val_mae: 0.0616\n",
      "Epoch [834/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0031, val_mae: 0.0627\n",
      "Epoch [835/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0025, val_mae: 0.0617\n",
      "Epoch [836/1000] - train_loss: 0.0000, val_loss: 0.0053, train_mae: 0.0022, val_mae: 0.0621\n",
      "Epoch [837/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0023, val_mae: 0.0641\n",
      "Epoch [838/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0046, val_mae: 0.0630\n",
      "Epoch [839/1000] - train_loss: 0.0000, val_loss: 0.0055, train_mae: 0.0053, val_mae: 0.0630\n",
      "Epoch [840/1000] - train_loss: 0.0000, val_loss: 0.0052, train_mae: 0.0035, val_mae: 0.0604\n",
      "Epoch [841/1000] - train_loss: 0.0001, val_loss: 0.0050, train_mae: 0.0056, val_mae: 0.0589\n",
      "Epoch [842/1000] - train_loss: 0.0000, val_loss: 0.0056, train_mae: 0.0045, val_mae: 0.0632\n",
      "Epoch [843/1000] - train_loss: 0.0000, val_loss: 0.0061, train_mae: 0.0053, val_mae: 0.0657\n",
      "Epoch [844/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0057, val_mae: 0.0619\n",
      "Epoch [845/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0045, val_mae: 0.0625\n",
      "Epoch [846/1000] - train_loss: 0.0001, val_loss: 0.0053, train_mae: 0.0074, val_mae: 0.0629\n",
      "Epoch [847/1000] - train_loss: 0.0000, val_loss: 0.0054, train_mae: 0.0049, val_mae: 0.0629\n",
      "Timeout reached (10.012875318527222 sec)\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_maes, val_maes, best_state = train_loop(net, train_dataloader, val_dataloader, timeout=10, patience=5000, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_losses, './train_losses.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Overfit Experiment\n",
    "\n",
    "I trained a `6x50` model for 48h on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neurons = 50\n",
    "h = 2\n",
    "w = 2\n",
    "net = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(3*h*w, n_neurons),     torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(), \n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, n_neurons), torch.nn.ReLU(),\n",
    "                                              torch.nn.Linear(n_neurons, 1))\n",
    "\n",
    "state = torch.load('./net_overfit_best_state.pth')\n",
    "net.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = torch.load('./train_losses.pth')\n",
    "val_losses   = torch.load('./val_losses.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x75819e89a290>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"train mse\")\n",
    "plt.plot(val_losses, label=\"val mse\", alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a noticeable improvement from epoch $2000$ onward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 2, 2)\n",
    "\n",
    "models = [net]\n",
    "onnx_files = ['net6x50_overfit.onnx']\n",
    "\n",
    "for model, filename in zip(models, onnx_files):\n",
    "    torch.onnx.export(model, dummy_input, './verification/models/' + filename, export_params=True, do_constant_folding=True, opset_version=7, input_names=['X'], output_names=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcrown-fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
